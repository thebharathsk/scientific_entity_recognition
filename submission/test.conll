People -X- _ O
often -X- _ O
imagine -X- _ O
relevant -X- _ O
scenes -X- _ O
to -X- _ O
aid -X- _ O
in -X- _ O
the -X- _ O
writing -X- _ O
process -X- _ O
. -X- _ O
In -X- _ O
this -X- _ O
work -X- _ O
, -X- _ O
we -X- _ O
aim -X- _ O
to -X- _ O
utilize -X- _ O
visual -X- _ O
information -X- _ O
for -X- _ O
composition -X- _ O
in -X- _ O
the -X- _ O
same -X- _ O
manner -X- _ O
as -X- _ O
humans -X- _ O
. -X- _ O
We -X- _ O
propose -X- _ O
a -X- _ O
method -X- _ O
, -X- _ O
LIVE -X- _ B-MethodName
, -X- _ O
that -X- _ O
makes -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
language -X- _ O
models -X- _ O
( -X- _ O
PLMs -X- _ O
) -X- _ O
Learn -X- _ B-MethodName
to -X- _ I-MethodName
Imagine -X- _ I-MethodName
for -X- _ I-MethodName
Visuallyaugmented -X- _ I-MethodName
natural -X- _ I-MethodName
language -X- _ I-MethodName
gEneration -X- _ I-MethodName
. -X- _ O
First -X- _ O
, -X- _ O
we -X- _ O
imagine -X- _ O
the -X- _ O
scene -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
text -X- _ O
: -X- _ O
we -X- _ O
use -X- _ O
a -X- _ O
diffusion -X- _ B-MethodName
model -X- _ O
to -X- _ O
synthesize -X- _ O
high -X- _ O
- -X- _ O
quality -X- _ O
images -X- _ O
conditioned -X- _ O
on -X- _ O
the -X- _ O
input -X- _ O
texts -X- _ O
. -X- _ O
Second -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
CLIP -X- _ O
to -X- _ O
determine -X- _ O
whether -X- _ O
the -X- _ O
text -X- _ O
can -X- _ O
evoke -X- _ O
the -X- _ O
imagination -X- _ O
in -X- _ O
a -X- _ O
posterior -X- _ O
way -X- _ O
. -X- _ O
Finally -X- _ O
, -X- _ O
our -X- _ O
imagination -X- _ O
is -X- _ O
dynamic -X- _ O
, -X- _ O
and -X- _ O
we -X- _ O
conduct -X- _ O
synthesis -X- _ O
for -X- _ O
each -X- _ O
sentence -X- _ O
rather -X- _ O
than -X- _ O
generate -X- _ O
only -X- _ O
one -X- _ O
image -X- _ O
for -X- _ O
an -X- _ O
entire -X- _ O
paragraph -X- _ O
. -X- _ O
Technically -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
a -X- _ O
novel -X- _ O
plug -X- _ O
- -X- _ O
and -X- _ O
- -X- _ O
play -X- _ O
fusion -X- _ O
layer -X- _ O
to -X- _ O
obtain -X- _ O
visually -X- _ O
- -X- _ O
augmented -X- _ O
representations -X- _ O
for -X- _ O
each -X- _ O
text -X- _ O
. -X- _ O
Our -X- _ O
vision -X- _ O
- -X- _ O
text -X- _ O
fusion -X- _ O
layer -X- _ O
is -X- _ O
compatible -X- _ O
with -X- _ O
Transformerbased -X- _ O
architecture -X- _ O
. -X- _ O
We -X- _ O
have -X- _ O
conducted -X- _ O
extensive -X- _ O
experiments -X- _ O
on -X- _ O
four -X- _ O
generation -X- _ O
tasks -X- _ O
using -X- _ O
BART -X- _ B-MethodName
and -X- _ O
T5 -X- _ B-MethodName
, -X- _ O
and -X- _ O
the -X- _ O
automatic -X- _ O
results -X- _ O
and -X- _ O
human -X- _ O
evaluation -X- _ O
demonstrate -X- _ O
the -X- _ O
effectiveness -X- _ O
of -X- _ O
our -X- _ O
proposed -X- _ O
method -X- _ O
. -X- _ O
We -X- _ O
will -X- _ O
release -X- _ O
the -X- _ O
code -X- _ O
, -X- _ O
model -X- _ O
, -X- _ O
and -X- _ O
data -X- _ O
at -X- _ O
the -X- _ O
link -X- _ O
: -X- _ O
https -X- _ O
: -X- _ O
/ -X- _ O
/ -X- _ O
github.com -X- _ O
/ -X- _ O
RUCAIBox -X- _ O
/ -X- _ O
LIVE -X- _ O
. -X- _ O

To -X- _ O
improve -X- _ O
the -X- _ O
generation -X- _ O
capacity -X- _ O
of -X- _ O
PLMs -X- _ O
, -X- _ O
existing -X- _ O
work -X- _ O
has -X- _ O
widely -X- _ O
explored -X- _ O
various -X- _ O
methods -X- _ O
to -X- _ O
incorporate -X- _ O
visual -X- _ O
knowledge -X- _ O
into -X- _ O
models -X- _ O
, -X- _ O
which -X- _ O
can -X- _ O
be -X- _ O
roughly -X- _ O
divided -X- _ O
into -X- _ O
two -X- _ O
lines -X- _ O
of -X- _ O
research -X- _ O
. -X- _ O
The -X- _ O
first -X- _ O
line -X- _ O
designs -X- _ O
specific -X- _ O
visually -X- _ O
- -X- _ O
enhanced -X- _ O
training -X- _ O
tasks -X- _ O
such -X- _ O
as -X- _ O
continual -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
on -X- _ O
text -X- _ O
- -X- _ O
image -X- _ O
data -X- _ O
or -X- _ O
knowledge -X- _ O
distillation -X- _ O
with -X- _ O
vision -X- _ O
- -X- _ O
language -X- _ O
models -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
these -X- _ O
methods -X- _ O
usually -X- _ O
perform -X- _ O
well -X- _ O
only -X- _ O
on -X- _ O
multimodal -X- _ B-TaskName
generation -X- _ I-TaskName
tasks -X- _ I-TaskName
( -X- _ O
e.g. -X- _ O
, -X- _ O
visual -X- _ O
question -X- _ O
answering -X- _ O
) -X- _ O
but -X- _ O
not -X- _ O
text -X- _ O
generation -X- _ O
tasks -X- _ O
, -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
semantic -X- _ O
disparity -X- _ O
across -X- _ O
modalities -X- _ O
( -X- _ O
Tan -X- _ O
and -X- _ O
Bansal -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
. -X- _ O
As -X- _ O
the -X- _ O
second -X- _ O
line -X- _ O
, -X- _ O
several -X- _ O
studies -X- _ O
retrieve -X- _ O
or -X- _ O
synthesize -X- _ O
images -X- _ O
related -X- _ O
to -X- _ O
the -X- _ O
input -X- _ O
and -X- _ O
then -X- _ O
fuse -X- _ O
the -X- _ O
image -X- _ O
representations -X- _ O
into -X- _ O
PLMs -X- _ O
( -X- _ O
Wang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022b -X- _ O
; -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
they -X- _ O
simply -X- _ O
treat -X- _ O
the -X- _ O
input -X- _ O
as -X- _ O
a -X- _ O
whole -X- _ O
( -X- _ O
even -X- _ O
for -X- _ O
long -X- _ O
texts -X- _ O
) -X- _ O
for -X- _ O
retrieving -X- _ O
or -X- _ O
synthesizing -X- _ O
related -X- _ O
images -X- _ O
, -X- _ O
which -X- _ O
can -X- _ O
not -X- _ O
sufficiently -X- _ O
leverage -X- _ O
fine -X- _ O
- -X- _ O
grained -X- _ O
visual -X- _ O
semantics -X- _ O
. -X- _ O

To -X- _ O
this -X- _ O
end -X- _ O
, -X- _ O
in -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
a -X- _ O
novel -X- _ O
approach -X- _ O
, -X- _ O
LIVE -X- _ B-MethodName
, -X- _ O
that -X- _ O
enables -X- _ O
PLMs -X- _ O
to -X- _ O
Learn -X- _ B-MethodName
to -X- _ I-MethodName
Imagine -X- _ I-MethodName
for -X- _ I-MethodName
Visually -X- _ I-MethodName
- -X- _ I-MethodName
augmented -X- _ I-MethodName
natural -X- _ I-MethodName
language -X- _ I-MethodName
gEneration -X- _ I-MethodName
. -X- _ O
Different -X- _ O
from -X- _ O
previous -X- _ O
methods -X- _ O
, -X- _ O
our -X- _ O
augmentation -X- _ O
approach -X- _ O
is -X- _ O
relevant -X- _ O
, -X- _ O
selective -X- _ O
, -X- _ O
and -X- _ O
dynamic -X- _ O
. -X- _ O
To -X- _ O
be -X- _ O
relevant -X- _ O
, -X- _ O
we -X- _ O
utilize -X- _ O
the -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
theart -X- _ O
text -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
image -X- _ O
model -X- _ O
, -X- _ O
Stable -X- _ O
Diffusion -X- _ O
( -X- _ O
Rombach -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
, -X- _ O
to -X- _ O
synthesize -X- _ O
realistic -X- _ O
images -X- _ O
for -X- _ O
fine -X- _ O
- -X- _ O
grained -X- _ O
semantic -X- _ O
units -X- _ O
( -X- _ O
i.e. -X- _ O
, -X- _ O
sentences -X- _ O
) -X- _ O
. -X- _ O
Compared -X- _ O
to -X- _ O
the -X- _ O
retrieval -X- _ O
- -X- _ O
based -X- _ O
approach -X- _ O
, -X- _ O
our -X- _ O
method -X- _ O
can -X- _ O
generate -X- _ O
more -X- _ O
relevant -X- _ O
, -X- _ O
diverse -X- _ O
images -X- _ O
that -X- _ O
The -X- _ O
overall -X- _ O
illustration -X- _ O
of -X- _ O
our -X- _ O
proposed -X- _ O
approach -X- _ O
LIVE -X- _ B-MethodName
, -X- _ O
consisting -X- _ O
of -X- _ O
the -X- _ O
text -X- _ O
- -X- _ O
related -X- _ O
image -X- _ O
generation -X- _ O
and -X- _ O
the -X- _ O
plug -X- _ O
- -X- _ O
and -X- _ O
- -X- _ O
play -X- _ O
vision -X- _ O
- -X- _ O
text -X- _ O
fusion -X- _ O
layer -X- _ O
. -X- _ O
The -X- _ O
fusion -X- _ O
attention -X- _ O
mask -X- _ O
means -X- _ O
that -X- _ O
the -X- _ O
first -X- _ O
sentence -X- _ O
x -X- _ O
1 -X- _ O
lacks -X- _ O
visuality -X- _ O
and -X- _ O
will -X- _ O
skip -X- _ O
the -X- _ O
fusion -X- _ O
layer -X- _ O
( -X- _ O
green -X- _ O
flow -X- _ O
) -X- _ O
, -X- _ O
while -X- _ O
the -X- _ O
second -X- _ O
sentence -X- _ O
x -X- _ O
2 -X- _ O
has -X- _ O
high -X- _ O
visuality -X- _ O
, -X- _ O
and -X- _ O
each -X- _ O
word -X- _ O
x -X- _ O
2i -X- _ O
of -X- _ O
x -X- _ O
2 -X- _ O
will -X- _ O
attend -X- _ O
to -X- _ O
the -X- _ O
synthesized -X- _ O
image -X- _ O
to -X- _ O
obtain -X- _ O
visually -X- _ O
- -X- _ O
augmented -X- _ O
text -X- _ O
representations -X- _ O
( -X- _ O
red -X- _ O
flow -X- _ O
) -X- _ O
. -X- _ O

may -X- _ O
not -X- _ O
exist -X- _ O
in -X- _ O
real -X- _ O
- -X- _ O
world -X- _ O
image -X- _ O
databases -X- _ O
. -X- _ O
To -X- _ O
be -X- _ O
selective -X- _ O
, -X- _ O
we -X- _ O
evaluate -X- _ O
the -X- _ O
degree -X- _ O
to -X- _ O
which -X- _ O
the -X- _ O
text -X- _ O
's -X- _ O
meaning -X- _ O
can -X- _ O
be -X- _ O
visualized -X- _ O
in -X- _ O
an -X- _ O
image -X- _ O
and -X- _ O
only -X- _ O
invoke -X- _ O
the -X- _ O
use -X- _ O
of -X- _ O
synthesized -X- _ O
images -X- _ O
when -X- _ O
it -X- _ O
is -X- _ O
actually -X- _ O
needed -X- _ O
. -X- _ O
To -X- _ O
be -X- _ O
dynamic -X- _ O
, -X- _ O
we -X- _ O
synthesize -X- _ O
images -X- _ O
for -X- _ O
each -X- _ O
sentence -X- _ O
in -X- _ O
the -X- _ O
input -X- _ O
text -X- _ O
so -X- _ O
that -X- _ O
the -X- _ O
visual -X- _ O
knowledge -X- _ O
is -X- _ O
more -X- _ O
fine -X- _ O
- -X- _ O
grained -X- _ O
compared -X- _ O
to -X- _ O
a -X- _ O
single -X- _ O
image -X- _ O
for -X- _ O
the -X- _ O
whole -X- _ O
input -X- _ O
. -X- _ O
In -X- _ O
order -X- _ O
to -X- _ O
deeply -X- _ O
fuse -X- _ O
the -X- _ O
visual -X- _ O
knowledge -X- _ O
of -X- _ O
synthesized -X- _ O
images -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
a -X- _ O
plug -X- _ B-MethodName
- -X- _ I-MethodName
and -X- _ I-MethodName
- -X- _ I-MethodName
play -X- _ I-MethodName
vision -X- _ I-MethodName
- -X- _ I-MethodName
text -X- _ I-MethodName
fusion -X- _ I-MethodName
layer -X- _ I-MethodName
for -X- _ O
Transformer -X- _ O
- -X- _ O
based -X- _ O
models -X- _ O
. -X- _ O
We -X- _ O
also -X- _ O
design -X- _ O
specific -X- _ O
mechanisms -X- _ O
to -X- _ O
support -X- _ O
efficient -X- _ O
text -X- _ O
- -X- _ O
image -X- _ O
cross -X- _ O
- -X- _ O
attention -X- _ O
and -X- _ O
enable -X- _ O
the -X- _ O
controllability -X- _ O
of -X- _ O
the -X- _ O
use -X- _ O
of -X- _ O
visual -X- _ O
knowledge -X- _ O
. -X- _ O

• -X- _ O
We -X- _ O
propose -X- _ O
a -X- _ O
new -X- _ O
approach -X- _ O
, -X- _ O
called -X- _ O
LIVE -X- _ B-MethodName
, -X- _ O
to -X- _ O
learning -X- _ O
to -X- _ O
use -X- _ O
synthesized -X- _ O
images -X- _ O
to -X- _ O
improve -X- _ O
natural -X- _ B-MetricName
language -X- _ I-MetricName
generation -X- _ I-MetricName
, -X- _ O
imitating -X- _ O
the -X- _ O
process -X- _ O
of -X- _ O
human -X- _ O
writing -X- _ O
. -X- _ O

• -X- _ O
We -X- _ O
propose -X- _ O
a -X- _ O
plug -X- _ B-MethodName
- -X- _ I-MethodName
and -X- _ I-MethodName
- -X- _ I-MethodName
play -X- _ I-MethodName
vision -X- _ I-MethodName
- -X- _ I-MethodName
text -X- _ I-MethodName
fusion -X- _ I-MethodName
layer -X- _ I-MethodName
to -X- _ O
incorporate -X- _ O
visual -X- _ O
knowledge -X- _ O
and -X- _ O
obtain -X- _ O
visually -X- _ O
- -X- _ O
augmented -X- _ O
text -X- _ O
representations -X- _ O
. -X- _ O

Natural -X- _ O
language -X- _ O
generation -X- _ O
( -X- _ O
a.k.a -X- _ O
. -X- _ O
, -X- _ O
text -X- _ O
generation -X- _ O
) -X- _ O
aims -X- _ O
to -X- _ O
capture -X- _ O
the -X- _ O
semantic -X- _ O
mapping -X- _ O
relation -X- _ O
from -X- _ O
an -X- _ O
input -X- _ O
text -X- _ O
X -X- _ B-HyperparameterName
= -X- _ O
⟨x -X- _ O
1 -X- _ O
, -X- _ O
... -X- _ O
, -X- _ O
x -X- _ O
k -X- _ O
, -X- _ O
... -X- _ O
, -X- _ O
x -X- _ O
m -X- _ O
⟩ -X- _ O
to -X- _ O
an -X- _ O
output -X- _ O
text -X- _ O
Y -X- _ B-HyperparameterName
= -X- _ O
⟨y -X- _ O
1 -X- _ O
, -X- _ O
... -X- _ O
, -X- _ O
y -X- _ O
k -X- _ O
, -X- _ O
... -X- _ O
, -X- _ O
y -X- _ O
n -X- _ O
⟩ -X- _ O
, -X- _ O
where -X- _ O
x -X- _ O
k -X- _ O
and -X- _ O
y -X- _ O
k -X- _ O
denote -X- _ O
the -X- _ O
k -X- _ O
- -X- _ O
th -X- _ O
sentences -X- _ O
of -X- _ O
the -X- _ O
input -X- _ O
and -X- _ O
output -X- _ O
texts -X- _ O
, -X- _ O
respectively -X- _ O
. -X- _ O
In -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
focus -X- _ O
on -X- _ O
the -X- _ O
task -X- _ O
of -X- _ O
visually -X- _ B-TaskName
augmented -X- _ I-TaskName
natural -X- _ I-TaskName
language -X- _ I-TaskName
generation -X- _ I-TaskName
( -X- _ B-TaskName
VA -X- _ I-TaskName
- -X- _ I-TaskName
NLG -X- _ I-TaskName
) -X- _ I-TaskName
. -X- _ O
Following -X- _ O
prior -X- _ O
works -X- _ O
Wang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022b -X- _ O
) -X- _ O
, -X- _ O
VA -X- _ B-TaskName
- -X- _ I-TaskName
NLG -X- _ I-TaskName
further -X- _ O
assumes -X- _ O
text -X- _ O
- -X- _ O
related -X- _ O
image -X- _ O
data -X- _ O
can -X- _ O
be -X- _ O
obtained -X- _ O
to -X- _ O
help -X- _ O
text -X- _ O
generation -X- _ O
. -X- _ O
Here -X- _ O
, -X- _ O
we -X- _ O
consider -X- _ O
a -X- _ O
generalized -X- _ O
way -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
retrieval -X- _ O
and -X- _ O
synthesis -X- _ O
) -X- _ O
to -X- _ O
obtain -X- _ O
the -X- _ O
related -X- _ O
images -X- _ O
with -X- _ O
an -X- _ O
image -X- _ O
augmenter -X- _ O
F -X- _ B-HyperparameterName
, -X- _ O
where -X- _ O
F -X- _ B-HyperparameterName
takes -X- _ O
as -X- _ O
input -X- _ O
a -X- _ O
sentence -X- _ O
x -X- _ B-HyperparameterName
( -X- _ O
or -X- _ O
a -X- _ O
text -X- _ O
) -X- _ O
and -X- _ O
outputs -X- _ O
an -X- _ O
image -X- _ O
i -X- _ O
x -X- _ O
related -X- _ O
to -X- _ O
x -X- _ O
: -X- _ O

The -X- _ O
goal -X- _ O
of -X- _ O
VA -X- _ B-TaskName
- -X- _ I-TaskName
NLG -X- _ I-TaskName
is -X- _ O
to -X- _ O
generate -X- _ O
readable -X- _ O
and -X- _ O
plausible -X- _ O
output -X- _ O
texts -X- _ O
Y -X- _ B-HyperparameterName
based -X- _ O
on -X- _ O
input -X- _ O
texts -X- _ O
X -X- _ B-HyperparameterName
and -X- _ O
image -X- _ O
augmenter -X- _ O
F -X- _ B-HyperparameterName
, -X- _ O
which -X- _ O
is -X- _ O
formally -X- _ O
defined -X- _ O
as -X- _ O
: -X- _ O

With -X- _ O
this -X- _ O
formulation -X- _ O
, -X- _ O
there -X- _ O
are -X- _ O
two -X- _ O
key -X- _ O
issues -X- _ O
for -X- _ O
this -X- _ O
task -X- _ O
: -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
how -X- _ O
to -X- _ O
design -X- _ O
the -X- _ O
image -X- _ O
augmenter -X- _ O
to -X- _ O
obtain -X- _ O
potentially -X- _ O
useful -X- _ O
images -X- _ O
, -X- _ O
and -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
how -X- _ O
to -X- _ O
use -X- _ O
the -X- _ O
augmented -X- _ O
images -X- _ O
for -X- _ O
improving -X- _ O
text -X- _ O
generation -X- _ O
. -X- _ O
Considering -X- _ O
the -X- _ O
two -X- _ O
issues -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
LIVE -X- _ B-MethodName
, -X- _ O
a -X- _ O
general -X- _ O
approach -X- _ O
to -X- _ O
augmenting -X- _ O
NLG -X- _ B-TaskName
tasks -X- _ O
with -X- _ O
related -X- _ O
images -X- _ O
, -X- _ O
with -X- _ O
sentence -X- _ O
- -X- _ O
level -X- _ O
image -X- _ O
synthesis -X- _ O
via -X- _ O
text -X- _ B-MethodName
- -X- _ I-MethodName
to -X- _ I-MethodName
- -X- _ I-MethodName
image -X- _ I-MethodName
diffusion -X- _ I-MethodName
model -X- _ I-MethodName
( -X- _ O
Section -X- _ O
3.2 -X- _ O
) -X- _ O
and -X- _ O
plug -X- _ B-MethodName
- -X- _ I-MethodName
and -X- _ I-MethodName
- -X- _ I-MethodName
play -X- _ I-MethodName
vision -X- _ I-MethodName
- -X- _ I-MethodName
text -X- _ I-MethodName
fusion -X- _ I-MethodName
for -X- _ O
using -X- _ O
augmented -X- _ O
images -X- _ O
( -X- _ O
Section -X- _ O
3.3 -X- _ O
) -X- _ O
. -X- _ O

Synthesizing -X- _ O
Relevant -X- _ O
Images -X- _ O
. -X- _ O
To -X- _ O
circumvent -X- _ O
the -X- _ O
limitation -X- _ O
of -X- _ O
static -X- _ O
image -X- _ O
resources -X- _ O
, -X- _ O
we -X- _ O
instead -X- _ O
propose -X- _ O
to -X- _ O
automatically -X- _ O
generate -X- _ O
images -X- _ O
for -X- _ O
given -X- _ O
texts -X- _ O
by -X- _ O
leveraging -X- _ O
text -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
image -X- _ O
generation -X- _ O
models -X- _ O
. -X- _ O
In -X- _ O
contrast -X- _ O
to -X- _ O
previous -X- _ O
works -X- _ O
that -X- _ O
utilize -X- _ O
GAN -X- _ O
- -X- _ O
based -X- _ O
( -X- _ O
Esser -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
or -X- _ O
auto -X- _ O
- -X- _ O
regressive -X- _ O
( -X- _ O
Wang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022a -X- _ O
) -X- _ O
generation -X- _ O
models -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
the -X- _ O
stateof -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
Stable -X- _ O
Diffusion -X- _ B-MethodName
model -X- _ O
( -X- _ O
Rombach -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
, -X- _ O
a -X- _ O
probabilistic -X- _ B-MethodName
diffusion -X- _ I-MethodName
model -X- _ O
guided -X- _ O
by -X- _ O
CLIP -X- _ O
- -X- _ O
encoded -X- _ O
input -X- _ O
text -X- _ O
representations -X- _ O
, -X- _ O
to -X- _ O
synthesize -X- _ O
high -X- _ O
- -X- _ O
quality -X- _ O
images -X- _ O
. -X- _ O
With -X- _ O
Stable -X- _ O
Diffusion -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
flexibly -X- _ O
perform -X- _ O
image -X- _ O
generation -X- _ O
based -X- _ O
on -X- _ O
different -X- _ O
text -X- _ O
units -X- _ O
. -X- _ O
Here -X- _ O
, -X- _ O
we -X- _ O
consider -X- _ O
sentences -X- _ O
as -X- _ O
synthesis -X- _ O
units -X- _ O
, -X- _ O
which -X- _ O
contain -X- _ O
a -X- _ O
moderate -X- _ O
amount -X- _ O
of -X- _ O
information -X- _ O
for -X- _ O
an -X- _ O
image -X- _ O
. -X- _ O
Compared -X- _ O
with -X- _ O
the -X- _ O
previous -X- _ O
work -X- _ O
that -X- _ O
synthesize -X- _ O
a -X- _ O
single -X- _ O
image -X- _ O
for -X- _ O
the -X- _ O
whole -X- _ O
input -X- _ O
, -X- _ O
our -X- _ O
sentence -X- _ O
- -X- _ O
level -X- _ O
generation -X- _ O
is -X- _ O
more -X- _ O
fine -X- _ O
- -X- _ O
grained -X- _ O
. -X- _ O
It -X- _ O
is -X- _ O
inspired -X- _ O
by -X- _ O
the -X- _ O
writing -X- _ O
behavior -X- _ O
of -X- _ O
people -X- _ O
: -X- _ O
one -X- _ O
would -X- _ O
switch -X- _ O
the -X- _ O
imagined -X- _ O
scenes -X- _ O
for -X- _ O
different -X- _ O
sentences -X- _ O
. -X- _ O

For -X- _ O
each -X- _ O
input -X- _ O
sentence -X- _ O
x -X- _ O
k -X- _ O
, -X- _ O
we -X- _ O
apply -X- _ O
Stable -X- _ O
Diffusion -X- _ O
to -X- _ O
synthesize -X- _ O
its -X- _ O
corresponding -X- _ O
creative -X- _ O
image -X- _ O
i -X- _ O
x -X- _ O
k -X- _ O
. -X- _ O
Equipped -X- _ O
with -X- _ O
the -X- _ O
acceleration -X- _ O
method -X- _ O
of -X- _ O
DDIM -X- _ O
, -X- _ O
Stable -X- _ O
Diffusion -X- _ O
is -X- _ O
able -X- _ O
to -X- _ O
synthesize -X- _ O
photographic -X- _ O
images -X- _ O
normally -X- _ O
in -X- _ O
50 -X- _ O
steps -X- _ O
( -X- _ O
Rombach -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
. -X- _ O
In -X- _ O
practice -X- _ O
, -X- _ O
we -X- _ O
empirically -X- _ O
find -X- _ O
that -X- _ O
using -X- _ O
a -X- _ O
25 -X- _ B-HyperparameterValue
- -X- _ O
step -X- _ O
synthesis -X- _ O
can -X- _ O
usually -X- _ O
lead -X- _ O
to -X- _ O
a -X- _ O
decent -X- _ O
performance -X- _ O
in -X- _ O
our -X- _ O
task -X- _ O
( -X- _ O
see -X- _ O
Section -X- _ O
5.4 -X- _ O
for -X- _ O
more -X- _ O
analysis -X- _ O
about -X- _ O
the -X- _ O
diffusion -X- _ O
quality -X- _ O
and -X- _ O
efficiency -X- _ O
) -X- _ O
. -X- _ O

Evaluating -X- _ O
the -X- _ O
Text -X- _ O
Visuality -X- _ O
. -X- _ O
Although -X- _ O
the -X- _ O
generation -X- _ O
- -X- _ O
based -X- _ O
method -X- _ O
is -X- _ O
flexible -X- _ O
to -X- _ O
produce -X- _ O
images -X- _ O
on -X- _ O
various -X- _ O
topics -X- _ O
, -X- _ O
not -X- _ O
all -X- _ O
texts -X- _ O
can -X- _ O
inspire -X- _ O
the -X- _ O
generative -X- _ O
model -X- _ O
to -X- _ O
generate -X- _ O
meaningful -X- _ O
images -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
the -X- _ O
rule -X- _ O
text -X- _ O
" -X- _ O
ACL -X- _ O
2023 -X- _ O
requires -X- _ O
all -X- _ O
papers -X- _ O
to -X- _ O
have -X- _ O
a -X- _ O
clear -X- _ O
discussion -X- _ O
of -X- _ O
limitations -X- _ O
" -X- _ O
. -X- _ O
Only -X- _ O
texts -X- _ O
with -X- _ O
visually -X- _ O
rich -X- _ O
content -X- _ O
can -X- _ O
be -X- _ O
associated -X- _ O
with -X- _ O
images -X- _ O
. -X- _ O
Previous -X- _ O
work -X- _ O
usually -X- _ O
synthesizes -X- _ O
or -X- _ O
retrieves -X- _ O
images -X- _ O
without -X- _ O
considering -X- _ O
the -X- _ O
visuality -X- _ O
of -X- _ O
texts -X- _ O
, -X- _ O
tending -X- _ O
to -X- _ O
incorporate -X- _ O
irrelevant -X- _ O
or -X- _ O
noisy -X- _ O
images -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
difficult -X- _ O
to -X- _ O
directly -X- _ O
measure -X- _ O
the -X- _ O
visuality -X- _ O
of -X- _ O
a -X- _ O
text -X- _ O
. -X- _ O
As -X- _ O
a -X- _ O
compromise -X- _ O
, -X- _ O
we -X- _ O
estimate -X- _ O
the -X- _ O
similarity -X- _ B-MetricName
score -X- _ O
in -X- _ O
a -X- _ O
posterior -X- _ O
way -X- _ O
between -X- _ O
a -X- _ O
sentence -X- _ O
x -X- _ O
k -X- _ O
and -X- _ O
a -X- _ O
synthesized -X- _ O
image -X- _ O
i -X- _ O
x -X- _ O
k -X- _ O
using -X- _ O
CLIP -X- _ O
( -X- _ O
Radford -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
: -X- _ O

γ -X- _ B-HyperparameterName
= -X- _ O
CLIP -X- _ O
( -X- _ O
x -X- _ O
k -X- _ O
, -X- _ O
i -X- _ O
x -X- _ O
k -X- _ O
) -X- _ O
∈ -X- _ O
[ -X- _ O
−1 -X- _ O
, -X- _ O
1 -X- _ O
] -X- _ O
. -X- _ O

CLIP -X- _ O
is -X- _ O
a -X- _ O
vision -X- _ O
- -X- _ O
language -X- _ O
model -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
on -X- _ O
a -X- _ O
massive -X- _ O
amount -X- _ O
of -X- _ O
text -X- _ O
- -X- _ O
image -X- _ O
pairs -X- _ O
using -X- _ O
contrastive -X- _ O
learning -X- _ O
which -X- _ O
excels -X- _ O
at -X- _ O
evaluating -X- _ O
the -X- _ O
similarity -X- _ O
between -X- _ O
text -X- _ O
and -X- _ O
image -X- _ O
. -X- _ O
In -X- _ O
our -X- _ O
work -X- _ O
, -X- _ O
we -X- _ O
manually -X- _ O
set -X- _ O
a -X- _ O
threshold -X- _ B-HyperparameterName
value -X- _ I-HyperparameterName
θ -X- _ B-HyperparameterName
. -X- _ O
If -X- _ O
γ -X- _ B-HyperparameterName
exceeds -X- _ O
the -X- _ O
threshold -X- _ B-HyperparameterName
value -X- _ I-HyperparameterName
, -X- _ O
the -X- _ O
text -X- _ O
is -X- _ O
considered -X- _ O
to -X- _ O
have -X- _ O
high -X- _ O
visuality -X- _ O
; -X- _ O
otherwise -X- _ O
, -X- _ O
we -X- _ O
consider -X- _ O
that -X- _ O
the -X- _ O
text -X- _ O
has -X- _ O
weak -X- _ O
visuality -X- _ O
and -X- _ O
discard -X- _ O
the -X- _ O
synthesized -X- _ O
image -X- _ O
. -X- _ O
We -X- _ O
will -X- _ O
discuss -X- _ O
the -X- _ O
influence -X- _ O
of -X- _ O
θ -X- _ B-HyperparameterName
in -X- _ O
Section -X- _ O
5.3 -X- _ O
. -X- _ O

After -X- _ O
synthesizing -X- _ O
relevant -X- _ O
images -X- _ O
for -X- _ O
given -X- _ O
texts -X- _ O
, -X- _ O
we -X- _ O
study -X- _ O
how -X- _ O
to -X- _ O
leverage -X- _ O
visual -X- _ O
images -X- _ O
for -X- _ O
improving -X- _ O
text -X- _ O
generation -X- _ O
. -X- _ O
Instead -X- _ O
of -X- _ O
using -X- _ O
VLP -X- _ O
models -X- _ O
, -X- _ O
we -X- _ O
aim -X- _ O
to -X- _ O
fuse -X- _ O
the -X- _ O
visual -X- _ O
knowledge -X- _ O
into -X- _ O
a -X- _ O
PLM -X- _ O
- -X- _ O
based -X- _ O
backbone -X- _ O
, -X- _ O
since -X- _ O
text -X- _ B-TaskName
generation -X- _ B-TaskName
is -X- _ O
essentially -X- _ O
a -X- _ O
language -X- _ B-TaskName
modeling -X- _ I-TaskName
task -X- _ O
. -X- _ O
To -X- _ O
enhance -X- _ O
the -X- _ O
cross -X- _ O
- -X- _ O
modality -X- _ O
fusion -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
a -X- _ O
plug -X- _ B-MethodName
- -X- _ I-MethodName
and -X- _ I-MethodName
- -X- _ I-MethodName
play -X- _ I-MethodName
vision -X- _ I-MethodName
- -X- _ I-MethodName
text -X- _ I-MethodName
fusion -X- _ I-MethodName
module -X- _ I-MethodName
to -X- _ O
obtain -X- _ O
deeply -X- _ O
- -X- _ O
fused -X- _ O
visually -X- _ O
- -X- _ O
augmented -X- _ O
text -X- _ O
representations -X- _ O
. -X- _ O

Vision -X- _ O
- -X- _ O
Text -X- _ O
Fusion -X- _ O
for -X- _ O
PLMs -X- _ O
. -X- _ O
Our -X- _ O
fusion -X- _ O
module -X- _ O
is -X- _ O
a -X- _ O
plug -X- _ O
- -X- _ O
and -X- _ O
- -X- _ O
play -X- _ O
attention -X- _ O
layer -X- _ O
for -X- _ O
Transformer -X- _ O
- -X- _ O
based -X- _ O
( -X- _ O
Vaswani -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
models -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
BART -X- _ B-MethodName
( -X- _ O
Lewis -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
and -X- _ O
T5 -X- _ B-MethodName
( -X- _ O
Raffel -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
. -X- _ O
We -X- _ O
insert -X- _ O
the -X- _ O
fusion -X- _ O
layer -X- _ O
after -X- _ O
the -X- _ O
self -X- _ O
- -X- _ O
attention -X- _ O
layer -X- _ O
in -X- _ O
the -X- _ O
encoder -X- _ O
. -X- _ O
Our -X- _ O
fusion -X- _ O
layer -X- _ O
is -X- _ O
a -X- _ O
layer -X- _ O
- -X- _ O
wise -X- _ O
cross -X- _ O
- -X- _ O
attention -X- _ O
module -X- _ O
to -X- _ O
augment -X- _ O
the -X- _ O
word -X- _ O
representations -X- _ O
with -X- _ O
visual -X- _ O
information -X- _ O
. -X- _ O
In -X- _ O
particular -X- _ O
, -X- _ O
for -X- _ O
a -X- _ O
sentence -X- _ O
x -X- _ O
k -X- _ O
and -X- _ O
the -X- _ O
corresponding -X- _ O
synthesized -X- _ O
image -X- _ O
i -X- _ O
x -X- _ O
k -X- _ O
, -X- _ O
we -X- _ O
first -X- _ O
utilize -X- _ O
CLIP -X- _ O
to -X- _ O
encode -X- _ O
the -X- _ O
image -X- _ O
into -X- _ O
patch -X- _ O
representations -X- _ O
I -X- _ O
k -X- _ O
∈ -X- _ O
R -X- _ O
p×d -X- _ O
. -X- _ O
Then -X- _ O
, -X- _ O
we -X- _ O
feed -X- _ O
the -X- _ O
sentence -X- _ O
into -X- _ O
the -X- _ O
Transformer -X- _ O
model -X- _ O
and -X- _ O
obtain -X- _ O
the -X- _ O
output -X- _ O
representation -X- _ O
S -X- _ O
k -X- _ O
, -X- _ O
l -X- _ O
for -X- _ O
the -X- _ O
self -X- _ O
- -X- _ O
attention -X- _ O
sub -X- _ O
- -X- _ O
layer -X- _ O
in -X- _ O
the -X- _ O
l -X- _ O
- -X- _ O
th -X- _ O
layer -X- _ O
of -X- _ O
the -X- _ O
encoder -X- _ O
. -X- _ O
Finally -X- _ O
, -X- _ O
we -X- _ O
pass -X- _ O
S -X- _ O
k -X- _ O
, -X- _ O
l -X- _ O
to -X- _ O
our -X- _ O
l -X- _ O
- -X- _ O
th -X- _ O
plug -X- _ O
- -X- _ O
and -X- _ O
- -X- _ O
play -X- _ O
fusion -X- _ O
layer -X- _ O
to -X- _ O
obtain -X- _ O
the -X- _ O
visually -X- _ O
- -X- _ O
augmented -X- _ O
text -X- _ O
representations -X- _ O
: -X- _ O

where -X- _ O
γ -X- _ B-MetricName
is -X- _ O
the -X- _ O
similarity -X- _ B-MetricName
score -X- _ I-MetricName
computed -X- _ O
in -X- _ O
Equation -X- _ O
2 -X- _ O
, -X- _ O
and -X- _ O
FusionLayer -X- _ O
l -X- _ O
conducts -X- _ O
multi -X- _ O
- -X- _ O
head -X- _ O
attention -X- _ O
on -X- _ O
the -X- _ O
query -X- _ O
, -X- _ O
key -X- _ O
, -X- _ O
and -X- _ O
value -X- _ O
matrices -X- _ O
, -X- _ O
followed -X- _ O
by -X- _ O
residual -X- _ O
connection -X- _ O
and -X- _ O
layer -X- _ O
normalization -X- _ O
. -X- _ O
Here -X- _ O
, -X- _ O
we -X- _ O
introduce -X- _ O
γ -X- _ B-MetricName
to -X- _ O
control -X- _ O
whether -X- _ O
a -X- _ O
generated -X- _ O
image -X- _ O
will -X- _ O
be -X- _ O
used -X- _ O
or -X- _ O
not -X- _ O
. -X- _ O

In -X- _ O
order -X- _ O
to -X- _ O
achieve -X- _ O
decent -X- _ O
performance -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
pre -X- _ O
- -X- _ O
train -X- _ O
the -X- _ O
key -X- _ O
component -X- _ O
of -X- _ O
our -X- _ O
approach -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
the -X- _ O
fusion -X- _ O
layer -X- _ O
( -X- _ O
Section -X- _ O
3.3 -X- _ O
) -X- _ O
, -X- _ O
with -X- _ O
text -X- _ O
- -X- _ O
image -X- _ O
paired -X- _ O
datasets -X- _ O
. -X- _ O
Specially -X- _ O
, -X- _ O
we -X- _ O
collect -X- _ O
the -X- _ O
image -X- _ O
caption -X- _ O
datasets -X- _ O
MS -X- _ B-DatasetName
COCO -X- _ I-DatasetName
( -X- _ O
Lin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2014 -X- _ O
) -X- _ O
, -X- _ O
Flickr30k -X- _ B-DatasetName
( -X- _ O
Plummer -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
, -X- _ O
CC3 -X- _ B-DatasetName
m -X- _ I-DatasetName
( -X- _ O
Sharma -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
Visual -X- _ B-DatasetName
Genome -X- _ I-DatasetName
( -X- _ O
Krishna -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
as -X- _ O
text -X- _ O
- -X- _ O
image -X- _ O
pairs -X- _ O
, -X- _ O
and -X- _ O
utilize -X- _ O
the -X- _ O
caption -X- _ O
text -X- _ O
to -X- _ O
synthesize -X- _ O
images -X- _ O
using -X- _ O
Stable -X- _ O
Diffusion -X- _ O
to -X- _ O
enrich -X- _ O
the -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
pairs -X- _ O
. -X- _ O
In -X- _ O
this -X- _ O
way -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
obtain -X- _ O
9 -X- _ O
million -X- _ O
text -X- _ O
- -X- _ O
image -X- _ O
pairs -X- _ O
in -X- _ O
total -X- _ O
. -X- _ O
Then -X- _ O
, -X- _ O
we -X- _ O
apply -X- _ O
image -X- _ O
- -X- _ O
based -X- _ O
denoising -X- _ O
autoencoding -X- _ O
as -X- _ O
the -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
objective -X- _ O
, -X- _ O
which -X- _ O
teaches -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
recover -X- _ O
the -X- _ O
caption -X- _ O
based -X- _ O
on -X- _ O
a -X- _ O
noisy -X- _ O
text -X- _ O
. -X- _ O
Such -X- _ O
a -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
strategy -X- _ O
can -X- _ O
make -X- _ O
the -X- _ O
fusion -X- _ O
layer -X- _ O
better -X- _ O
map -X- _ O
the -X- _ O
visual -X- _ O
knowledge -X- _ O
into -X- _ O
text -X- _ O
space -X- _ O
. -X- _ O

Next -X- _ O
, -X- _ O
we -X- _ O
describe -X- _ O
the -X- _ O
overall -X- _ O
optimization -X- _ O
process -X- _ O
of -X- _ O
our -X- _ O
approach -X- _ O
. -X- _ O
During -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
, -X- _ O
we -X- _ O
freeze -X- _ O
the -X- _ O
PLM -X- _ O
backbone -X- _ O
and -X- _ O
only -X- _ O
pre -X- _ O
- -X- _ O
train -X- _ O
the -X- _ O
fusion -X- _ O
layer -X- _ O
; -X- _ O
therefore -X- _ O
, -X- _ O
if -X- _ O
we -X- _ O
plug -X- _ O
- -X- _ O
out -X- _ O
the -X- _ O
fusion -X- _ O
layer -X- _ O
, -X- _ O
the -X- _ O
PLM -X- _ O
retains -X- _ O
its -X- _ O
original -X- _ O
language -X- _ O
generation -X- _ O
ability -X- _ O
. -X- _ O
The -X- _ O
fusion -X- _ O
layer -X- _ O
is -X- _ O
a -X- _ O
lightweight -X- _ O
module -X- _ O
and -X- _ O
has -X- _ O
18 -X- _ B-HyperparameterValue
M -X- _ I-HyperparameterValue
parameters -X- _ O
for -X- _ O
BART -X- _ O
BASE -X- _ O
( -X- _ O
140 -X- _ B-HyperparameterValue
M -X- _ I-HyperparameterValue
) -X- _ O
. -X- _ O
During -X- _ O
fine -X- _ O
- -X- _ O
tuning -X- _ O
, -X- _ O
we -X- _ O
utilize -X- _ O
Stable -X- _ O
Diffusion -X- _ O
and -X- _ O
CLIP -X- _ O
models -X- _ O
to -X- _ O
synthesize -X- _ O
images -X- _ O
and -X- _ O
compute -X- _ O
similarity -X- _ B-MetricName
scores -X- _ O
. -X- _ O
These -X- _ O
operations -X- _ O
can -X- _ O
be -X- _ O
done -X- _ O
offline -X- _ O
for -X- _ O
efficiency -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
diffusion -X- _ O
and -X- _ O
CLIP -X- _ O
models -X- _ O
will -X- _ O
not -X- _ O
be -X- _ O
updated -X- _ O
. -X- _ O
We -X- _ O
only -X- _ O
need -X- _ O
to -X- _ O
fine -X- _ O
- -X- _ O
tune -X- _ O
the -X- _ O
whole -X- _ O
PLM -X- _ O
as -X- _ O
usual -X- _ O
, -X- _ O
in -X- _ O
addition -X- _ O
to -X- _ O
the -X- _ O
small -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
fusion -X- _ O
layer -X- _ O
. -X- _ O

• -X- _ O
E2E -X- _ B-DatasetName
( -X- _ O
Novikova -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
is -X- _ O
a -X- _ O
data -X- _ O
- -X- _ O
togeneration -X- _ O
task -X- _ O
with -X- _ O
the -X- _ O
aim -X- _ O
of -X- _ O
converting -X- _ O
multiple -X- _ O
input -X- _ O
meaning -X- _ O
representations -X- _ O
into -X- _ O
fluent -X- _ O
texts -X- _ O
. -X- _ O

• -X- _ O
CommonGen -X- _ B-DatasetName
( -X- _ O
Lin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
requires -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
generate -X- _ O
a -X- _ O
coherent -X- _ O
and -X- _ O
reasonable -X- _ O
text -X- _ O
given -X- _ O
a -X- _ O
collection -X- _ O
of -X- _ O
common -X- _ O
concepts -X- _ O
. -X- _ O

• -X- _ O
SAMSum -X- _ B-DatasetName
( -X- _ O
Gliwa -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
is -X- _ O
a -X- _ O
dialogue -X- _ O
summarization -X- _ O
dataset -X- _ O
that -X- _ O
evaluates -X- _ O
the -X- _ O
model -X- _ O
's -X- _ O
summary -X- _ O
and -X- _ O
dialogue -X- _ O
understanding -X- _ O
abilities -X- _ O
. -X- _ O

• -X- _ O
ROCStories -X- _ B-DatasetName
( -X- _ O
Mostafazadeh -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
consists -X- _ O
of -X- _ O
five -X- _ O
- -X- _ O
sentence -X- _ O
stories -X- _ O
, -X- _ O
and -X- _ O
we -X- _ O
utilize -X- _ O
the -X- _ O
first -X- _ O
sentence -X- _ O
as -X- _ O
input -X- _ O
to -X- _ O
generate -X- _ O
the -X- _ O
remaining -X- _ O
four -X- _ O
. -X- _ O

The -X- _ O
details -X- _ O
of -X- _ O
the -X- _ O
statistics -X- _ O
and -X- _ O
license -X- _ O
of -X- _ O
each -X- _ O
dataset -X- _ O
are -X- _ O
listed -X- _ O
in -X- _ O
Table -X- _ O
1 -X- _ O
. -X- _ O
For -X- _ O
each -X- _ O
dataset -X- _ O
, -X- _ O
we -X- _ O
utilize -X- _ O
NLTK -X- _ O
1 -X- _ O
to -X- _ O
tokenize -X- _ O
the -X- _ O
input -X- _ O
texts -X- _ O
into -X- _ O
sentences -X- _ O
, -X- _ O
except -X- _ O
that -X- _ O
we -X- _ O
treat -X- _ O
each -X- _ O
key -X- _ O
- -X- _ O
value -X- _ O
pair -X- _ O
in -X- _ O
the -X- _ O
input -X- _ O
as -X- _ O
a -X- _ O
sentence -X- _ O
for -X- _ O
the -X- _ O
E2E -X- _ B-DatasetName
dataset -X- _ O
. -X- _ O

We -X- _ O
adopt -X- _ O
five -X- _ O
automatic -X- _ O
metrics -X- _ O
, -X- _ O
namely -X- _ O
BLEU -X- _ B-MetricName
( -X- _ O
Papineni -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2002 -X- _ O
) -X- _ O
, -X- _ O
ROUGE -X- _ B-MetricName
( -X- _ O
Lin -X- _ O
, -X- _ O
2004 -X- _ O
) -X- _ O
, -X- _ O
CIDEr -X- _ B-MetricName
( -X- _ O
Vedantam -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
, -X- _ O
SPICE -X- _ B-MetricName
( -X- _ O
Anderson -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
Distinct -X- _ B-MetricName
( -X- _ O
Li -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
, -X- _ O
to -X- _ O
compare -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
different -X- _ O
methods -X- _ O
. -X- _ O
BLEU -X- _ B-MetricName
, -X- _ O
ROUGE -X- _ B-MetricName
, -X- _ O
and -X- _ O
CIDEr -X- _ B-MetricName
compute -X- _ O
the -X- _ O
n -X- _ O
- -X- _ O
gram -X- _ O
overlap -X- _ O
between -X- _ O
the -X- _ O
candidate -X- _ O
text -X- _ O
and -X- _ O
the -X- _ O
reference -X- _ O
text -X- _ O
( -X- _ O
s -X- _ O
) -X- _ O
. -X- _ O
SPICE -X- _ B-MetricName
further -X- _ O
takes -X- _ O
semantic -X- _ O
meaning -X- _ O
into -X- _ O
consideration -X- _ O
. -X- _ O
Distinct -X- _ O
mainly -X- _ O
evaluates -X- _ O
the -X- _ O
diversity -X- _ O
of -X- _ O
the -X- _ O
generated -X- _ O
texts -X- _ O
and -X- _ O
is -X- _ O
always -X- _ O
used -X- _ O
in -X- _ O
open -X- _ B-TaskName
- -X- _ I-TaskName
ended -X- _ I-TaskName
generation -X- _ I-TaskName
tasks -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
story -X- _ B-TaskName
generation -X- _ I-TaskName
. -X- _ O
We -X- _ O
also -X- _ O
conduct -X- _ O
the -X- _ O
human -X- _ B-MetricName
evaluation -X- _ I-MetricName
in -X- _ O
Section -X- _ O
5.5 -X- _ O
. -X- _ O

We -X- _ O
utilize -X- _ O
two -X- _ O
commonly -X- _ O
used -X- _ O
text -X- _ B-TaskName
generation -X- _ I-TaskName
PLMs -X- _ I-TaskName
, -X- _ O
BART -X- _ B-TaskName
( -X- _ O
Lewis -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
and -X- _ O
T5 -X- _ B-TaskName
( -X- _ O
Raffel -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
, -X- _ O
as -X- _ O
text -X- _ O
- -X- _ O
only -X- _ O
baselines -X- _ O
. -X- _ O
We -X- _ O
further -X- _ O
compare -X- _ O
them -X- _ O
to -X- _ O
two -X- _ O
multimodal -X- _ B-TaskName
VLP -X- _ I-TaskName
models -X- _ O
: -X- _ O

• -X- _ O
BLIP -X- _ O
) -X- _ O
uses -X- _ O
a -X- _ O
multimodal -X- _ O
mixture -X- _ O
of -X- _ O
encoder -X- _ O
- -X- _ O
decoder -X- _ O
with -X- _ O
the -X- _ O
objectives -X- _ O
of -X- _ O
textimage -X- _ B-TaskName
contrast -X- _ I-TaskName
, -X- _ I-TaskName
text -X- _ I-TaskName
- -X- _ I-TaskName
image -X- _ I-TaskName
matching -X- _ I-TaskName
, -X- _ I-TaskName
and -X- _ I-TaskName
language -X- _ I-TaskName
modeling -X- _ I-TaskName
on -X- _ O
bootstrapped -X- _ O
text -X- _ O
- -X- _ O
image -X- _ O
pairs -X- _ O
. -X- _ O

• -X- _ O
OFA -X- _ B-TaskName
( -X- _ O
Wang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022a -X- _ O
) -X- _ O
unifies -X- _ O
text -X- _ O
and -X- _ O
image -X- _ O
modalities -X- _ O
using -X- _ O
a -X- _ O
unified -X- _ O
architecture -X- _ O
and -X- _ O
multi -X- _ B-TaskName
- -X- _ I-TaskName
task -X- _ I-TaskName
sequence -X- _ I-TaskName
- -X- _ I-TaskName
to -X- _ I-TaskName
- -X- _ I-TaskName
sequence -X- _ I-TaskName
learning -X- _ I-TaskName
. -X- _ O
In -X- _ O
addition -X- _ O
, -X- _ O
we -X- _ O
consider -X- _ O
a -X- _ O
variant -X- _ O
and -X- _ O
attempt -X- _ O
to -X- _ O
use -X- _ O
OFA -X- _ O
with -X- _ O
only -X- _ O
text -X- _ O
, -X- _ O
denoted -X- _ O
by -X- _ O
OFA -X- _ O
w -X- _ O
/ -X- _ O
o -X- _ O
image -X- _ O
. -X- _ O

We -X- _ O
integrate -X- _ O
our -X- _ O
LIVE -X- _ B-MethodName
framework -X- _ O
with -X- _ O
BART -X- _ B-MethodName
and -X- _ O
T5 -X- _ B-MethodName
, -X- _ O
and -X- _ O
consider -X- _ O
the -X- _ O
following -X- _ O
visuallyaugmented -X- _ O
methods -X- _ O
as -X- _ O
comparisons -X- _ O
: -X- _ O

• -X- _ O
VL -X- _ O
adds -X- _ O
visual -X- _ O
embeddings -X- _ O
for -X- _ O
the -X- _ O
original -X- _ O
BART -X- _ B-MethodName
and -X- _ O
T5 -X- _ B-MethodName
and -X- _ O
conducts -X- _ O
continued -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
on -X- _ O
text -X- _ O
- -X- _ O
image -X- _ O
pairs -X- _ O
. -X- _ O

E2E -X- _ B-DatasetName

CommonGen -X- _ B-DatasetName
SAMSum -X- _ B-DatasetName
ROCStories -X- _ B-DatasetName
Since -X- _ O
iNLG -X- _ O
does -X- _ O
not -X- _ O
offer -X- _ O
a -X- _ O
T5 -X- _ B-MethodName
version -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
only -X- _ O
combine -X- _ O
it -X- _ O
with -X- _ O
BART -X- _ B-MethodName
for -X- _ O
comparison -X- _ O
. -X- _ O

For -X- _ O
all -X- _ O
baselines -X- _ O
, -X- _ O
we -X- _ O
utilize -X- _ O
the -X- _ O
base -X- _ O
versions -X- _ O
of -X- _ O
PLMs -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
BART -X- _ B-MethodName
BASE -X- _ I-MethodName
, -X- _ I-MethodName
T5 -X- _ I-MethodName
BASE -X- _ I-MethodName
, -X- _ I-MethodName
BLIP -X- _ I-MethodName
BASE -X- _ I-MethodName
, -X- _ I-MethodName
and -X- _ I-MethodName
OFA -X- _ I-MethodName
BASE -X- _ I-MethodName
, -X- _ O
which -X- _ O
have -X- _ O
a -X- _ O
comparable -X- _ O
number -X- _ O
of -X- _ O
parameters -X- _ O
to -X- _ O
ensure -X- _ O
a -X- _ O
fair -X- _ O
comparison -X- _ O
. -X- _ O
For -X- _ O
BLIP -X- _ B-MethodName
, -X- _ I-MethodName
OFA -X- _ I-MethodName
, -X- _ I-MethodName
VL -X- _ I-MethodName
- -X- _ I-MethodName
BART -X- _ I-MethodName
, -X- _ I-MethodName
and -X- _ I-MethodName
VL -X- _ I-MethodName
- -X- _ I-MethodName
T5 -X- _ I-MethodName
, -X- _ I-MethodName
we -X- _ O
provide -X- _ O
the -X- _ O
same -X- _ O
synthesized -X- _ O
image -X- _ O
as -X- _ O
our -X- _ O
method -X- _ O
, -X- _ O
and -X- _ O
we -X- _ O
fine -X- _ O
- -X- _ O
tune -X- _ O
them -X- _ O
similarly -X- _ O
to -X- _ O
how -X- _ O
they -X- _ O
perform -X- _ O
VQA -X- _ B-TaskName
tasks -X- _ O
. -X- _ O

For -X- _ O
iNLG -X- _ O
, -X- _ O
we -X- _ O
utilize -X- _ O
its -X- _ O
official -X- _ O
implementation -X- _ O
2 -X- _ O
. -X- _ O
As -X- _ O
for -X- _ O
our -X- _ O
method -X- _ O
, -X- _ O
we -X- _ O
employ -X- _ O
Stable -X- _ O
Diffusion -X- _ O
v1.4 -X- _ O
with -X- _ O
half -X- _ O
precision -X- _ O
3 -X- _ O
to -X- _ O
synthesize -X- _ O
images -X- _ O
in -X- _ O
25 -X- _ O
timesteps -X- _ O
for -X- _ O
efficiency -X- _ O
. -X- _ O
Then -X- _ O
, -X- _ O
we -X- _ O
adopt -X- _ O
CLIP -X- _ O
- -X- _ O
ViT -X- _ O
- -X- _ O
B -X- _ O
/ -X- _ O
32 -X- _ O
to -X- _ O
judge -X- _ O
the -X- _ O
similarity -X- _ B-MetricName
between -X- _ O
text -X- _ O
- -X- _ O
image -X- _ O
pairs -X- _ O
and -X- _ O
extract -X- _ O
image -X- _ O
features -X- _ O
. -X- _ O
We -X- _ O
empirically -X- _ O
set -X- _ O
the -X- _ O
threshold -X- _ B-HyperparameterName
value -X- _ I-HyperparameterName
θ -X- _ I-HyperparameterName
= -X- _ O
0.27 -X- _ B-HyperparameterValue
. -X- _ O
After -X- _ O
extraction -X- _ O
, -X- _ O
an -X- _ O
MLP -X- _ O
layer -X- _ O
is -X- _ O
appended -X- _ O
to -X- _ O
project -X- _ O
the -X- _ O
image -X- _ O
representation -X- _ O
into -X- _ O
the -X- _ O
text -X- _ O
space -X- _ O
and -X- _ O
obtain -X- _ O
an -X- _ O
image -X- _ O
representation -X- _ O
I -X- _ O
i -X- _ O
∈ -X- _ O
R -X- _ O
50×768 -X- _ O
. -X- _ O
The -X- _ O
aforementioned -X- _ O
operations -X- _ O
can -X- _ O
be -X- _ O
performed -X- _ O
offline -X- _ O
for -X- _ O
efficiency -X- _ O
. -X- _ O

In -X- _ O
the -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
stage -X- _ O
of -X- _ O
our -X- _ O
fusion -X- _ O
layer -X- _ O
, -X- _ O
we -X- _ O
mask -X- _ O
50 -X- _ B-HyperparameterValue
% -X- _ I-HyperparameterValue
of -X- _ O
the -X- _ O
input -X- _ O
text -X- _ O
with -X- _ O
span -X- _ O
lengths -X- _ O
drawn -X- _ O
from -X- _ O
a -X- _ O
Poisson -X- _ B-HyperparameterName
distribution -X- _ I-HyperparameterName
with -X- _ O
λ -X- _ O
= -X- _ O
3.5 -X- _ B-HyperparameterValue
for -X- _ O
BART -X- _ B-MethodName
and -X- _ O
force -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
recover -X- _ O
the -X- _ O
input -X- _ O
with -X- _ O
the -X- _ O
image -X- _ O
. -X- _ O
As -X- _ O
for -X- _ O
T5 -X- _ B-HyperparameterValue
, -X- _ O
we -X- _ O
split -X- _ O
the -X- _ O
caption -X- _ O
into -X- _ O
two -X- _ O
parts -X- _ O
and -X- _ O
train -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
generate -X- _ O
the -X- _ O
second -X- _ O
part -X- _ O
using -X- _ O
the -X- _ O
first -X- _ O
part -X- _ O
and -X- _ O
the -X- _ O
image -X- _ O
. -X- _ O
We -X- _ O
pretrain -X- _ O
the -X- _ O
fusion -X- _ O
layer -X- _ O
with -X- _ O
a -X- _ O
batch -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
of -X- _ O
384 -X- _ B-HyperparameterValue
, -X- _ O
optimize -X- _ O
BART -X- _ O
using -X- _ O
AdamW -X- _ O
( -X- _ O
Loshchilov -X- _ O
and -X- _ O
Hutter -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
with -X- _ O
a -X- _ O
constant -X- _ O
learning -X- _ B-HyperparameterName
rate -X- _ I-HyperparameterName
of -X- _ O
1×10 -X- _ B-HyperparameterValue
−5 -X- _ I-HyperparameterValue
, -X- _ O
and -X- _ O
optimize -X- _ O
T5 -X- _ O
using -X- _ O
Adafactor -X- _ O
( -X- _ O
Shazeer -X- _ O
and -X- _ O
Stern -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
with -X- _ O
a -X- _ O
learning -X- _ B-HyperparameterName
rate -X- _ I-HyperparameterName
of -X- _ O
1 -X- _ B-HyperparameterValue
× -X- _ I-HyperparameterValue
10 -X- _ I-HyperparameterValue
−3 -X- _ I-HyperparameterValue
. -X- _ O

In -X- _ O
the -X- _ O
fine -X- _ O
- -X- _ O
tuning -X- _ O
stage -X- _ O
, -X- _ O
we -X- _ O
tune -X- _ O
the -X- _ O
entire -X- _ O
model -X- _ O
, -X- _ O
including -X- _ O
the -X- _ O
PLM -X- _ O
backbone -X- _ O
and -X- _ O
the -X- _ O
fusion -X- _ O
layer -X- _ O
. -X- _ O
We -X- _ O
set -X- _ O
the -X- _ O
batch -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
to -X- _ O
32 -X- _ B-HyperparameterValue
and -X- _ O
employ -X- _ O
the -X- _ O
same -X- _ O
optimizer -X- _ O
and -X- _ O
learning -X- _ O
rate -X- _ O
as -X- _ O
in -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
. -X- _ O
We -X- _ O
optimize -X- _ O
the -X- _ O
model -X- _ O
using -X- _ O
crossentropy -X- _ B-MetricName
sequence -X- _ I-MetricName
- -X- _ I-MetricName
to -X- _ I-MetricName
- -X- _ I-MetricName
sequence -X- _ I-MetricName
loss -X- _ I-MetricName
with -X- _ O
a -X- _ O
label -X- _ B-HyperparameterName
smoothing -X- _ I-HyperparameterName
factor -X- _ O
( -X- _ O
Szegedy -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
of -X- _ O
0.1 -X- _ B-HyperparameterValue
. -X- _ O
During -X- _ O
inference -X- _ O
, -X- _ O
we -X- _ O
choose -X- _ O
the -X- _ O
checkpoint -X- _ O
with -X- _ O
the -X- _ O
highest -X- _ O
validation -X- _ B-MetricName
metric -X- _ O
score -X- _ O
for -X- _ O
generation -X- _ O
. -X- _ O
During -X- _ O
generation -X- _ O
, -X- _ O
we -X- _ O
apply -X- _ O
beam -X- _ O
search -X- _ O
with -X- _ O
a -X- _ O
beam -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
of -X- _ O
5 -X- _ B-HyperparameterValue
for -X- _ O
E2E -X- _ B-TaskName
, -X- _ I-TaskName
CommonGen -X- _ I-TaskName
, -X- _ I-TaskName
and -X- _ I-TaskName
SAMSum -X- _ I-TaskName
, -X- _ O
while -X- _ O
utilizing -X- _ O
the -X- _ O
nucleus -X- _ O
sampling -X- _ O
with -X- _ O
p -X- _ O
= -X- _ O
0.9 -X- _ B-HyperparameterValue
and -X- _ O
t -X- _ O
= -X- _ O
0.7 -X- _ B-HyperparameterValue
for -X- _ O
ROCStories -X- _ O
. -X- _ O

All -X- _ O
the -X- _ O
experiments -X- _ O
are -X- _ O
conducted -X- _ O
using -X- _ O
the -X- _ O
text -X- _ O
generation -X- _ O
library -X- _ O
TextBox -X- _ B-DatasetName
( -X- _ O
Tang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
on -X- _ O
NVIDIA -X- _ O
GeForce -X- _ O
RTX -X- _ O
3090 -X- _ O
24 -X- _ O
GB -X- _ O
GPUs -X- _ O
using -X- _ O
Ubuntu -X- _ O
20.04.1 -X- _ O
SMP -X- _ O
. -X- _ O
All -X- _ O
these -X- _ O
hyper -X- _ O
- -X- _ O
parameters -X- _ O
are -X- _ O
identical -X- _ O
for -X- _ O
our -X- _ O
method -X- _ O
and -X- _ O
baselines -X- _ O
. -X- _ O

Firstly -X- _ O
, -X- _ O
the -X- _ O
results -X- _ O
of -X- _ O
multimodal -X- _ O
models -X- _ O
( -X- _ O
i.e. -X- _ O
, -X- _ O
BLIP -X- _ B-MethodName
and -X- _ O
OFA -X- _ B-MethodName
) -X- _ O
can -X- _ O
not -X- _ O
achieve -X- _ O
satisfactory -X- _ O
results -X- _ O
when -X- _ O
compared -X- _ O
with -X- _ O
text -X- _ O
- -X- _ O
only -X- _ O
models -X- _ O
( -X- _ O
i.e. -X- _ O
, -X- _ O
BART -X- _ B-MethodName
and -X- _ O
T5 -X- _ B-MethodName
) -X- _ O
on -X- _ O
pure -X- _ O
text -X- _ O
tasks -X- _ O
. -X- _ O
This -X- _ O
finding -X- _ O
further -X- _ O
proves -X- _ O
the -X- _ O
existence -X- _ O
of -X- _ O
semantic -X- _ O
disparity -X- _ O
( -X- _ O
Tan -X- _ O
and -X- _ O
Bansal -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
across -X- _ O
modalities -X- _ O
of -X- _ O
generation -X- _ O
tasks -X- _ O
. -X- _ O
OFA -X- _ B-MethodName
without -X- _ O
images -X- _ O
even -X- _ O
outperforms -X- _ O
OFA -X- _ B-MethodName
with -X- _ O
images -X- _ O
slightly -X- _ O
, -X- _ O
which -X- _ O
indicates -X- _ O
that -X- _ O
images -X- _ O
may -X- _ O
be -X- _ O
a -X- _ O
burden -X- _ O
for -X- _ O
text -X- _ B-TaskName
generation -X- _ I-TaskName
tasks -X- _ O
when -X- _ O
the -X- _ O
fusion -X- _ O
method -X- _ O
is -X- _ O
not -X- _ O
appropriate -X- _ O
. -X- _ O

Secondly -X- _ O
, -X- _ O
the -X- _ O
visually -X- _ O
- -X- _ O
augmented -X- _ O
methods -X- _ O
( -X- _ O
i.e. -X- _ O
, -X- _ O
VL -X- _ B-MethodName
- -X- _ I-MethodName
BART -X- _ I-MethodName
, -X- _ O
VL -X- _ B-MethodName
- -X- _ I-MethodName
T5 -X- _ I-MethodName
, -X- _ O
and -X- _ O
iNLG -X- _ B-MethodName
) -X- _ O
can -X- _ O
achieve -X- _ O
superior -X- _ O
performance -X- _ O
than -X- _ O
their -X- _ O
base -X- _ O
PLMs -X- _ O
on -X- _ O
certain -X- _ O
tasks -X- _ O
but -X- _ O
can -X- _ O
not -X- _ O
achieve -X- _ O
overall -X- _ O
improvement -X- _ O
on -X- _ O
all -X- _ O
tasks -X- _ O
. -X- _ O
A -X- _ O
major -X- _ O
reason -X- _ O
might -X- _ O
be -X- _ O
that -X- _ O
they -X- _ O
synthesize -X- _ O
only -X- _ O
one -X- _ O
image -X- _ O
for -X- _ O
each -X- _ O
input -X- _ O
without -X- _ O
considering -X- _ O
its -X- _ O
relevance -X- _ O
and -X- _ O
sentence -X- _ O
- -X- _ O
level -X- _ O
semantics -X- _ O
. -X- _ O

Finally -X- _ O
, -X- _ O
our -X- _ O
LIVE -X- _ B-MethodName
method -X- _ O
can -X- _ O
outperform -X- _ O
all -X- _ O
baselines -X- _ O
on -X- _ O
all -X- _ O
four -X- _ O
text -X- _ B-TaskName
generation -X- _ I-TaskName
tasks -X- _ O
. -X- _ O
Equipping -X- _ O
BART -X- _ B-MethodName
with -X- _ O
our -X- _ O
LIVE -X- _ B-MethodName
method -X- _ O
, -X- _ O
LIVE -X- _ B-MethodName
- -X- _ I-MethodName
BART -X- _ I-MethodName
can -X- _ O
outperform -X- _ O
its -X- _ O
text -X- _ O
- -X- _ O
only -X- _ O
counterpart -X- _ O
BART -X- _ O
by -X- _ O
2.80 -X- _ B-MetricValue
% -X- _ I-MetricValue
in -X- _ O
ratio -X- _ O
. -X- _ O
LIVE -X- _ B-MethodName
can -X- _ O
also -X- _ O
work -X- _ O
with -X- _ O
T5 -X- _ B-MethodName
, -X- _ O
yielding -X- _ O
an -X- _ O
average -X- _ O
improvement -X- _ O
of -X- _ O
2.08 -X- _ B-MetricValue
% -X- _ I-MetricValue
. -X- _ O
These -X- _ O
automatic -X- _ O
results -X- _ O
demonstrate -X- _ O
the -X- _ O
effectiveness -X- _ O
and -X- _ O
compatibility -X- _ O
of -X- _ O
our -X- _ O
text -X- _ O
- -X- _ O
related -X- _ O
image -X- _ O
generation -X- _ O
approach -X- _ O
and -X- _ O
plug -X- _ O
- -X- _ O
and -X- _ O
- -X- _ O
play -X- _ O
fusion -X- _ O
layer -X- _ O
. -X- _ O

We -X- _ O
investigate -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
our -X- _ O
LIVE -X- _ O
methods -X- _ O
in -X- _ O
a -X- _ O
low -X- _ O
- -X- _ O
resource -X- _ O
situation -X- _ O
. -X- _ O
We -X- _ O
keep -X- _ O
0.1 -X- _ B-HyperparameterValue
% -X- _ I-HyperparameterValue
, -X- _ I-HyperparameterValue
0.3 -X- _ I-HyperparameterValue
% -X- _ I-HyperparameterValue
, -X- _ I-HyperparameterValue
1 -X- _ I-HyperparameterValue
% -X- _ I-HyperparameterValue
, -X- _ I-HyperparameterValue
and -X- _ I-HyperparameterValue
3 -X- _ I-HyperparameterValue
% -X- _ I-HyperparameterValue
of -X- _ O
the -X- _ O
training -X- _ B-HyperparameterName
set -X- _ I-HyperparameterName
for -X- _ O
the -X- _ O
E2E -X- _ O
dataset -X- _ O
. -X- _ O
For -X- _ O
each -X- _ O
split -X- _ B-HyperparameterName
, -X- _ O
we -X- _ O
choose -X- _ O
five -X- _ O
independent -X- _ O
groups -X- _ O
to -X- _ O
decrease -X- _ O
the -X- _ O
randomness -X- _ O
. -X- _ O
From -X- _ O
the -X- _ O
results -X- _ O
in -X- _ O
Table -X- _ O
3 -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
observe -X- _ O
that -X- _ O
our -X- _ O
methods -X- _ O
remarkably -X- _ O
boost -X- _ O
the -X- _ O
performance -X- _ O
under -X- _ O
few -X- _ O
- -X- _ O
shot -X- _ O
settings -X- _ O
compared -X- _ O
with -X- _ O
baselines -X- _ O
, -X- _ O
especially -X- _ O
in -X- _ O
extreme -X- _ O
situations -X- _ O
( -X- _ O
0.1 -X- _ O
% -X- _ O
and -X- _ O
0.3 -X- _ O
% -X- _ O
) -X- _ O
. -X- _ O
We -X- _ O
assume -X- _ O
that -X- _ O
synthesized -X- _ O
images -X- _ O
can -X- _ O
provide -X- _ O
visual -X- _ O
knowledge -X- _ O
as -X- _ O
a -X- _ O
supplement -X- _ O
when -X- _ O
training -X- _ O
data -X- _ O
is -X- _ O
scarce -X- _ O
. -X- _ O

Model -X- _ O
Sensitivity -X- _ O
w.r.t -X- _ O
. -X- _ O
the -X- _ O
Similarity -X- _ B-MetricName
Threshold -X- _ I-MetricName
Value -X- _ I-MetricName
θ -X- _ I-MetricName

In -X- _ O
Section -X- _ O
3.2 -X- _ O
, -X- _ O
we -X- _ O
set -X- _ O
a -X- _ O
threshold -X- _ B-MetricName
value -X- _ I-MetricName
θ -X- _ I-MetricName
to -X- _ O
measure -X- _ O
the -X- _ O
text -X- _ B-MetricName
visuality -X- _ I-MetricName
. -X- _ O
Here -X- _ O
, -X- _ O
we -X- _ O
investigate -X- _ O
the -X- _ O
model -X- _ O
's -X- _ O
performance -X- _ O
when -X- _ O
θ -X- _ O
varies -X- _ O
. -X- _ O
If -X- _ O
θ -X- _ B-MetricName
= -X- _ O
0 -X- _ B-MetricValue
, -X- _ O
all -X- _ O
the -X- _ O
sentences -X- _ O
will -X- _ O
be -X- _ O
visually -X- _ O
- -X- _ O
augmented -X- _ O
. -X- _ O
If -X- _ O
θ -X- _ B-MetricName
= -X- _ O
1 -X- _ B-MetricValue
, -X- _ O
all -X- _ O
the -X- _ O
sentences -X- _ O
will -X- _ O
not -X- _ O
be -X- _ O
visually -X- _ O
- -X- _ O
augmented -X- _ O
, -X- _ O
and -X- _ O
it -X- _ O
degenerates -X- _ O
to -X- _ O
text -X- _ O
- -X- _ O
only -X- _ O
BART -X- _ O
. -X- _ O
As -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
2 -X- _ O
, -X- _ O
LIVE -X- _ O
- -X- _ O
BART -X- _ O
with -X- _ O
θ -X- _ B-MetricName
= -X- _ O
0.27 -X- _ B-MetricValue
achieves -X- _ O
the -X- _ O
best -X- _ O
performance -X- _ O
, -X- _ O
and -X- _ O
we -X- _ O
find -X- _ O
that -X- _ O
0.27 -X- _ B-MetricValue
is -X- _ O
close -X- _ O
to -X- _ O
the -X- _ O
median -X- _ O
of -X- _ O
text -X- _ O
visuality -X- _ O
scores -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
nearly -X- _ O
half -X- _ O
of -X- _ O
the -X- _ O
sentences -X- _ O
will -X- _ O
be -X- _ O
augmented -X- _ O
and -X- _ O
the -X- _ O
others -X- _ O
will -X- _ O
not -X- _ O
be -X- _ O
. -X- _ O
Therefore -X- _ O
, -X- _ O
we -X- _ O
set -X- _ O
θ -X- _ B-MetricName
= -X- _ O
0.27 -X- _ B-MetricValue
for -X- _ O
our -X- _ O
LIVE -X- _ O
methods -X- _ O
in -X- _ O
experiments -X- _ O
. -X- _ O
In -X- _ O
this -X- _ O
subsection -X- _ O
, -X- _ O
we -X- _ O
first -X- _ O
demonstrate -X- _ O
that -X- _ O
visual -X- _ O
information -X- _ O
is -X- _ O
truly -X- _ O
favorable -X- _ O
for -X- _ O
text -X- _ O
generation -X- _ O
. -X- _ O
Following -X- _ O
the -X- _ O
previous -X- _ O
works -X- _ O
, -X- _ O
we -X- _ O
replace -X- _ O
the -X- _ O
image -X- _ O
representations -X- _ O
with -X- _ O
random -X- _ O
noise -X- _ O
or -X- _ O
utilize -X- _ O
the -X- _ O
input -X- _ O
text -X- _ O
as -X- _ O
a -X- _ O
negative -X- _ O
prompt -X- _ O
to -X- _ O
synthesize -X- _ O
irrelevant -X- _ O
images -X- _ O
. -X- _ O
The -X- _ O
results -X- _ O
in -X- _ O
Figure -X- _ O
3 -X- _ O
further -X- _ O
prove -X- _ O
the -X- _ O
necessity -X- _ O
of -X- _ O
visual -X- _ O
knowledge -X- _ O
for -X- _ O
text -X- _ O
generation -X- _ O
. -X- _ O
Moreover -X- _ O
, -X- _ O
we -X- _ O
vary -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
diffusion -X- _ O
steps -X- _ O
since -X- _ O
it -X- _ O
is -X- _ O
a -X- _ O
trade -X- _ O
- -X- _ O
off -X- _ O
between -X- _ O
synthesis -X- _ O
quality -X- _ O
and -X- _ O
efficiency -X- _ O
. -X- _ O
Surprisingly -X- _ O
, -X- _ O
increasing -X- _ O
the -X- _ O
diffusion -X- _ O
steps -X- _ O
will -X- _ O
not -X- _ O
lead -X- _ O
to -X- _ O
performance -X- _ O
gains -X- _ O
. -X- _ O
We -X- _ O
speculate -X- _ O
that -X- _ O
diffusion -X- _ O
with -X- _ O
certain -X- _ O
steps -X- _ O
can -X- _ O
provide -X- _ O
enough -X- _ O
visual -X- _ O
knowledge -X- _ O
for -X- _ O
the -X- _ O
PLM -X- _ O
, -X- _ O
and -X- _ O
more -X- _ O
steps -X- _ O
may -X- _ O
just -X- _ O
help -X- _ O
to -X- _ O
achieve -X- _ O
higher -X- _ O
resolution -X- _ O
. -X- _ O
Thus -X- _ O
, -X- _ O
we -X- _ O
only -X- _ O
synthesize -X- _ O
for -X- _ O
25 -X- _ O
steps -X- _ O
considering -X- _ O
the -X- _ O
efficiency -X- _ O
. -X- _ O

Human -X- _ B-MethodName
Evaluation -X- _ I-MethodName

Considering -X- _ O
that -X- _ O
the -X- _ O
automatic -X- _ O
evaluation -X- _ O
may -X- _ O
be -X- _ O
inconsistent -X- _ O
with -X- _ O
human -X- _ O
judgments -X- _ O
, -X- _ O
we -X- _ O
further -X- _ O
invite -X- _ O
five -X- _ O
college -X- _ O
students -X- _ O
to -X- _ O
assess -X- _ O
the -X- _ O
generated -X- _ O
texts -X- _ O
. -X- _ O
We -X- _ O
randomly -X- _ O
choose -X- _ O
100 -X- _ O
samples -X- _ O
from -X- _ O
the -X- _ O
test -X- _ O
set -X- _ O
of -X- _ O
each -X- _ O
dataset -X- _ O
and -X- _ O
showcase -X- _ O
the -X- _ O
generated -X- _ O
texts -X- _ O
of -X- _ O
both -X- _ O
BART -X- _ O
and -X- _ O
LIVE -X- _ O
- -X- _ O
BART -X- _ O
. -X- _ O
The -X- _ O
annotators -X- _ B-MethodName
should -X- _ O
choose -X- _ O
which -X- _ O
one -X- _ O
is -X- _ O
better -X- _ O
or -X- _ O
choose -X- _ O
a -X- _ O
tie -X- _ O
based -X- _ O
on -X- _ O
their -X- _ O
subjective -X- _ O
feelings -X- _ O
. -X- _ O
From -X- _ O
the -X- _ O
results -X- _ O
in -X- _ O
Table -X- _ O
6 -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
observe -X- _ O
that -X- _ O
our -X- _ O
LIVE -X- _ O
method -X- _ O
can -X- _ O
make -X- _ O
BART -X- _ O
generate -X- _ O
more -X- _ O
satisfactory -X- _ O
texts -X- _ O
in -X- _ O
all -X- _ O
tasks -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
present -X- _ O
the -X- _ O
LIVE -X- _ B-MethodName
method -X- _ O
for -X- _ O
natural -X- _ O
language -X- _ O
generation -X- _ O
. -X- _ O
First -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
an -X- _ O
imagination -X- _ O
- -X- _ O
based -X- _ O
method -X- _ O
, -X- _ O
imitating -X- _ O
the -X- _ O
process -X- _ O
of -X- _ O
human -X- _ O
writing -X- _ O
. -X- _ O
It -X- _ O
is -X- _ O
a -X- _ O
relevant -X- _ O
, -X- _ O
selective -X- _ O
, -X- _ O
and -X- _ O
dynamic -X- _ O
approach -X- _ O
that -X- _ O
leverages -X- _ O
Stable -X- _ O
Diffusion -X- _ O
to -X- _ O
synthesize -X- _ O
images -X- _ O
for -X- _ O
each -X- _ O
input -X- _ O
sentence -X- _ O
and -X- _ O
discard -X- _ O
the -X- _ O
images -X- _ O
with -X- _ O
lower -X- _ O
text -X- _ O
visuality -X- _ O
computed -X- _ O
by -X- _ O
CLIP -X- _ O
. -X- _ O
Furthermore -X- _ O
, -X- _ O
we -X- _ O
introduce -X- _ O
a -X- _ O
plug -X- _ O
- -X- _ O
andplay -X- _ O
vision -X- _ O
- -X- _ O
text -X- _ O
fusion -X- _ O
layer -X- _ O
to -X- _ O
deeply -X- _ O
incorporate -X- _ O
visual -X- _ O
knowledge -X- _ O
into -X- _ O
PLMs -X- _ O
and -X- _ O
obtain -X- _ O
visuallyaugmented -X- _ O
text -X- _ O
representations -X- _ O
for -X- _ O
text -X- _ O
generation -X- _ O
. -X- _ O

Extensive -X- _ O
experiments -X- _ O
have -X- _ O
demonstrated -X- _ O
that -X- _ O
our -X- _ O
LIVE -X- _ B-MethodName
methods -X- _ O
are -X- _ O
compatible -X- _ O
with -X- _ O
two -X- _ O
PLMs -X- _ O
( -X- _ O
i.e. -X- _ O
, -X- _ O
BART -X- _ B-MethodName
and -X- _ O
T5 -X- _ B-MethodName
) -X- _ O
and -X- _ O
can -X- _ O
achieve -X- _ O
superior -X- _ O
performance -X- _ O
over -X- _ O
all -X- _ O
the -X- _ O
baseline -X- _ O
models -X- _ O
. -X- _ O

We -X- _ O
only -X- _ O
conduct -X- _ O
experiments -X- _ O
on -X- _ O
four -X- _ O
natural -X- _ B-TaskName
language -X- _ I-TaskName
generation -X- _ I-TaskName
tasks -X- _ O
without -X- _ O
considering -X- _ O
the -X- _ O
expandability -X- _ O
to -X- _ O
more -X- _ O
NLP -X- _ O
tasks -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
language -X- _ O
understanding -X- _ O
or -X- _ O
reasoning -X- _ O
. -X- _ O
It -X- _ O
is -X- _ O
also -X- _ O
meaningful -X- _ O
to -X- _ O
investigate -X- _ O
the -X- _ O
robustness -X- _ O
of -X- _ O
our -X- _ O
methods -X- _ O
with -X- _ O
different -X- _ O
text -X- _ O
formats -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
text -X- _ O
length -X- _ O
and -X- _ O
literary -X- _ O
form -X- _ O
) -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
examine -X- _ O
which -X- _ O
situations -X- _ O
and -X- _ O
why -X- _ O
our -X- _ O
methods -X- _ O
can -X- _ O
achieve -X- _ O
better -X- _ O
performance -X- _ O
. -X- _ O
Due -X- _ O
to -X- _ O
the -X- _ O
limitation -X- _ O
of -X- _ O
computing -X- _ O
power -X- _ O
, -X- _ O
we -X- _ O
do -X- _ O
not -X- _ O
explore -X- _ O
the -X- _ O
effectiveness -X- _ O
of -X- _ O
our -X- _ O
methods -X- _ O
under -X- _ O
different -X- _ O
PLMs -X- _ O
with -X- _ O
various -X- _ O
scales -X- _ O
. -X- _ O
Besides -X- _ O
, -X- _ O
we -X- _ O
utilize -X- _ O
CLIP -X- _ O
to -X- _ O
evaluate -X- _ O
the -X- _ O
text -X- _ O
visuality -X- _ O
and -X- _ O
encode -X- _ O
images -X- _ O
into -X- _ O
representations -X- _ O
, -X- _ O
and -X- _ O
this -X- _ O
is -X- _ O
also -X- _ O
interesting -X- _ O
to -X- _ O
research -X- _ O
which -X- _ O
vision -X- _ O
encoder -X- _ O
has -X- _ O
higher -X- _ O
suitability -X- _ O
with -X- _ O
PLMs -X- _ O
. -X- _ O

A -X- _ O
Computational -X- _ O
Acquisition -X- _ O
Model -X- _ O
for -X- _ O
Multimodal -X- _ B-TaskName
Word -X- _ I-TaskName
Categorization -X- _ I-TaskName

Recent -X- _ O
advances -X- _ O
in -X- _ O
self -X- _ O
- -X- _ O
supervised -X- _ O
modeling -X- _ O
of -X- _ O
text -X- _ O
and -X- _ O
images -X- _ O
open -X- _ O
new -X- _ O
opportunities -X- _ O
for -X- _ O
computational -X- _ O
models -X- _ O
of -X- _ O
child -X- _ O
language -X- _ O
acquisition -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
believed -X- _ O
to -X- _ O
rely -X- _ O
heavily -X- _ O
on -X- _ O
cross -X- _ O
- -X- _ O
modal -X- _ O
signals -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
prior -X- _ O
studies -X- _ O
have -X- _ O
been -X- _ O
limited -X- _ O
by -X- _ O
their -X- _ O
reliance -X- _ O
on -X- _ O
vision -X- _ O
models -X- _ O
trained -X- _ O
on -X- _ O
large -X- _ O
image -X- _ O
datasets -X- _ O
annotated -X- _ O
with -X- _ O
a -X- _ O
pre -X- _ O
- -X- _ O
defined -X- _ O
set -X- _ O
of -X- _ O
depicted -X- _ O
object -X- _ O
categories -X- _ O
. -X- _ O
This -X- _ O
is -X- _ O
( -X- _ O
a -X- _ O
) -X- _ O
not -X- _ O
faithful -X- _ O
to -X- _ O
the -X- _ O
information -X- _ O
children -X- _ O
receive -X- _ O
and -X- _ O
( -X- _ O
b -X- _ O
) -X- _ O
prohibits -X- _ O
the -X- _ O
evaluation -X- _ O
of -X- _ O
such -X- _ O
models -X- _ O
with -X- _ O
respect -X- _ O
to -X- _ O
category -X- _ O
learning -X- _ O
tasks -X- _ O
, -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
pre -X- _ O
- -X- _ O
imposed -X- _ O
category -X- _ O
structure -X- _ O
. -X- _ O
We -X- _ O
address -X- _ O
this -X- _ O
gap -X- _ O
, -X- _ O
and -X- _ O
present -X- _ O
a -X- _ O
cognitively -X- _ B-MethodName
- -X- _ I-MethodName
inspired -X- _ I-MethodName
, -X- _ I-MethodName
multimodal -X- _ I-MethodName
acquisition -X- _ I-MethodName
model -X- _ I-MethodName
, -X- _ O
trained -X- _ O
from -X- _ O
image -X- _ O
- -X- _ O
caption -X- _ O
pairs -X- _ O
on -X- _ O
naturalistic -X- _ O
data -X- _ O
using -X- _ O
cross -X- _ O
- -X- _ O
modal -X- _ O
self -X- _ O
- -X- _ O
supervision -X- _ O
. -X- _ O
We -X- _ O
show -X- _ O
that -X- _ O
the -X- _ O
model -X- _ O
learns -X- _ O
word -X- _ O
categories -X- _ O
and -X- _ O
object -X- _ O
recognition -X- _ O
abilities -X- _ O
, -X- _ O
and -X- _ O
presents -X- _ O
trends -X- _ O
reminiscent -X- _ O
of -X- _ O
those -X- _ O
reported -X- _ O
in -X- _ O
the -X- _ O
developmental -X- _ O
literature -X- _ O
. -X- _ O
We -X- _ O
make -X- _ O
our -X- _ O
code -X- _ O
and -X- _ O
trained -X- _ O
models -X- _ O
public -X- _ O
for -X- _ O
future -X- _ O
reference -X- _ O
and -X- _ O
use -X- _ O
1 -X- _ O
. -X- _ O

Introduction -X- _ O

To -X- _ O
date -X- _ O
, -X- _ O
the -X- _ O
mechanisms -X- _ O
underlying -X- _ O
the -X- _ O
efficiency -X- _ O
with -X- _ O
which -X- _ O
infants -X- _ O
learn -X- _ O
to -X- _ O
speak -X- _ O
and -X- _ O
understand -X- _ O
natural -X- _ O
language -X- _ O
remain -X- _ O
an -X- _ O
open -X- _ O
research -X- _ O
question -X- _ O
. -X- _ O
Research -X- _ O
suggests -X- _ O
that -X- _ O
children -X- _ O
leverage -X- _ O
contextual -X- _ O
, -X- _ O
inter -X- _ O
- -X- _ O
personal -X- _ O
and -X- _ O
non -X- _ O
- -X- _ O
linguistic -X- _ O
information -X- _ O
. -X- _ O
Visual -X- _ O
input -X- _ O
is -X- _ O
a -X- _ O
case -X- _ O
in -X- _ O
point -X- _ O
: -X- _ O
when -X- _ O
spoken -X- _ O
to -X- _ O
, -X- _ O
infants -X- _ O
visually -X- _ O
perceive -X- _ O
their -X- _ O
environment -X- _ O
, -X- _ O
and -X- _ O
paired -X- _ O
with -X- _ O
the -X- _ O
input -X- _ O
speech -X- _ O
, -X- _ O
the -X- _ O
visual -X- _ O
environment -X- _ O
could -X- _ O
help -X- _ O
bootstrap -X- _ O
linguistic -X- _ O
knowledge -X- _ O
( -X- _ O
Tomasello -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
1996 -X- _ O
) -X- _ O
. -X- _ O
Unlike -X- _ O
social -X- _ O
cues -X- _ O
, -X- _ O
visual -X- _ O
input -X- _ O
has -X- _ O
a -X- _ O
natural -X- _ O
physical -X- _ O
representation -X- _ O
, -X- _ O
in -X- _ O
the -X- _ O
form -X- _ O
of -X- _ O
pixel -X- _ O
maps -X- _ O
or -X- _ O
videos -X- _ O
. -X- _ O

Previous -X- _ O
multimodal -X- _ B-TaskName
language -X- _ I-TaskName
acquisition -X- _ I-TaskName
studies -X- _ O
either -X- _ O
considered -X- _ O
toy -X- _ O
scenarios -X- _ O
with -X- _ O
small -X- _ O
vocabularies -X- _ O
( -X- _ O
Roy -X- _ O
and -X- _ O
Pentland -X- _ O
, -X- _ O
2002 -X- _ O
; -X- _ O
Frank -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2007 -X- _ O
) -X- _ O
, -X- _ O
or -X- _ O
used -X- _ O
visual -X- _ O
encoders -X- _ O
that -X- _ O
were -X- _ O
pretrained -X- _ O
1 -X- _ O
github.com -X- _ O
/ -X- _ O
SLAB-NLP -X- _ O
/ -X- _ O
multimodal_clustering -X- _ O
on -X- _ O
large -X- _ O
labeled -X- _ O
data -X- _ O
bases -X- _ O
such -X- _ O
as -X- _ O
ImageNet -X- _ O
( -X- _ O
Deng -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2009 -X- _ O
) -X- _ O
or -X- _ O
Visual -X- _ O
Genome -X- _ O
( -X- _ O
Krishna -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O
This -X- _ O
has -X- _ O
two -X- _ O
drawbacks -X- _ O
: -X- _ O
first -X- _ O
, -X- _ O
systematic -X- _ O
access -X- _ O
to -X- _ O
labeled -X- _ O
data -X- _ O
is -X- _ O
a -X- _ O
cognitively -X- _ O
implausible -X- _ O
assumption -X- _ O
in -X- _ O
a -X- _ O
language -X- _ O
acquisition -X- _ O
setting -X- _ O
; -X- _ O
second -X- _ O
, -X- _ O
imposing -X- _ O
a -X- _ O
pre -X- _ O
- -X- _ O
defined -X- _ O
categorization -X- _ O
system -X- _ O
precludes -X- _ O
studying -X- _ O
categories -X- _ O
that -X- _ O
emerge -X- _ O
when -X- _ O
learning -X- _ O
from -X- _ O
unlabeled -X- _ O
multimodal -X- _ O
data -X- _ O
. -X- _ O
This -X- _ O
type -X- _ O
of -X- _ O
setting -X- _ O
more -X- _ O
closely -X- _ O
resembles -X- _ O
the -X- _ O
data -X- _ O
underlying -X- _ O
early -X- _ O
language -X- _ O
learning -X- _ O
at -X- _ O
a -X- _ O
time -X- _ O
when -X- _ O
the -X- _ O
child -X- _ O
has -X- _ O
only -X- _ O
acquired -X- _ O
little -X- _ O
conceptual -X- _ O
information -X- _ O
. -X- _ O
Although -X- _ O
the -X- _ O
subject -X- _ O
of -X- _ O
much -X- _ O
psycholinguistic -X- _ O
work -X- _ O
, -X- _ O
the -X- _ O
computational -X- _ O
study -X- _ O
of -X- _ O
multimodal -X- _ O
word -X- _ O
categories -X- _ O
, -X- _ O
formed -X- _ O
without -X- _ O
recourse -X- _ O
to -X- _ O
manual -X- _ O
supervision -X- _ O
has -X- _ O
been -X- _ O
scarcely -X- _ O
addressed -X- _ O
in -X- _ O
previous -X- _ O
work -X- _ O
. -X- _ O

We -X- _ O
present -X- _ O
a -X- _ O
model -X- _ O
that -X- _ O
learns -X- _ O
categories -X- _ O
as -X- _ O
clus- -X- _ O
( -X- _ O
BERT -X- _ B-MethodName
base -X- _ O
, -X- _ O
word2vec -X- _ B-MethodName
( -X- _ O
300d -X- _ O
, -X- _ O
and -X- _ O
CLIP -X- _ B-MethodName
with -X- _ O
ResNet50X64 -X- _ O
) -X- _ O
against -X- _ O
our -X- _ O
model -X- _ O
( -X- _ O
Ours -X- _ O
, -X- _ O
bold -X- _ O
) -X- _ O
and -X- _ O
typical -X- _ O
child -X- _ O
input -X- _ O
( -X- _ O
Children -X- _ B-MethodName
; -X- _ O
Gilkerson -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O

For -X- _ O
the -X- _ O
multimodal -X- _ O
models -X- _ O
( -X- _ O
CLIP -X- _ B-MethodName
and -X- _ O
Ours -X- _ O
) -X- _ O
we -X- _ O
only -X- _ O
mention -X- _ O
the -X- _ O
size -X- _ O
of -X- _ O
the -X- _ O
text -X- _ O
encoder -X- _ O
. -X- _ O

ters -X- _ O
of -X- _ O
words -X- _ O
from -X- _ O
large -X- _ O
- -X- _ O
scale -X- _ O
, -X- _ O
naturalistic -X- _ O
multimodal -X- _ O
data -X- _ O
without -X- _ O
any -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
. -X- _ O
Given -X- _ O
( -X- _ O
image -X- _ O
, -X- _ O
caption -X- _ O
) -X- _ O
-pairs -X- _ O
as -X- _ O
input -X- _ O
, -X- _ O
the -X- _ O
model -X- _ O
is -X- _ O
trained -X- _ O
to -X- _ O
cluster -X- _ O
words -X- _ O
and -X- _ O
images -X- _ O
in -X- _ O
a -X- _ O
shared -X- _ O
, -X- _ O
latent -X- _ O
space -X- _ O
, -X- _ O
where -X- _ O
each -X- _ O
cluster -X- _ O
pertains -X- _ O
to -X- _ O
a -X- _ O
semantic -X- _ O
category -X- _ O
. -X- _ O

In -X- _ O
a -X- _ O
self -X- _ O
- -X- _ O
supervised -X- _ O
setup -X- _ O
, -X- _ O
a -X- _ O
neural -X- _ O
image -X- _ O
classifier -X- _ O
and -X- _ O
a -X- _ O
co -X- _ O
- -X- _ O
occurrence -X- _ O
based -X- _ O
word -X- _ O
clustering -X- _ O
module -X- _ O
provide -X- _ O
mutual -X- _ O
supervision -X- _ O
through -X- _ O
joint -X- _ O
training -X- _ O
with -X- _ O
an -X- _ O
expectation -X- _ O
- -X- _ O
maximization -X- _ O
( -X- _ O
EM -X- _ O
) -X- _ O
style -X- _ O
algorithm -X- _ O
. -X- _ O
Figure -X- _ O
1 -X- _ O
illustrates -X- _ O
our -X- _ O
model -X- _ O
. -X- _ O

Our -X- _ O
input -X- _ O
representation -X- _ O
is -X- _ O
cognitively -X- _ O
plausible -X- _ O
in -X- _ O
that -X- _ O
we -X- _ O
use -X- _ O
raw -X- _ O
image -X- _ O
data -X- _ O
( -X- _ O
pixels -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
train -X- _ O
our -X- _ O
model -X- _ O
on -X- _ O
a -X- _ O
comparatively -X- _ O
small -X- _ O
data -X- _ O
set -X- _ O
( -X- _ O
Table -X- _ O
1 -X- _ O
) -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
we -X- _ O
follow -X- _ O
previous -X- _ O
work -X- _ O
( -X- _ O
Kádár -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
; -X- _ O
Nikolaus -X- _ O
and -X- _ O
Fourtassi -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
in -X- _ O
using -X- _ O
written -X- _ O
and -X- _ O
word -X- _ O
- -X- _ O
segmented -X- _ O
text -X- _ O
input -X- _ O
. -X- _ O
Young -X- _ O
infants -X- _ O
do -X- _ O
not -X- _ O
have -X- _ O
access -X- _ O
to -X- _ O
such -X- _ O
structure -X- _ O
, -X- _ O
and -X- _ O
this -X- _ O
assumption -X- _ O
therefore -X- _ O
deviates -X- _ O
from -X- _ O
cognitive -X- _ O
plausibility -X- _ O
( -X- _ O
but -X- _ O
opens -X- _ O
avenues -X- _ O
for -X- _ O
future -X- _ O
work -X- _ O
) -X- _ O
. -X- _ O

We -X- _ O
show -X- _ O
that -X- _ O
semantic -X- _ O
and -X- _ O
visual -X- _ O
knowledge -X- _ O
emerges -X- _ O
when -X- _ O
training -X- _ O
on -X- _ O
the -X- _ O
self -X- _ O
- -X- _ O
supervised -X- _ O
categorization -X- _ B-TaskName
task -X- _ O
. -X- _ O
In -X- _ O
a -X- _ O
zero -X- _ O
- -X- _ O
shot -X- _ O
setup -X- _ O
, -X- _ O
we -X- _ O
evaluate -X- _ O
our -X- _ O
model -X- _ O
on -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
word -X- _ B-MetricName
concreteness -X- _ I-MetricName
prediction -X- _ I-MetricName
as -X- _ O
a -X- _ O
proxy -X- _ O
for -X- _ O
noun -X- _ O
identification -X- _ O
, -X- _ O
one -X- _ O
of -X- _ O
the -X- _ O
first -X- _ O
cues -X- _ O
for -X- _ O
syntax -X- _ O
in -X- _ O
infant -X- _ O
language -X- _ B-TaskName
acquisition -X- _ I-TaskName
( -X- _ O
Fisher -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
1994 -X- _ O
) -X- _ O
; -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
visual -X- _ B-MetricName
classification -X- _ I-MetricName
and -X- _ O
object -X- _ B-MetricName
segmentation -X- _ I-MetricName
. -X- _ O
We -X- _ O
also -X- _ O
study -X- _ O
the -X- _ O
emerging -X- _ O
latent -X- _ O
word -X- _ O
clusters -X- _ O
and -X- _ O
show -X- _ O
that -X- _ O
words -X- _ O
are -X- _ O
clustered -X- _ O
syntagmatically -X- _ O
( -X- _ O
De -X- _ O
Saussure -X- _ O
, -X- _ O
1916 -X- _ O
) -X- _ O
: -X- _ O
words -X- _ O
representing -X- _ O
entities -X- _ O
that -X- _ O
are -X- _ O
likely -X- _ O
to -X- _ O
occur -X- _ O
together -X- _ O
are -X- _ O
more -X- _ O
likely -X- _ O
clustered -X- _ O
together -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
dog -X- _ O
- -X- _ O
bark -X- _ O
) -X- _ O
, -X- _ O
than -X- _ O
words -X- _ O
that -X- _ O
share -X- _ O
taxonomic -X- _ O
categories -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
dogcat -X- _ O
) -X- _ O
. -X- _ O
This -X- _ O
concurs -X- _ O
with -X- _ O
findings -X- _ O
that -X- _ O
young -X- _ O
children -X- _ O
acquire -X- _ O
syntagmatic -X- _ O
categories -X- _ O
more -X- _ O
readily -X- _ O
than -X- _ O
taxonomic -X- _ O
categories -X- _ O
( -X- _ O
Sloutsky -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O

Background -X- _ O
and -X- _ O
Related -X- _ O
Work -X- _ O

We -X- _ O
briefly -X- _ O
review -X- _ O
previous -X- _ O
studies -X- _ O
of -X- _ O
unimodal -X- _ O
learning -X- _ O
without -X- _ O
pretraining -X- _ O
( -X- _ O
for -X- _ O
both -X- _ O
text -X- _ O
and -X- _ O
vision -X- _ O
) -X- _ O
and -X- _ O
multimodal -X- _ O
learning -X- _ O
( -X- _ O
studied -X- _ O
mainly -X- _ O
in -X- _ O
acquisition -X- _ O
implausible -X- _ O
settings -X- _ O
) -X- _ O
to -X- _ O
highlight -X- _ O
the -X- _ O
gap -X- _ O
that -X- _ O
this -X- _ O
work -X- _ O
addresses -X- _ O
. -X- _ O

Unimodal -X- _ O
Learning -X- _ O
. -X- _ O
Self -X- _ O
- -X- _ O
supervised -X- _ O
learning -X- _ O
without -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
has -X- _ O
been -X- _ O
extensively -X- _ O
studied -X- _ O
, -X- _ O
but -X- _ O
predominantly -X- _ O
in -X- _ O
a -X- _ O
unimodal -X- _ O
scenario -X- _ O
or -X- _ O
under -X- _ O
cognitively -X- _ O
implausible -X- _ O
assumptions -X- _ O
. -X- _ O
For -X- _ O
the -X- _ O
text -X- _ O
modality -X- _ O
, -X- _ O
large -X- _ O
language -X- _ O
models -X- _ O
have -X- _ O
been -X- _ O
developed -X- _ O
in -X- _ O
recent -X- _ O
years -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
BERT -X- _ O
; -X- _ O
Devlin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
trained -X- _ O
on -X- _ O
large -X- _ O
unlabeled -X- _ O
text -X- _ O
corpora -X- _ O
( -X- _ O
Table -X- _ O
1 -X- _ O
) -X- _ O
. -X- _ O
A -X- _ O
more -X- _ O
cognitively -X- _ O
motivated -X- _ O
model -X- _ O
is -X- _ O
BabyBERTa -X- _ O
( -X- _ O
Huebner -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
, -X- _ O
a -X- _ O
smaller -X- _ O
version -X- _ O
of -X- _ O
RoBERTa -X- _ O
( -X- _ O
Liu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
( -X- _ O
also -X- _ O
) -X- _ O
trained -X- _ O
on -X- _ O
transcribed -X- _ O
child -X- _ O
directed -X- _ O
speech -X- _ O
. -X- _ O
In -X- _ O
the -X- _ O
visual -X- _ O
domain -X- _ O
, -X- _ O
self -X- _ O
- -X- _ O
supervision -X- _ O
is -X- _ O
typically -X- _ O
implemented -X- _ O
as -X- _ O
contrastive -X- _ O
learning -X- _ O
, -X- _ O
training -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
align -X- _ O
corrupted -X- _ O
images -X- _ O
with -X- _ O
their -X- _ O
original -X- _ O
counterparts -X- _ O
( -X- _ O
Chen -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020a -X- _ O
) -X- _ O
, -X- _ O
with -X- _ O
subsequent -X- _ O
fine -X- _ O
- -X- _ O
tuning -X- _ O
. -X- _ O

Multimodal -X- _ O
Language -X- _ O
Learning -X- _ O
. -X- _ O
Early -X- _ O
language -X- _ O
acquisition -X- _ O
studies -X- _ O
( -X- _ O
Roy -X- _ O
and -X- _ O
Pentland -X- _ O
, -X- _ O
2002 -X- _ O
) -X- _ O
considered -X- _ O
toy -X- _ O
scenarios -X- _ O
with -X- _ O
small -X- _ O
vocabularies -X- _ O
and -X- _ O
used -X- _ O
heuristics -X- _ O
for -X- _ O
image -X- _ O
processing -X- _ O
. -X- _ O
Silberer -X- _ O
and -X- _ O
Lapata -X- _ O
( -X- _ O
2014 -X- _ O
) -X- _ O
model -X- _ O
multi -X- _ O
- -X- _ O
modal -X- _ O
human -X- _ O
categorization -X- _ O
using -X- _ O
human -X- _ O
- -X- _ O
annotated -X- _ O
feature -X- _ O
vectors -X- _ O
as -X- _ O
input -X- _ O
for -X- _ O
a -X- _ O
multimodal -X- _ O
self -X- _ O
- -X- _ O
supervised -X- _ O
autoencoder -X- _ O
, -X- _ O
while -X- _ O
we -X- _ O
learn -X- _ O
the -X- _ O
features -X- _ O
from -X- _ O
raw -X- _ O
images -X- _ O
. -X- _ O

Unlike -X- _ O
our -X- _ O
work -X- _ O
, -X- _ O
recent -X- _ O
work -X- _ O
on -X- _ O
cross -X- _ O
- -X- _ O
modal -X- _ O
language -X- _ O
learning -X- _ O
( -X- _ O
Kádár -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
; -X- _ O
Chrupała -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
; -X- _ O
Ororbia -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Nikolaus -X- _ O
and -X- _ O
Fourtassi -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
typically -X- _ O
use -X- _ O
Convolutional -X- _ O
Neural -X- _ O
Networks -X- _ O
, -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
on -X- _ O
large -X- _ O
labeled -X- _ O
data -X- _ O
bases -X- _ O
like -X- _ O
ImageNet -X- _ O
( -X- _ O
Deng -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2009 -X- _ O
) -X- _ O
, -X- _ O
or -X- _ O
alternatively -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
Lu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Chen -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020b -X- _ O
) -X- _ O
use -X- _ O
object -X- _ O
detectors -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
on -X- _ O
Visual -X- _ O
Genome -X- _ O
( -X- _ O
Krishna -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
as -X- _ O
the -X- _ O
visual -X- _ O
model -X- _ O
. -X- _ O

Few -X- _ O
studies -X- _ O
assume -X- _ O
no -X- _ O
prior -X- _ O
knowledge -X- _ O
of -X- _ O
the -X- _ O
grounded -X- _ O
modality -X- _ O
. -X- _ O
Most -X- _ O
related -X- _ O
to -X- _ O
our -X- _ O
study -X- _ O
is -X- _ O
CLIP -X- _ O
( -X- _ O
Radford -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
, -X- _ O
a -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
off -X- _ O
- -X- _ O
theshelf -X- _ O
model -X- _ O
trained -X- _ O
to -X- _ O
project -X- _ O
matching -X- _ O
images -X- _ O
and -X- _ O
captions -X- _ O
to -X- _ O
similar -X- _ O
vectors -X- _ O
. -X- _ O
CLIP -X- _ O
assumes -X- _ O
a -X- _ O
multimodal -X- _ O
joint -X- _ O
space -X- _ O
which -X- _ O
is -X- _ O
continuous -X- _ O
, -X- _ O
unlike -X- _ O
our -X- _ O
binary -X- _ O
space -X- _ O
. -X- _ O
Liu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
use -X- _ O
CLIP -X- _ O
pretrained -X- _ O
encoders -X- _ O
to -X- _ O
learn -X- _ O
cross -X- _ O
- -X- _ O
modal -X- _ O
representations -X- _ O
with -X- _ O
a -X- _ O
similar -X- _ O
training -X- _ O
objective -X- _ O
as -X- _ O
ours -X- _ O
. -X- _ O
They -X- _ O
discretize -X- _ O
the -X- _ O
output -X- _ O
of -X- _ O
the -X- _ O
encoders -X- _ O
by -X- _ O
mapping -X- _ O
it -X- _ O
to -X- _ O
the -X- _ O
closest -X- _ O
vector -X- _ O
from -X- _ O
a -X- _ O
finite -X- _ O
set -X- _ O
of -X- _ O
learned -X- _ O
vectors -X- _ O
, -X- _ O
which -X- _ O
can -X- _ O
be -X- _ O
viewed -X- _ O
as -X- _ O
a -X- _ O
form -X- _ O
of -X- _ O
categorization -X- _ O
. -X- _ O
CLIP -X- _ O
- -X- _ O
based -X- _ O
works -X- _ O
are -X- _ O
trained -X- _ O
to -X- _ O
match -X- _ O
entire -X- _ O
sentences -X- _ O
to -X- _ O
images -X- _ O
and -X- _ O
have -X- _ O
no -X- _ O
explicit -X- _ O
representation -X- _ O
of -X- _ O
words -X- _ O
and -X- _ O
phrases -X- _ O
. -X- _ O
We -X- _ O
therefore -X- _ O
view -X- _ O
it -X- _ O
as -X- _ O
a -X- _ O
cognitively -X- _ O
less -X- _ O
plausible -X- _ O
setting -X- _ O
than -X- _ O
is -X- _ O
presented -X- _ O
in -X- _ O
the -X- _ O
current -X- _ O
study -X- _ O
. -X- _ O
Nevertheless -X- _ O
, -X- _ O
we -X- _ O
include -X- _ O
CLIP -X- _ B-MethodName
as -X- _ O
a -X- _ O
point -X- _ O
of -X- _ O
comparison -X- _ O
when -X- _ O
applicable -X- _ O
. -X- _ O

Model -X- _ O

Our -X- _ O
goal -X- _ O
is -X- _ O
to -X- _ O
learn -X- _ O
the -X- _ O
meaning -X- _ O
of -X- _ O
words -X- _ O
and -X- _ O
raw -X- _ O
images -X- _ O
through -X- _ O
mutual -X- _ O
supervision -X- _ O
by -X- _ O
mapping -X- _ O
both -X- _ O
modalities -X- _ O
to -X- _ O
a -X- _ O
joint -X- _ O
representation -X- _ O
. -X- _ O
Intuitively -X- _ O
, -X- _ O
given -X- _ O
a -X- _ O
visual -X- _ O
input -X- _ O
paired -X- _ O
with -X- _ O
relevant -X- _ O
text -X- _ O
( -X- _ O
approximating -X- _ O
a -X- _ O
typical -X- _ O
learning -X- _ O
scenario -X- _ O
) -X- _ O
, -X- _ O
the -X- _ O
output -X- _ O
for -X- _ O
each -X- _ O
of -X- _ O
the -X- _ O
modalities -X- _ O
is -X- _ O
a -X- _ O
binary -X- _ O
vector -X- _ O
in -X- _ O
{ -X- _ O
0 -X- _ O
, -X- _ O
1 -X- _ O
} -X- _ O
N -X- _ O
, -X- _ O
with -X- _ O
non -X- _ O
- -X- _ O
zero -X- _ O
dimensions -X- _ O
indicating -X- _ O
the -X- _ O
clusters -X- _ O
2 -X- _ O
to -X- _ O
which -X- _ O
the -X- _ O
input -X- _ O
is -X- _ O
assigned -X- _ O
and -X- _ O
N -X- _ B-HyperparameterName
is -X- _ O
the -X- _ O
total -X- _ B-HyperparameterName
number -X- _ I-HyperparameterName
of -X- _ I-HyperparameterName
clusters -X- _ I-HyperparameterName
( -X- _ O
a -X- _ O
predefined -X- _ O
hyper -X- _ O
- -X- _ O
parameter -X- _ O
) -X- _ O
. -X- _ O
The -X- _ O
clusters -X- _ O
are -X- _ O
unknown -X- _ O
a -X- _ O
priori -X- _ O
and -X- _ O
are -X- _ O
formed -X- _ O
during -X- _ O
training -X- _ O
. -X- _ O
The -X- _ O
goal -X- _ O
is -X- _ O
to -X- _ O
assign -X- _ O
matching -X- _ O
text -X- _ O
and -X- _ O
image -X- _ O
to -X- _ O
the -X- _ O
same -X- _ O
clusters -X- _ O
. -X- _ O
In -X- _ O
order -X- _ O
to -X- _ O
minimize -X- _ O
assumptions -X- _ O
on -X- _ O
innate -X- _ O
knowledge -X- _ O
or -X- _ O
pre -X- _ O
- -X- _ O
imposed -X- _ O
categories -X- _ O
available -X- _ O
to -X- _ O
the -X- _ O
language -X- _ O
learner -X- _ O
, -X- _ O
and -X- _ O
enable -X- _ O
the -X- _ O
study -X- _ O
of -X- _ O
emerging -X- _ O
categories -X- _ O
from -X- _ O
large -X- _ O
- -X- _ O
scale -X- _ O
multi -X- _ O
- -X- _ O
modal -X- _ O
input -X- _ O
data -X- _ O
, -X- _ O
we -X- _ O
deliberately -X- _ O
avoid -X- _ O
any -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
of -X- _ O
our -X- _ O
models -X- _ O
. -X- _ O

Visual -X- _ O
Encoder -X- _ O

The -X- _ O
visual -X- _ O
encoder -X- _ O
( -X- _ O
Figure -X- _ O
1 -X- _ O
, -X- _ O
top -X- _ O
) -X- _ O
is -X- _ O
a -X- _ O
randomly -X- _ O
initialized -X- _ O
ResNet50 -X- _ O
( -X- _ O
He -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
, -X- _ O
without -X- _ O
pretraining -X- _ O
. -X- _ O
We -X- _ O
set -X- _ O
the -X- _ O
output -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
of -X- _ I-HyperparameterName
the -X- _ I-HyperparameterName
network -X- _ I-HyperparameterName
to -X- _ O
N -X- _ B-HyperparameterName
( -X- _ I-HyperparameterName
the -X- _ I-HyperparameterName
number -X- _ I-HyperparameterName
of -X- _ I-HyperparameterName
clusters -X- _ I-HyperparameterName
) -X- _ I-HyperparameterName
and -X- _ O
add -X- _ O
an -X- _ O
element -X- _ O
- -X- _ O
wise -X- _ O
sigmoid -X- _ O
layer -X- _ O
. -X- _ O
3 -X- _ O
To -X- _ O
produce -X- _ O
the -X- _ O
binary -X- _ O
output -X- _ O
and -X- _ O
predict -X- _ O
the -X- _ O
clusters -X- _ O
given -X- _ O
an -X- _ O
input -X- _ O
image -X- _ O
, -X- _ O
we -X- _ O
apply -X- _ O
a -X- _ O
hyper -X- _ O
- -X- _ O
parameter -X- _ O
threshold -X- _ B-HyperparameterName
θ -X- _ I-HyperparameterName
v -X- _ O
to -X- _ O
the -X- _ O
output -X- _ O
of -X- _ O
the -X- _ O
sigmoid -X- _ O
layer -X- _ O
. -X- _ O

Text -X- _ O
Encoder -X- _ O

The -X- _ O
text -X- _ O
encoder -X- _ O
( -X- _ O
Figure -X- _ O
1 -X- _ O
, -X- _ O
bottom -X- _ O
) -X- _ O
is -X- _ O
a -X- _ O
simple -X- _ O
probabilistic -X- _ O
model -X- _ O
based -X- _ O
on -X- _ O
word -X- _ O
- -X- _ O
cluster -X- _ O
cooccurrence -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
intuitively -X- _ O
interpretable -X- _ O
and -X- _ O
makes -X- _ O
minimal -X- _ O
structural -X- _ O
assumptions -X- _ O
. -X- _ O
Given -X- _ O
a -X- _ O
sentence -X- _ O
, -X- _ O
the -X- _ O
model -X- _ O
assigns -X- _ O
each -X- _ O
word -X- _ O
to -X- _ O
at -X- _ O
most -X- _ O
one -X- _ O
cluster -X- _ O
. -X- _ O
The -X- _ O
sentence -X- _ O
is -X- _ O
assigned -X- _ O
to -X- _ O
the -X- _ O
union -X- _ O
of -X- _ O
the -X- _ O
clusters -X- _ O
to -X- _ O
which -X- _ O
the -X- _ O
words -X- _ O
in -X- _ O
it -X- _ O
are -X- _ O
assigned -X- _ O
. -X- _ O

Formally -X- _ O
, -X- _ O
given -X- _ O
a -X- _ O
sentence -X- _ O
s= -X- _ O
( -X- _ O
w -X- _ O
1 -X- _ O
, -X- _ O
w -X- _ O
2 -X- _ O
, -X- _ O
... -X- _ O
, -X- _ O
w -X- _ O
n -X- _ O
) -X- _ O
of -X- _ O
words -X- _ O
w -X- _ O
i -X- _ O
, -X- _ O
and -X- _ O
an -X- _ O
assignment -X- _ O
of -X- _ O
the -X- _ O
words -X- _ O
to -X- _ O
clusters -X- _ O
f -X- _ O
: -X- _ O
{ -X- _ O
w -X- _ O
1 -X- _ O
, -X- _ O
... -X- _ O
, -X- _ O
w -X- _ O
n -X- _ O
} -X- _ O
→ -X- _ O
{ -X- _ O
1 -X- _ O
, -X- _ O
... -X- _ O
, -X- _ O
N -X- _ O
} -X- _ O
∪ -X- _ O
{ -X- _ O
∅ -X- _ O
} -X- _ O
, -X- _ O
the -X- _ O
clusters -X- _ O
to -X- _ O
which -X- _ O
the -X- _ O
sentence -X- _ O
is -X- _ O
assigned -X- _ O
are -X- _ O
: -X- _ O

c|if -X- _ O
∃w -X- _ O
i -X- _ O
s.t -X- _ O
. -X- _ O
f -X- _ O
( -X- _ O
w -X- _ O
i -X- _ O
) -X- _ O
= -X- _ O
c -X- _ O
N -X- _ O
c=1 -X- _ O
. -X- _ O

When -X- _ O
assigning -X- _ O
words -X- _ O
to -X- _ O
clusters -X- _ O
, -X- _ O
we -X- _ O
make -X- _ O
two -X- _ O
simplifying -X- _ O
assumptions -X- _ O
: -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
the -X- _ O
probability -X- _ O
that -X- _ O
a -X- _ O
word -X- _ O
is -X- _ O
assigned -X- _ O
to -X- _ O
a -X- _ O
specific -X- _ O
cluster -X- _ O
is -X- _ O
independent -X- _ O
of -X- _ O
the -X- _ O
linguistic -X- _ O
context -X- _ O
, -X- _ O
meaning -X- _ O
that -X- _ O
we -X- _ O
assign -X- _ O
to -X- _ O
clusters -X- _ O
on -X- _ O
the -X- _ O
type -X- _ O
- -X- _ O
rather -X- _ O
than -X- _ O
the -X- _ O
token -X- _ O
level -X- _ O
( -X- _ O
a -X- _ O
reasonable -X- _ O
assumption -X- _ O
given -X- _ O
that -X- _ O
children -X- _ O
learn -X- _ O
single -X- _ O
words -X- _ O
first -X- _ O
) -X- _ O
; -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
a -X- _ O
single -X- _ O
word -X- _ O
can -X- _ O
not -X- _ O
be -X- _ O
assigned -X- _ O
to -X- _ O
more -X- _ O
than -X- _ O
one -X- _ O
cluster -X- _ O
, -X- _ O
but -X- _ O
it -X- _ O
might -X- _ O
be -X- _ O
assigned -X- _ O
to -X- _ O
no -X- _ O
cluster -X- _ O
at -X- _ O
all -X- _ O
if -X- _ O
it -X- _ O
does -X- _ O
not -X- _ O
have -X- _ O
a -X- _ O
visual -X- _ O
correspondent -X- _ O
in -X- _ O
the -X- _ O
image -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
function -X- _ O
words -X- _ O
) -X- _ O
. -X- _ O
Under -X- _ O
these -X- _ O
assumptions -X- _ O
, -X- _ O
the -X- _ O
encoder -X- _ O
estimates -X- _ O
P -X- _ O
( -X- _ O
c|w -X- _ O
) -X- _ O
for -X- _ O
each -X- _ O
c -X- _ O
∈ -X- _ O
{ -X- _ O
1 -X- _ O
, -X- _ O
. -X- _ O
. -X- _ O
. -X- _ O
, -X- _ O
N -X- _ O
} -X- _ O
and -X- _ O
for -X- _ O
each -X- _ O
w -X- _ O
∈ -X- _ O
V -X- _ O
, -X- _ O
where -X- _ O
V -X- _ O
is -X- _ O
the -X- _ O
vocabulary -X- _ O
. -X- _ O
If -X- _ O
the -X- _ O
probability -X- _ O
of -X- _ O
assigning -X- _ O
a -X- _ O
given -X- _ O
word -X- _ O
in -X- _ O
a -X- _ O
sentence -X- _ O
to -X- _ O
any -X- _ O
of -X- _ O
the -X- _ O
clusters -X- _ O
exceeds -X- _ O
a -X- _ O
hyper -X- _ B-HyperparameterName
- -X- _ I-HyperparameterName
parameter -X- _ I-HyperparameterName
threshold -X- _ I-HyperparameterName
θ -X- _ I-HyperparameterName
t -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
assigned -X- _ O
to -X- _ O
the -X- _ O
cluster -X- _ O
with -X- _ O
the -X- _ O
highest -X- _ O
probability -X- _ O
, -X- _ O
otherwise -X- _ O
it -X- _ O
is -X- _ O
not -X- _ O
assigned -X- _ O
to -X- _ O
any -X- _ O
cluster -X- _ O
. -X- _ O
Formally -X- _ O
: -X- _ O

We -X- _ O
trained -X- _ O
our -X- _ O
model -X- _ O
on -X- _ O
the -X- _ O
2014 -X- _ O
split -X- _ B-HyperparameterName
of -X- _ O
MSCOCO -X- _ B-DatasetName
( -X- _ O
Lin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2014 -X- _ O
) -X- _ O
, -X- _ O
a -X- _ O
dataset -X- _ O
of -X- _ O
naturalistic -X- _ O
images -X- _ O
with -X- _ O
one -X- _ O
or -X- _ O
more -X- _ O
corresponding -X- _ O
captions -X- _ O
, -X- _ O
where -X- _ O
each -X- _ O
image -X- _ O
is -X- _ O
labeled -X- _ O
with -X- _ O
a -X- _ O
list -X- _ O
of -X- _ O
object -X- _ O
classes -X- _ O
it -X- _ O
depicts -X- _ O
. -X- _ O
MSCOCO -X- _ B-DatasetName
has -X- _ O
80 -X- _ O
object -X- _ O
classes -X- _ O
, -X- _ O
123 -X- _ O
K -X- _ O
images -X- _ O
and -X- _ O
616 -X- _ O
K -X- _ O
captions -X- _ O
( -X- _ O
split -X- _ O
into -X- _ O
67 -X- _ O
% -X- _ O
train -X- _ O
, -X- _ O
33 -X- _ O
% -X- _ O
test -X- _ O
) -X- _ O
. -X- _ O
We -X- _ O
filtered -X- _ O
out -X- _ O
images -X- _ O
that -X- _ O
did -X- _ O
not -X- _ O
contain -X- _ O
any -X- _ O
labeled -X- _ O
objects -X- _ O
, -X- _ O
and -X- _ O
images -X- _ O
that -X- _ O
contained -X- _ O
objects -X- _ O
with -X- _ O
a -X- _ O
multi -X- _ O
- -X- _ O
token -X- _ O
label -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
" -X- _ O
fire -X- _ O
hydrant -X- _ O
" -X- _ O
) -X- _ O
. -X- _ O
5 -X- _ O
After -X- _ O
filtering -X- _ O
, -X- _ O
we -X- _ O
are -X- _ O
left -X- _ O
with -X- _ O
65 -X- _ O
ground -X- _ O
- -X- _ O
truth -X- _ O
classes -X- _ O
. -X- _ O
The -X- _ O
filtered -X- _ O
training -X- _ O
( -X- _ O
test -X- _ O
) -X- _ O
set -X- _ O
contains -X- _ O
56 -X- _ O
K -X- _ O
( -X- _ O
27 -X- _ O
K -X- _ O
) -X- _ O
images -X- _ O
and -X- _ O
279 -X- _ O
K -X- _ O
( -X- _ O
137 -X- _ O
K -X- _ O
) -X- _ O
captions -X- _ O
. -X- _ O
We -X- _ O
set -X- _ O
apart -X- _ O
20 -X- _ B-HyperparameterValue
% -X- _ I-HyperparameterValue
of -X- _ O
the -X- _ O
training -X- _ B-HyperparameterName
set -X- _ I-HyperparameterName
for -X- _ O
hyper -X- _ O
- -X- _ O
parameter -X- _ O
tuning -X- _ O
. -X- _ O

We -X- _ O
trained -X- _ O
our -X- _ O
model -X- _ O
with -X- _ O
a -X- _ O
batch -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
of -X- _ O
50 -X- _ B-HyperparameterValue
until -X- _ O
we -X- _ O
observed -X- _ O
no -X- _ O
improvement -X- _ O
in -X- _ O
the -X- _ O
F -X- _ B-MetricName
- -X- _ O
score -X- _ O
measure -X- _ O
from -X- _ O
Section -X- _ O
5.1 -X- _ O
( -X- _ O
40 -X- _ B-HyperparameterValue
epochs -X- _ B-HyperparameterName
) -X- _ O
. -X- _ O
Training -X- _ O
took -X- _ O
4 -X- _ O
days -X- _ O
on -X- _ O
a -X- _ O
single -X- _ O
GM204GL -X- _ O
GPU -X- _ O
. -X- _ O
We -X- _ O
used -X- _ O
N -X- _ B-HyperparameterName
= -X- _ O
150 -X- _ B-HyperparameterValue
clusters -X- _ O
, -X- _ O
θ -X- _ B-HyperparameterName
t -X- _ I-HyperparameterName
= -X- _ O
0.08 -X- _ B-HyperparameterValue
, -X- _ O
and -X- _ O
θ -X- _ B-HyperparameterName
v -X- _ I-HyperparameterName
= -X- _ O
0.5 -X- _ B-HyperparameterValue
. -X- _ O
The -X- _ O
visual -X- _ B-HyperparameterName
threshold -X- _ I-HyperparameterName
θ -X- _ I-HyperparameterName
v -X- _ I-HyperparameterName
was -X- _ O
first -X- _ O
set -X- _ O
heuristically -X- _ O
to -X- _ O
0.5 -X- _ B-HyperparameterValue
to -X- _ O
avoid -X- _ O
degenerate -X- _ O
solutions -X- _ O
( -X- _ O
images -X- _ O
being -X- _ O
assigned -X- _ O
to -X- _ O
all -X- _ O
or -X- _ O
no -X- _ O
clusters -X- _ O
initially -X- _ O
) -X- _ O
. -X- _ O
Then -X- _ O
, -X- _ O
N -X- _ B-HyperparameterName
and -X- _ O
θ -X- _ B-HyperparameterName
t -X- _ I-HyperparameterName
were -X- _ O
determined -X- _ O
in -X- _ O
a -X- _ O
grid -X- _ O
search -X- _ O
, -X- _ O
optimizing -X- _ O
the -X- _ O
F -X- _ B-MetricName
- -X- _ O
score -X- _ O
measure -X- _ O
from -X- _ O
Section -X- _ O
5.1 -X- _ O
. -X- _ O
We -X- _ O
used -X- _ O
spaCy -X- _ O
( -X- _ O
Honnibal -X- _ O
and -X- _ O
Montani -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
for -X- _ O
tokenization -X- _ O
. -X- _ O

We -X- _ O
evaluated -X- _ O
induced -X- _ O
clusters -X- _ O
against -X- _ O
a -X- _ O
taxonomic -X- _ O
and -X- _ O
a -X- _ O
syntagmatic -X- _ O
reference -X- _ O
data -X- _ O
set -X- _ O
. -X- _ O
First -X- _ O
, -X- _ O
we -X- _ O
followed -X- _ O
Silberer -X- _ O
and -X- _ O
Lapata -X- _ O
( -X- _ O
2014 -X- _ O
) -X- _ O
, -X- _ O
used -X- _ O
the -X- _ O
categorization -X- _ B-DatasetName
dataset -X- _ I-DatasetName
from -X- _ I-DatasetName
Fountain -X- _ I-DatasetName
and -X- _ I-DatasetName
Lapata -X- _ I-DatasetName
( -X- _ O
2010 -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
transformed -X- _ O
the -X- _ O
dataset -X- _ O
into -X- _ O
hard -X- _ O
categories -X- _ O
by -X- _ O
assigning -X- _ O
each -X- _ O
noun -X- _ O
to -X- _ O
its -X- _ O
most -X- _ O
typical -X- _ O
category -X- _ O
as -X- _ O
extrapolated -X- _ O
from -X- _ O
human -X- _ O
typicality -X- _ O
ratings -X- _ O
. -X- _ O
The -X- _ O
resulting -X- _ O
dataset -X- _ O
contains -X- _ O
516 -X- _ O
words -X- _ O
grouped -X- _ O
into -X- _ O
41 -X- _ O
taxononmic -X- _ O
categories -X- _ O
. -X- _ O
We -X- _ O
filtered -X- _ O
the -X- _ O
dataset -X- _ O
to -X- _ O
contain -X- _ O
only -X- _ O
words -X- _ O
that -X- _ O
occur -X- _ O
in -X- _ O
the -X- _ O
MSCOCO -X- _ B-DatasetName
training -X- _ O
set -X- _ O
and -X- _ O
in -X- _ O
the -X- _ O
word2vec -X- _ B-DatasetName
( -X- _ O
Mikolov -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2013 -X- _ O
) -X- _ O
dictionary -X- _ O
, -X- _ O
obtaining -X- _ O
the -X- _ O
final -X- _ O
dataset -X- _ O
with -X- _ O
444 -X- _ O
words -X- _ O
grouped -X- _ O
into -X- _ O
41 -X- _ O
categories -X- _ O
. -X- _ O

In -X- _ O
order -X- _ O
to -X- _ O
quantify -X- _ O
the -X- _ O
syntagmatic -X- _ O
nature -X- _ O
of -X- _ O
the -X- _ O
induced -X- _ O
clusters -X- _ O
, -X- _ O
we -X- _ O
used -X- _ O
a -X- _ O
large -X- _ O
dataset -X- _ O
of -X- _ O
human -X- _ O
word -X- _ O
associations -X- _ O
, -X- _ O
the -X- _ O
" -X- _ O
Small -X- _ B-DatasetName
World -X- _ I-DatasetName
of -X- _ I-DatasetName
Words -X- _ I-DatasetName
" -X- _ O
( -X- _ O
SWOW -X- _ B-DatasetName
, -X- _ O
De -X- _ O
Deyne -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
. -X- _ O
SWOW -X- _ B-DatasetName
was -X- _ O
compiled -X- _ O
by -X- _ O
presenting -X- _ O
a -X- _ O
cue -X- _ O
word -X- _ O
to -X- _ O
human -X- _ O
participants -X- _ O
and -X- _ O
requesting -X- _ O
them -X- _ O
to -X- _ O
respond -X- _ O
with -X- _ O
the -X- _ O
first -X- _ O
three -X- _ O
words -X- _ O
that -X- _ O
came -X- _ O
to -X- _ O
mind -X- _ O
. -X- _ O
The -X- _ O
association -X- _ O
strength -X- _ O
of -X- _ O
a -X- _ O
pair -X- _ O
of -X- _ O
words -X- _ O
( -X- _ O
w -X- _ O
1 -X- _ O
, -X- _ O
w -X- _ O
2 -X- _ O
) -X- _ O
is -X- _ O
determined -X- _ O
by -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
participants -X- _ O
who -X- _ O
responded -X- _ O
with -X- _ O
w -X- _ O
2 -X- _ O
to -X- _ O
cue -X- _ O
word -X- _ O
w -X- _ O
1 -X- _ O
. -X- _ O
Prior -X- _ O
work -X- _ O
has -X- _ O
shown -X- _ O
that -X- _ O
word -X- _ O
associations -X- _ O
are -X- _ O
to -X- _ O
a -X- _ O
large -X- _ O
extent -X- _ O
driven -X- _ O
by -X- _ O
syntagmatic -X- _ O
relations -X- _ O
( -X- _ O
Santos -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2011 -X- _ O
) -X- _ O
. -X- _ O

Comparison -X- _ O
with -X- _ O
other -X- _ O
models -X- _ O
. -X- _ O
We -X- _ O
compare -X- _ O
against -X- _ O
several -X- _ O
word -X- _ O
embedding -X- _ O
models -X- _ O
, -X- _ O
6 -X- _ O
where -X- _ O
for -X- _ O
each -X- _ O
model -X- _ O
we -X- _ O
first -X- _ O
induce -X- _ O
embeddings -X- _ O
, -X- _ O
which -X- _ O
we -X- _ O
then -X- _ O
cluster -X- _ O
into -X- _ O
K=41 -X- _ O
clusters -X- _ O
( -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
taxonomic -X- _ O
gold -X- _ O
classes -X- _ O
) -X- _ O
using -X- _ O
K -X- _ O
- -X- _ O
Means -X- _ O
. -X- _ O
We -X- _ O
compare -X- _ O
against -X- _ O
a -X- _ O
text -X- _ O
- -X- _ O
only -X- _ O
variant -X- _ O
of -X- _ O
our -X- _ O
model -X- _ O
7 -X- _ O
by -X- _ O
creating -X- _ O
a -X- _ O
co -X- _ O
- -X- _ O
occurrence -X- _ O
matrix -X- _ O
C -X- _ O
where -X- _ O
C -X- _ O
i -X- _ O
, -X- _ O
j -X- _ O
is -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
captions -X- _ O
in -X- _ O
which -X- _ O
tokens -X- _ O
i -X- _ O
, -X- _ O
j -X- _ O
in -X- _ O
the -X- _ O
vocabulary -X- _ O
co -X- _ O
- -X- _ O
occur -X- _ O
. -X- _ O
The -X- _ O
normalized -X- _ O
rows -X- _ O
of -X- _ O
C -X- _ O
are -X- _ O
the -X- _ O
vector -X- _ O
embeddings -X- _ O
of -X- _ O
words -X- _ O
in -X- _ O
the -X- _ O
vocabulary -X- _ O
. -X- _ O
We -X- _ O
compare -X- _ O
against -X- _ O
off -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
shelf -X- _ O
word2vec -X- _ B-MethodName
and -X- _ O
BERT -X- _ B-MethodName
BASE -X- _ I-MethodName
embeddings -X- _ O
. -X- _ O
For -X- _ O
BERT -X- _ B-MethodName
, -X- _ O
given -X- _ O
a -X- _ O
word -X- _ O
w -X- _ O
, -X- _ O
we -X- _ O
feed -X- _ O
an -X- _ O
artificial -X- _ O
context -X- _ O
( -X- _ O
" -X- _ O
this -X- _ O
is -X- _ O
a -X- _ O
w -X- _ O
" -X- _ O
) -X- _ O
and -X- _ O
take -X- _ O
the -X- _ O
embedding -X- _ O
of -X- _ O
the -X- _ O
first -X- _ O
subword -X- _ O
of -X- _ O
w. -X- _ O
We -X- _ O
also -X- _ O
include -X- _ O
the -X- _ O
multi -X- _ O
- -X- _ O
modal -X- _ O
CLIP -X- _ O
, -X- _ O
using -X- _ O
prompts -X- _ O
as -X- _ O
suggested -X- _ O
in -X- _ O
the -X- _ O
original -X- _ O
paper -X- _ O
( -X- _ O
" -X- _ O
a -X- _ O
photo -X- _ O
of -X- _ O
a -X- _ O
w -X- _ O
" -X- _ O
) -X- _ O
. -X- _ O
8 -X- _ O
Finally -X- _ O
, -X- _ O
we -X- _ O
include -X- _ O
a -X- _ O
randomized -X- _ O
baseline -X- _ O
, -X- _ O
which -X- _ O
assigns -X- _ O
each -X- _ O
word -X- _ O
at -X- _ O
random -X- _ O
to -X- _ O
one -X- _ O
of -X- _ O
41 -X- _ O
clusters -X- _ O
. -X- _ O
Implementation -X- _ O
details -X- _ O
can -X- _ O
be -X- _ O
found -X- _ O
in -X- _ O
Appendix -X- _ O
A.1 -X- _ O
. -X- _ O
Taxonomic -X- _ O
categorization -X- _ O
. -X- _ O
We -X- _ O
use -X- _ O
the -X- _ O
F -X- _ B-MetricName
- -X- _ O
score -X- _ O
metric -X- _ O
following -X- _ O
Silberer -X- _ O
and -X- _ O
Lapata -X- _ O
( -X- _ O
2014 -X- _ O
) -X- _ O
. -X- _ O
The -X- _ O
Fvalue -X- _ B-MetricName
of -X- _ O
a -X- _ O
( -X- _ O
gold -X- _ O
class -X- _ O
, -X- _ O
cluster -X- _ O
) -X- _ O
-pair -X- _ O
is -X- _ O
the -X- _ O
harmonic -X- _ O
mean -X- _ O
of -X- _ O
precision -X- _ O
and -X- _ O
recall -X- _ O
defined -X- _ O
as -X- _ O
the -X- _ O
size -X- _ O
of -X- _ O
intersection -X- _ O
divided -X- _ O
by -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
items -X- _ O
in -X- _ O
the -X- _ O
cluster -X- _ O
and -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
items -X- _ O
in -X- _ O
the -X- _ O
class -X- _ O
, -X- _ O
respectively -X- _ O
. -X- _ O
The -X- _ O
F -X- _ B-MetricName
- -X- _ O
score -X- _ O
of -X- _ O
a -X- _ O
class -X- _ O
is -X- _ O
the -X- _ O
maximum -X- _ O
F -X- _ O
- -X- _ O
value -X- _ O
attained -X- _ O
at -X- _ O
any -X- _ O
cluster -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
F -X- _ B-MetricName
- -X- _ O
score -X- _ O
of -X- _ O
the -X- _ O
entire -X- _ O
clustering -X- _ O
is -X- _ O
the -X- _ O
size -X- _ O
- -X- _ O
weighted -X- _ O
sum -X- _ O
of -X- _ O
F -X- _ B-MetricName
- -X- _ O
scores -X- _ O
of -X- _ O
all -X- _ O
classes -X- _ O
. -X- _ O
We -X- _ O
report -X- _ O
performance -X- _ O
over -X- _ O
five -X- _ O
random -X- _ O
restarts -X- _ O
for -X- _ O
all -X- _ O
models -X- _ O
. -X- _ O

Results -X- _ O
are -X- _ O
presented -X- _ O
in -X- _ O
Table -X- _ O
2 -X- _ O
. -X- _ O
The -X- _ O
text -X- _ B-MethodName
- -X- _ I-MethodName
only -X- _ I-MethodName
baseline -X- _ O
improves -X- _ O
results -X- _ O
over -X- _ O
a -X- _ O
random -X- _ O
categorization -X- _ O
algorithm -X- _ O
. -X- _ O
Our -X- _ O
multi -X- _ O
- -X- _ O
modal -X- _ O
model -X- _ O
grounded -X- _ O
in -X- _ O
visual -X- _ O
input -X- _ O
improves -X- _ O
over -X- _ O
its -X- _ O
unimodal -X- _ O
variant -X- _ O
. -X- _ O
Our -X- _ O
model -X- _ O
is -X- _ O
competitive -X- _ O
with -X- _ O
BERT -X- _ B-MethodName
and -X- _ O
is -X- _ O
surpassed -X- _ O
by -X- _ O
word2vec -X- _ B-MethodName
and -X- _ O
CLIP -X- _ B-MethodName
. -X- _ O
However -X- _ O
, -X- _ O
considering -X- _ O
the -X- _ O
small -X- _ O
model -X- _ O
and -X- _ O
training -X- _ O
data -X- _ O
we -X- _ O
used -X- _ O
( -X- _ O
see -X- _ O
Syntagmatic -X- _ O
categorization -X- _ O
. -X- _ O
We -X- _ O
quantify -X- _ O
the -X- _ O
syntagmatic -X- _ O
nature -X- _ O
of -X- _ O
a -X- _ O
clustering -X- _ O
by -X- _ O
the -X- _ O
mean -X- _ B-MetricName
association -X- _ I-MetricName
strength -X- _ I-MetricName
( -X- _ I-MetricName
MAS -X- _ I-MetricName
) -X- _ I-MetricName
of -X- _ O
pairs -X- _ O
of -X- _ O
words -X- _ O
in -X- _ O
the -X- _ O
SWOW -X- _ B-DatasetName
dataset -X- _ O
, -X- _ O
where -X- _ O
association -X- _ O
strength -X- _ O
of -X- _ O
a -X- _ O
pair -X- _ O
of -X- _ O
words -X- _ O
( -X- _ O
w -X- _ O
1 -X- _ O
, -X- _ O
w -X- _ O
2 -X- _ O
) -X- _ O
is -X- _ O
again -X- _ O
number -X- _ O
of -X- _ O
participants -X- _ O
who -X- _ O
responded -X- _ O
with -X- _ O
w2 -X- _ O
to -X- _ O
cue -X- _ O
word -X- _ O
w1 -X- _ O
. -X- _ O
MAS -X- _ B-MetricName
is -X- _ O
computed -X- _ O
across -X- _ O
all -X- _ O
word -X- _ O
pairs -X- _ O
from -X- _ O
the -X- _ O
taxonomic -X- _ O
dataset -X- _ O
in -X- _ O
which -X- _ O
both -X- _ O
words -X- _ O
were -X- _ O
assigned -X- _ O
the -X- _ O
same -X- _ O
cluster -X- _ O
by -X- _ O
this -X- _ O
clustering -X- _ O
solution -X- _ O
. -X- _ O

Results -X- _ O
are -X- _ O
presented -X- _ O
in -X- _ O
Table -X- _ O
3 -X- _ O
. -X- _ O
The -X- _ O
multimodal -X- _ O
models -X- _ O
( -X- _ O
ours -X- _ O
and -X- _ O
CLIP -X- _ B-MethodName
) -X- _ O
outperform -X- _ O
all -X- _ O
unimodal -X- _ O
models -X- _ O
, -X- _ O
an -X- _ O
indication -X- _ O
of -X- _ O
the -X- _ O
impact -X- _ O
of -X- _ O
multimodality -X- _ O
on -X- _ O
category -X- _ O
learning -X- _ O
: -X- _ O
multimodal -X- _ O
word -X- _ O
learning -X- _ O
shifts -X- _ O
the -X- _ O
learner -X- _ O
towards -X- _ O
syntagmatic -X- _ O
relations -X- _ O
more -X- _ O
significantly -X- _ O
than -X- _ O
unimodal -X- _ O
word -X- _ O
learning -X- _ O
. -X- _ O
To -X- _ O
our -X- _ O
knowledge -X- _ O
, -X- _ O
this -X- _ O
is -X- _ O
the -X- _ O
first -X- _ O
computational -X- _ O
result -X- _ O
to -X- _ O
support -X- _ O
this -X- _ O
hypothesis -X- _ O
, -X- _ O
shown -X- _ O
empirically -X- _ O
in -X- _ O
human -X- _ O
studies -X- _ O
with -X- _ O
infants -X- _ O
( -X- _ O
Elbers -X- _ O
and -X- _ O
van -X- _ O
Loon -X- _ O
- -X- _ O
Vervoorn -X- _ O
, -X- _ O
1999 -X- _ O
; -X- _ O
Mikolajczak -X- _ O
- -X- _ O
Matyja -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
. -X- _ O
clusters -X- _ O
created -X- _ O
by -X- _ O
our -X- _ O
model -X- _ O
and -X- _ O
one -X- _ O
cluster -X- _ O
created -X- _ O
by -X- _ O
word2vec -X- _ O
for -X- _ O
the -X- _ O
taxonomic -X- _ O
categorization -X- _ O
dataset -X- _ O
. -X- _ O
9 -X- _ O
The -X- _ O
clusters -X- _ O
formed -X- _ O
by -X- _ O
our -X- _ O
algorithm -X- _ O
are -X- _ O
syntagmatic -X- _ O
, -X- _ O
associating -X- _ O
words -X- _ O
frequently -X- _ O
observed -X- _ O
together -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
tokens -X- _ O
in -X- _ O
cluster -X- _ O
1 -X- _ O
are -X- _ O
related -X- _ O
to -X- _ O
snow -X- _ O
activity -X- _ O
, -X- _ O
while -X- _ O
cluster -X- _ O
2 -X- _ O
broadly -X- _ O
relates -X- _ O
to -X- _ O
water -X- _ O
) -X- _ O
. -X- _ O
The -X- _ O
cluster -X- _ O
formed -X- _ O
by -X- _ O
word2vec -X- _ O
embeddings -X- _ O
is -X- _ O
taxonomic -X- _ O
( -X- _ O
all -X- _ O
tokens -X- _ O
are -X- _ O
food -X- _ O
products -X- _ O
) -X- _ O
. -X- _ O
Our -X- _ O
results -X- _ O
provide -X- _ O
initial -X- _ O
evidence -X- _ O
that -X- _ O
syntagmatic -X- _ O
clusters -X- _ O
emerge -X- _ O
from -X- _ O
an -X- _ O
unsupervised -X- _ O
training -X- _ O
algorithm -X- _ O
drawing -X- _ O
on -X- _ O
simple -X- _ O
joint -X- _ O
clustering -X- _ O
of -X- _ O
words -X- _ O
and -X- _ O
images -X- _ O
. -X- _ O

Setting -X- _ O
. -X- _ O
We -X- _ O
evaluate -X- _ O
concreteness -X- _ O
estimation -X- _ O
using -X- _ O
the -X- _ O
dataset -X- _ O
by -X- _ O
Brysbaert -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2013 -X- _ O
) -X- _ O
, -X- _ O
which -X- _ O
contains -X- _ O
concreteness -X- _ O
ratings -X- _ O
for -X- _ O
40 -X- _ O
K -X- _ O
English -X- _ O
words -X- _ O
averaged -X- _ O
over -X- _ O
multiple -X- _ O
human -X- _ O
annotated -X- _ O
ratings -X- _ O
on -X- _ O
a -X- _ O
scale -X- _ O
of -X- _ O
1 -X- _ O
to -X- _ O
5 -X- _ O
. -X- _ O
We -X- _ O
estimate -X- _ O
the -X- _ O
concreteness -X- _ O
of -X- _ O
a -X- _ O
word -X- _ O
as -X- _ O
the -X- _ O
maximum -X- _ O
probability -X- _ O
with -X- _ O
which -X- _ O
it -X- _ O
was -X- _ O
assigned -X- _ O
to -X- _ O
any -X- _ O
cluster -X- _ O
. -X- _ O
For -X- _ O
evaluation -X- _ O
, -X- _ O
we -X- _ O
follow -X- _ O
Charbonnier -X- _ O
and -X- _ O
Wartena -X- _ O
( -X- _ O
2019 -X- _ O
) -X- _ O
and -X- _ O
compute -X- _ O
the -X- _ O
Pearson -X- _ O
correlation -X- _ O
coefficient -X- _ O
of -X- _ O
our -X- _ O
predictions -X- _ O
with -X- _ O
the -X- _ O
ground -X- _ O
- -X- _ O
truth -X- _ O
values -X- _ O
. -X- _ O
In -X- _ O
addition -X- _ O
, -X- _ O
we -X- _ O
investigate -X- _ O
the -X- _ O
impact -X- _ O
of -X- _ O
word -X- _ O
frequency -X- _ O
on -X- _ O
our -X- _ O
model -X- _ O
's -X- _ O
predictions -X- _ O
by -X- _ O
evaluating -X- _ O
the -X- _ O
model -X- _ O
on -X- _ O
subsets -X- _ O
of -X- _ O
words -X- _ O
in -X- _ O
the -X- _ O
Brysbaert -X- _ O
data -X- _ O
of -X- _ O
increasing -X- _ O
minimum -X- _ O
frequency -X- _ O
in -X- _ O
MSCOCO -X- _ B-DatasetName
. -X- _ O

Comparison -X- _ O
with -X- _ O
other -X- _ O
models -X- _ O
. -X- _ O
First -X- _ O
, -X- _ O
we -X- _ O
compare -X- _ O
against -X- _ O
supervised -X- _ B-MethodName
SVM -X- _ I-MethodName
regression -X- _ I-MethodName
models -X- _ O
, -X- _ O
which -X- _ O
have -X- _ O
shown -X- _ O
strong -X- _ O
performance -X- _ O
on -X- _ O
the -X- _ O
Brysbaert -X- _ B-DatasetName
data -X- _ O
in -X- _ O
prior -X- _ O
work -X- _ O
( -X- _ O
Charbonnier -X- _ O
and -X- _ O
Wartena -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
. -X- _ O
Following -X- _ O
their -X- _ O
work -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
two -X- _ O
feature -X- _ O
configurations -X- _ O
: -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
POS -X- _ O
tags -X- _ O
+ -X- _ O
suffixes -X- _ O
, -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
POS -X- _ O
tags -X- _ O
+ -X- _ O
suffixes -X- _ O
+ -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
FastText -X- _ O
embeddings -X- _ O
( -X- _ O
Joulin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O
We -X- _ O
train -X- _ O
the -X- _ O
SVMs -X- _ B-MethodName
on -X- _ O
the -X- _ O
full -X- _ O
Brysbaert -X- _ B-DatasetName
data -X- _ O
. -X- _ O

Second -X- _ O
, -X- _ O
we -X- _ O
compare -X- _ O
with -X- _ O
a -X- _ O
minimally -X- _ B-MethodName
supervised -X- _ I-MethodName
text -X- _ I-MethodName
- -X- _ I-MethodName
only -X- _ I-MethodName
model -X- _ O
. -X- _ O
As -X- _ O
in -X- _ O
Sec -X- _ O
5.1 -X- _ O
, -X- _ O
we -X- _ O
create -X- _ O
word -X- _ O
vector -X- _ O
representations -X- _ O
from -X- _ O
co -X- _ O
- -X- _ O
occurrence -X- _ O
counts -X- _ O
. -X- _ O
Next -X- _ O
, -X- _ O
following -X- _ O
prior -X- _ O
work -X- _ O
( -X- _ O
Turney -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2011 -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
select -X- _ O
concrete -X- _ O
( -X- _ O
abstract -X- _ O
) -X- _ O
representative -X- _ O
words -X- _ O
by -X- _ O
taking -X- _ O
the -X- _ O
20 -X- _ O
words -X- _ O
with -X- _ O
the -X- _ O
highest -X- _ O
( -X- _ O
lowest -X- _ O
) -X- _ O
concreteness -X- _ O
value -X- _ O
in -X- _ O
the -X- _ O
Brysbaert -X- _ B-DatasetName
data -X- _ O
that -X- _ O
occur -X- _ O
more -X- _ O
than -X- _ O
10 -X- _ O
times -X- _ O
in -X- _ O
the -X- _ O
MSCOCO -X- _ B-DatasetName
training -X- _ O
set -X- _ O
. -X- _ O
We -X- _ O
predict -X- _ O
a -X- _ O
word -X- _ O
's -X- _ O
concreteness -X- _ O
by -X- _ O
computing -X- _ O
its -X- _ O
average -X- _ O
cosine -X- _ O
similarity -X- _ O
to -X- _ O
the -X- _ O
concrete -X- _ O
representative -X- _ O
words -X- _ O
minus -X- _ O
the -X- _ O
average -X- _ O
of -X- _ O
its -X- _ O
cosine -X- _ O
similarity -X- _ O
to -X- _ O
the -X- _ O
abstract -X- _ O
representative -X- _ O
words -X- _ O
. -X- _ O

Results -X- _ O
. -X- _ O
Figure -X- _ O
2 -X- _ O
presents -X- _ O
the -X- _ O
results -X- _ O
in -X- _ O
terms -X- _ O
of -X- _ O
Pearson -X- _ B-MetricName
correlation -X- _ I-MetricName
when -X- _ O
evaluated -X- _ O
on -X- _ O
words -X- _ O
of -X- _ O
varying -X- _ O
minimum -X- _ O
frequency -X- _ O
in -X- _ O
MSCOCO -X- _ B-DatasetName
. -X- _ O
When -X- _ O
considering -X- _ O
frequent -X- _ O
tokens -X- _ O
only -X- _ O
, -X- _ O
our -X- _ O
model -X- _ O
predicts -X- _ O
word -X- _ O
concreteness -X- _ O
with -X- _ O
an -X- _ O
accuracy -X- _ B-MetricName
higher -X- _ O
than -X- _ O
the -X- _ O
SVM -X- _ O
with -X- _ O
POS -X- _ O
and -X- _ O
suffix -X- _ O
features -X- _ O
, -X- _ O
although -X- _ O
additional -X- _ O
embedding -X- _ O
features -X- _ O
improve -X- _ O
SVM -X- _ O
performance -X- _ O
further -X- _ O
. -X- _ O
Note -X- _ O
that -X- _ O
the -X- _ O
supervised -X- _ O
baseline -X- _ O
was -X- _ O
trained -X- _ O
on -X- _ O
the -X- _ O
full -X- _ O
data -X- _ O
set -X- _ O
, -X- _ O
and -X- _ O
hence -X- _ O
evaluated -X- _ O
on -X- _ O
a -X- _ O
subset -X- _ O
of -X- _ O
its -X- _ O
training -X- _ O
set -X- _ O
. -X- _ O
Our -X- _ O
multimodal -X- _ O
model -X- _ O
performs -X- _ O
better -X- _ O
than -X- _ O
its -X- _ O
textonly -X- _ O
variant -X- _ O
for -X- _ O
tokens -X- _ O
that -X- _ O
occur -X- _ O
at -X- _ O
least -X- _ O
100 -X- _ O
times -X- _ O
, -X- _ O
even -X- _ O
though -X- _ O
the -X- _ O
text -X- _ O
- -X- _ O
only -X- _ O
model -X- _ O
has -X- _ O
received -X- _ O
some -X- _ O
supervision -X- _ O
( -X- _ O
by -X- _ O
selecting -X- _ O
the -X- _ O
representative -X- _ O
words -X- _ O
) -X- _ O
. -X- _ O

Figure -X- _ O
2 -X- _ O
: -X- _ O
Pearson -X- _ B-MetricName
correlation -X- _ I-MetricName
between -X- _ O
predicted -X- _ O
word -X- _ O
concreteness -X- _ O
and -X- _ O
gold -X- _ O
standard -X- _ O
human -X- _ O
ratings -X- _ O
, -X- _ O
evaluated -X- _ O
over -X- _ O
test -X- _ O
sets -X- _ O
with -X- _ O
increasing -X- _ O
minimum -X- _ O
frequency -X- _ O
in -X- _ O
the -X- _ O
MSCOCO -X- _ B-DatasetName
data -X- _ O
. -X- _ O
Results -X- _ O
are -X- _ O
averaged -X- _ O
across -X- _ O
5 -X- _ B-HyperparameterValue
random -X- _ B-HyperparameterName
initializations -X- _ I-HyperparameterName
( -X- _ O
std -X- _ O
was -X- _ O
consistently -X- _ O
< -X- _ O
0.03 -X- _ O
) -X- _ O
. -X- _ O

In -X- _ O
addition -X- _ O
to -X- _ O
linguistic -X- _ B-TaskName
knowledge -X- _ I-TaskName
, -X- _ O
infants -X- _ O
acquire -X- _ O
visual -X- _ B-TaskName
semantic -X- _ I-TaskName
knowledge -X- _ I-TaskName
with -X- _ O
little -X- _ O
explicit -X- _ O
supervision -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
they -X- _ O
learn -X- _ O
to -X- _ O
segment -X- _ O
and -X- _ O
classify -X- _ O
objects -X- _ O
. -X- _ O
To -X- _ O
test -X- _ O
whether -X- _ O
our -X- _ O
model -X- _ O
also -X- _ O
acquires -X- _ O
such -X- _ O
knowledge -X- _ O
we -X- _ O
evaluated -X- _ O
it -X- _ O
on -X- _ O
the -X- _ O
multi -X- _ B-TaskName
- -X- _ I-TaskName
label -X- _ I-TaskName
classification -X- _ I-TaskName
task -X- _ I-TaskName
: -X- _ O
For -X- _ O
each -X- _ O
image -X- _ O
in -X- _ O
the -X- _ O
MSCOCO -X- _ B-DatasetName
test -X- _ O
set -X- _ O
, -X- _ O
predict -X- _ O
the -X- _ O
classes -X- _ O
of -X- _ O
objects -X- _ O
in -X- _ O
the -X- _ O
image -X- _ O
. -X- _ O

In -X- _ O
a -X- _ O
zero -X- _ O
- -X- _ O
shot -X- _ O
setting -X- _ O
, -X- _ O
we -X- _ O
mapped -X- _ O
the -X- _ O
induced -X- _ O
clusters -X- _ O
to -X- _ O
predicted -X- _ O
lists -X- _ O
of -X- _ O
MSCOCO -X- _ B-DatasetName
classes -X- _ O
as -X- _ O
follows -X- _ O
. -X- _ O
We -X- _ O
first -X- _ O
provided -X- _ O
the -X- _ O
name -X- _ O
of -X- _ O
each -X- _ O
class -X- _ O
to -X- _ O
our -X- _ O
model -X- _ O
as -X- _ O
text -X- _ O
input -X- _ O
and -X- _ O
retrieved -X- _ O
the -X- _ O
assigned -X- _ O
cluster -X- _ O
, -X- _ O
thus -X- _ O
obtaining -X- _ O
a -X- _ O
( -X- _ O
one -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
many -X- _ O
) -X- _ O
cluster -X- _ O
- -X- _ O
toclasses -X- _ O
mapping -X- _ O
. -X- _ O
Now -X- _ O
, -X- _ O
for -X- _ O
each -X- _ O
test -X- _ O
image -X- _ O
, -X- _ O
we -X- _ O
used -X- _ O
the -X- _ O
visual -X- _ O
encoder -X- _ O
to -X- _ O
predict -X- _ O
the -X- _ O
assigned -X- _ O
cluster -X- _ O
( -X- _ O
s -X- _ O
) -X- _ O
. -X- _ O
The -X- _ O
predicted -X- _ O
set -X- _ O
of -X- _ O
MSCOCO -X- _ B-DatasetName
classes -X- _ O
is -X- _ O
the -X- _ O
union -X- _ O
of -X- _ O
the -X- _ O
lists -X- _ O
of -X- _ O
classes -X- _ O
to -X- _ O
which -X- _ O
the -X- _ O
predicted -X- _ O
clusters -X- _ O
are -X- _ O
mapped -X- _ O
. -X- _ O

Comparison -X- _ O
with -X- _ O
CLIP -X- _ B-MethodName
. -X- _ O
We -X- _ O
compare -X- _ O
our -X- _ O
results -X- _ O
against -X- _ O
CLIP -X- _ B-MethodName
. -X- _ O
To -X- _ O
ensure -X- _ O
comparability -X- _ O
with -X- _ O
our -X- _ O
model -X- _ O
we -X- _ O
use -X- _ O
CLIP -X- _ O
with -X- _ O
ResNet50 -X- _ O
. -X- _ O
We -X- _ O
use -X- _ O
CLIP -X- _ O
as -X- _ O
a -X- _ O
point -X- _ O
of -X- _ O
comparison -X- _ O
to -X- _ O
provide -X- _ O
perspective -X- _ O
on -X- _ O
the -X- _ O
capabilities -X- _ O
of -X- _ O
our -X- _ O
model -X- _ O
despite -X- _ O
differences -X- _ O
in -X- _ O
modeling -X- _ O
and -X- _ O
assumptions -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
we -X- _ O
note -X- _ O
two -X- _ O
caveats -X- _ O
regarding -X- _ O
this -X- _ O
comparison -X- _ O
. -X- _ O
First -X- _ O
, -X- _ O
CLIP -X- _ O
was -X- _ O
trained -X- _ O
on -X- _ O
a -X- _ O
much -X- _ O
larger -X- _ O
training -X- _ O
set -X- _ O
and -X- _ O
has -X- _ O
more -X- _ O
parameters -X- _ O
than -X- _ O
our -X- _ O
model -X- _ O
( -X- _ O
see -X- _ O
Table -X- _ O
1 -X- _ O
) -X- _ O
. -X- _ O
Second -X- _ O
, -X- _ O
CLIP -X- _ O
has -X- _ O
only -X- _ O
been -X- _ O
used -X- _ O
for -X- _ O
single- -X- _ O
( -X- _ O
not -X- _ O
multi- -X- _ O
) -X- _ O
label -X- _ O
classification -X- _ O
, -X- _ O
by -X- _ O
inferring -X- _ O
encodings -X- _ O
of -X- _ O
both -X- _ O
input -X- _ O
images -X- _ O
and -X- _ O
prompts -X- _ O
representing -X- _ O
the -X- _ O
groundtruth -X- _ O
classes -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
" -X- _ O
a -X- _ O
photo -X- _ O
of -X- _ O
a -X- _ O
bus -X- _ O
" -X- _ O
for -X- _ O
the -X- _ O
ground -X- _ O
truth -X- _ O
class -X- _ O
bus -X- _ O
) -X- _ O
and -X- _ O
assigning -X- _ O
the -X- _ O
image -X- _ O
to -X- _ O
the -X- _ O
class -X- _ O
with -X- _ O
highest -X- _ O
cosine -X- _ O
similarity -X- _ O
to -X- _ O
its -X- _ O
encoding -X- _ O
. -X- _ O
We -X- _ O
adapt -X- _ O
CLIP -X- _ O
to -X- _ O
a -X- _ O
multi -X- _ O
- -X- _ O
label -X- _ O
setting -X- _ O
as -X- _ O
follows -X- _ O
: -X- _ O
Instead -X- _ O
of -X- _ O
assigning -X- _ O
the -X- _ O
image -X- _ O
to -X- _ O
the -X- _ O
class -X- _ O
with -X- _ O
the -X- _ O
highest -X- _ O
cosine -X- _ O
similarity -X- _ O
, -X- _ O
we -X- _ O
take -X- _ O
into -X- _ O
account -X- _ O
the -X- _ O
cosine -X- _ O
similarity -X- _ O
with -X- _ O
all -X- _ O
classes -X- _ O
for -X- _ O
each -X- _ O
image -X- _ O
. -X- _ O

We -X- _ O
consider -X- _ O
a -X- _ O
class -X- _ O
as -X- _ O
predicted -X- _ O
if -X- _ O
its -X- _ O
cosine -X- _ B-MetricName
similarity -X- _ I-MetricName
exceeds -X- _ O
a -X- _ O
threshold -X- _ B-HyperparameterName
, -X- _ O
tuned -X- _ O
on -X- _ O
the -X- _ O
MSCOCO -X- _ B-DatasetName
training -X- _ O
split -X- _ O
. -X- _ O

Another -X- _ O
important -X- _ O
task -X- _ O
performed -X- _ O
by -X- _ O
infants -X- _ O
is -X- _ O
visual -X- _ B-TaskName
object -X- _ I-TaskName
localization -X- _ I-TaskName
. -X- _ O
To -X- _ O
test -X- _ O
our -X- _ O
model -X- _ O
's -X- _ O
ability -X- _ O
to -X- _ O
reliably -X- _ O
localize -X- _ O
objects -X- _ O
in -X- _ O
images -X- _ O
we -X- _ O
use -X- _ O
Class -X- _ O
Activation -X- _ O
Maps -X- _ O
( -X- _ O
CAM -X- _ O
) -X- _ O
described -X- _ O
by -X- _ O
Zhou -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2016 -X- _ O
) -X- _ O
. -X- _ O
Each -X- _ O
CAM -X- _ O
indicates -X- _ O
how -X- _ O
important -X- _ O
each -X- _ O
pixel -X- _ O
was -X- _ O
during -X- _ O
classification -X- _ O
for -X- _ O
a -X- _ O
specific -X- _ O
cluster -X- _ O
. -X- _ O

Quantitative -X- _ O
analysis -X- _ O
. -X- _ O
Most -X- _ O
previous -X- _ O
studies -X- _ O
of -X- _ O
zero -X- _ O
- -X- _ O
shot -X- _ O
segmentation -X- _ O
( -X- _ O
Bucher -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
trained -X- _ O
on -X- _ O
a -X- _ O
subset -X- _ O
of -X- _ O
" -X- _ O
seen -X- _ O
" -X- _ O
classes -X- _ O
, -X- _ O
and -X- _ O
evaluated -X- _ O
on -X- _ O
both -X- _ O
seen -X- _ O
and -X- _ O
unseen -X- _ O
classes -X- _ O
. -X- _ O
We -X- _ O
use -X- _ O
a -X- _ O
more -X- _ O
challenging -X- _ O
setup -X- _ O
previously -X- _ O
referred -X- _ O
to -X- _ O
as -X- _ O
annotation -X- _ O
- -X- _ O
free -X- _ O
segmentation -X- _ O
( -X- _ O
Zhou -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
, -X- _ O
where -X- _ O
we -X- _ O
evaluate -X- _ O
our -X- _ O
model -X- _ O
without -X- _ O
any -X- _ O
training -X- _ O
for -X- _ O
the -X- _ O
segmentation -X- _ O
task -X- _ O
. -X- _ O
We -X- _ O
use -X- _ O
MSCOCO -X- _ B-DatasetName
's -X- _ O
ground -X- _ O
- -X- _ O
truth -X- _ O
bounding -X- _ O
boxes -X- _ O
, -X- _ O
which -X- _ O
are -X- _ O
human -X- _ O
annotated -X- _ O
and -X- _ O
mark -X- _ O
objects -X- _ O
in -X- _ O
the -X- _ O
image -X- _ O
, -X- _ O
for -X- _ O
evaluation -X- _ O
. -X- _ O
Following -X- _ O
the -X- _ O
original -X- _ O
CAM -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
a -X- _ O
heuristic -X- _ O
method -X- _ O
to -X- _ O
predict -X- _ O
bounding -X- _ O
boxes -X- _ O
: -X- _ O
Given -X- _ O
a -X- _ O
CAM -X- _ O
, -X- _ O
we -X- _ O
segment -X- _ O
the -X- _ O
pixels -X- _ O
of -X- _ O
which -X- _ O
the -X- _ O
value -X- _ O
is -X- _ O
above -X- _ O
50 -X- _ O
% -X- _ O
of -X- _ O
the -X- _ O
max -X- _ O
value -X- _ O
of -X- _ O
the -X- _ O
CAM -X- _ O
and -X- _ O
take -X- _ O
the -X- _ O
bounding -X- _ O
box -X- _ O
that -X- _ O
covers -X- _ O
the -X- _ O
largest -X- _ O
connected -X- _ O
component -X- _ O
in -X- _ O
the -X- _ O
segmentation -X- _ O
map -X- _ O
. -X- _ O

We -X- _ O
use -X- _ O
precision -X- _ B-MetricName
and -X- _ O
recall -X- _ B-MetricName
for -X- _ O
evaluation -X- _ O
. -X- _ O
A -X- _ O
pair -X- _ O
of -X- _ O
bounding -X- _ O
boxes -X- _ O
is -X- _ O
considered -X- _ O
a -X- _ O
match -X- _ O
if -X- _ O
the -X- _ O
intersection -X- _ B-MetricName
over -X- _ I-MetricName
union -X- _ I-MetricName
( -X- _ I-MetricName
IoU -X- _ I-MetricName
) -X- _ O
of -X- _ O
the -X- _ O
pair -X- _ O
exceeds -X- _ O
0.5 -X- _ B-MetricValue
. -X- _ O
Given -X- _ O
lists -X- _ O
of -X- _ O
predicted -X- _ O
and -X- _ O
ground -X- _ O
- -X- _ O
truth -X- _ O
bounding -X- _ O
boxes -X- _ O
, -X- _ O
we -X- _ O
consider -X- _ O
each -X- _ O
matched -X- _ O
pair -X- _ O
as -X- _ O
a -X- _ O
true -X- _ O
positive -X- _ O
and -X- _ O
a -X- _ O
prediction -X- _ O
( -X- _ O
ground -X- _ O
- -X- _ O
truth -X- _ O
) -X- _ O
for -X- _ O
which -X- _ O
no -X- _ O
matching -X- _ O
ground -X- _ O
- -X- _ O
truth -X- _ O
( -X- _ O
prediction -X- _ O
) -X- _ O
was -X- _ O
found -X- _ O
as -X- _ O
a -X- _ O
false -X- _ O
positive -X- _ O
( -X- _ O
negative -X- _ O
) -X- _ O
. -X- _ O
We -X- _ O
compare -X- _ O
our -X- _ O
model -X- _ O
to -X- _ O
a -X- _ O
random -X- _ O
baseline -X- _ O
: -X- _ O
Sample -X- _ O
k -X- _ O
random -X- _ O
bounding -X- _ O
boxes -X- _ O
( -X- _ O
where -X- _ O
k -X- _ O
is -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
groundtruth -X- _ O
bounding -X- _ O
boxes -X- _ O
in -X- _ O
the -X- _ O
current -X- _ O
image -X- _ O
) -X- _ O
. -X- _ O
This -X- _ O
baseline -X- _ O
uses -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
ground -X- _ O
- -X- _ O
truth -X- _ O
bounding -X- _ O
boxes -X- _ O
in -X- _ O
each -X- _ O
image -X- _ O
( -X- _ O
our -X- _ O
model -X- _ O
is -X- _ O
not -X- _ O
exposed -X- _ O
to -X- _ O
this -X- _ O
information -X- _ O
) -X- _ O
. -X- _ O

The -X- _ O
results -X- _ O
are -X- _ O
presented -X- _ O
in -X- _ O
Table -X- _ O
6 -X- _ O
. -X- _ O
Our -X- _ O
model -X- _ O
is -X- _ O
significantly -X- _ O
more -X- _ O
precise -X- _ B-MetricName
than -X- _ O
the -X- _ O
random -X- _ O
baseline -X- _ O
, -X- _ O
but -X- _ O
achieves -X- _ O
similar -X- _ O
recall -X- _ B-MetricName
: -X- _ O
the -X- _ O
entire -X- _ O
MSCOCO -X- _ B-DatasetName
test -X- _ O
split -X- _ O
contains -X- _ O
a -X- _ O
total -X- _ O
of -X- _ O
164,750 -X- _ B-MetricValue
bounding -X- _ O
boxes -X- _ O
, -X- _ O
while -X- _ O
our -X- _ O
model -X- _ O
predicted -X- _ O
38,237 -X- _ B-MetricValue
bounding -X- _ O
boxes -X- _ O
. -X- _ O
This -X- _ O
problem -X- _ O
could -X- _ O
be -X- _ O
addressed -X- _ O
by -X- _ O
lowering -X- _ O
the -X- _ O
visual -X- _ O
threshold -X- _ B-HyperparameterName
. -X- _ O
We -X- _ O
leave -X- _ O
this -X- _ O
direction -X- _ O
for -X- _ O
future -X- _ O
research -X- _ O
. -X- _ O

We -X- _ O
proposed -X- _ O
a -X- _ O
model -X- _ O
for -X- _ O
unsupervised -X- _ B-TaskName
multimodal -X- _ I-TaskName
lagnguage -X- _ I-TaskName
acquisition -X- _ I-TaskName
, -X- _ O
trained -X- _ O
to -X- _ O
jointly -X- _ O
cluster -X- _ O
text -X- _ O
and -X- _ O
images -X- _ O
. -X- _ O
Many -X- _ O
of -X- _ O
our -X- _ O
design -X- _ O
choices -X- _ O
were -X- _ O
guided -X- _ O
by -X- _ O
findings -X- _ O
from -X- _ O
cognitive -X- _ O
studies -X- _ O
of -X- _ O
infant -X- _ O
language -X- _ O
acquisition -X- _ O
: -X- _ O
The -X- _ O
joint -X- _ O
learning -X- _ O
of -X- _ O
multiple -X- _ O
modalities -X- _ O
; -X- _ O
learning -X- _ O
word -X- _ O
- -X- _ O
level -X- _ O
semantics -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
Fisher -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
1994 -X- _ O
, -X- _ O
suggest -X- _ O
that -X- _ O
children -X- _ O
first -X- _ O
learn -X- _ O
to -X- _ O
identify -X- _ O
nouns -X- _ O
and -X- _ O
use -X- _ O
this -X- _ O
information -X- _ O
to -X- _ O
learn -X- _ O
sentence -X- _ O
- -X- _ O
level -X- _ O
semantics -X- _ O
) -X- _ O
; -X- _ O
and -X- _ O
crosssituational -X- _ O
learning -X- _ O
( -X- _ O
counting -X- _ O
how -X- _ O
many -X- _ O
times -X- _ O
each -X- _ O
word -X- _ O
co -X- _ O
- -X- _ O
occurred -X- _ O
with -X- _ O
each -X- _ O
cluster -X- _ O
, -X- _ O
see -X- _ O
Gleitman -X- _ O
, -X- _ O
1990 -X- _ O
) -X- _ O
. -X- _ O
After -X- _ O
training -X- _ O
, -X- _ O
our -X- _ O
model -X- _ O
demonstrates -X- _ O
capabilities -X- _ O
typical -X- _ O
of -X- _ O
infant -X- _ O
language -X- _ O
acquisition -X- _ O
: -X- _ O
Word -X- _ O
concreteness -X- _ O
prediction -X- _ O
and -X- _ O
identification -X- _ O
and -X- _ O
segmentation -X- _ O
of -X- _ O
objects -X- _ O
in -X- _ O
a -X- _ O
visual -X- _ O
scene -X- _ O
. -X- _ O

In -X- _ O
the -X- _ O
semantic -X- _ O
word -X- _ B-TaskName
categorization -X- _ I-TaskName
and -X- _ I-TaskName
concreteness -X- _ I-TaskName
prediction -X- _ I-TaskName
experiments -X- _ O
, -X- _ O
we -X- _ O
compared -X- _ O
our -X- _ O
multimodal -X- _ B-MethodName
model -X- _ I-MethodName
to -X- _ I-MethodName
unimodal -X- _ I-MethodName
text -X- _ I-MethodName
- -X- _ I-MethodName
only -X- _ I-MethodName
baselines -X- _ I-MethodName
, -X- _ O
which -X- _ O
we -X- _ O
chose -X- _ O
to -X- _ O
be -X- _ O
as -X- _ O
similar -X- _ O
as -X- _ O
possible -X- _ O
to -X- _ O
our -X- _ O
model -X- _ O
. -X- _ O
The -X- _ O
results -X- _ O
suggest -X- _ O
that -X- _ O
multimodality -X- _ O
improves -X- _ O
performance -X- _ O
on -X- _ O
both -X- _ O
text -X- _ O
tasks -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
unclear -X- _ O
which -X- _ O
specific -X- _ O
information -X- _ O
is -X- _ O
encoded -X- _ O
in -X- _ O
the -X- _ O
visual -X- _ O
modality -X- _ O
that -X- _ O
benefits -X- _ O
these -X- _ O
text -X- _ O
tasks -X- _ O
. -X- _ O
We -X- _ O
leave -X- _ O
this -X- _ O
question -X- _ O
for -X- _ O
future -X- _ O
research -X- _ O
. -X- _ O

For -X- _ O
clustering -X- _ O
of -X- _ O
word -X- _ O
embeddings -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
the -X- _ O
K -X- _ O
- -X- _ O
Means -X- _ O
implementation -X- _ O
in -X- _ O
scikit -X- _ O
- -X- _ O
learn -X- _ O
. -X- _ O
We -X- _ O
use -X- _ O
the -X- _ O
word2vec -X- _ B-MethodName
google -X- _ I-MethodName
- -X- _ I-MethodName
news-300 -X- _ I-MethodName
model -X- _ O
from -X- _ O
the -X- _ O
gensim -X- _ O
package -X- _ O
, -X- _ O
the -X- _ O
BERT -X- _ B-MethodName
BASE -X- _ I-MethodName
model -X- _ O
from -X- _ O
the -X- _ O
transformers -X- _ O
library -X- _ O
and -X- _ O
CLIP -X- _ B-MethodName
's -X- _ O
official -X- _ O
implementation -X- _ O
. -X- _ O

We -X- _ O
used -X- _ O
publicly -X- _ O
available -X- _ O
resources -X- _ O
to -X- _ O
train -X- _ O
our -X- _ O
model -X- _ O
( -X- _ O
Lin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2014 -X- _ O
) -X- _ O
. -X- _ O
As -X- _ O
with -X- _ O
other -X- _ O
statistical -X- _ O
methods -X- _ O
for -X- _ O
word -X- _ O
representations -X- _ O
, -X- _ O
our -X- _ O
approach -X- _ O
may -X- _ O
capture -X- _ O
social -X- _ O
biases -X- _ O
which -X- _ O
manifest -X- _ O
in -X- _ O
its -X- _ O
training -X- _ O
data -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
MSCOCO -X- _ B-DatasetName
was -X- _ O
shown -X- _ O
to -X- _ O
be -X- _ O
biased -X- _ O
with -X- _ O
respect -X- _ O
to -X- _ O
gender -X- _ O
, -X- _ O
Zhao -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O
Our -X- _ O
code -X- _ O
includes -X- _ O
a -X- _ O
model -X- _ O
card -X- _ O
( -X- _ O
Mitchell -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
which -X- _ O
reports -X- _ O
standard -X- _ O
information -X- _ O
regarding -X- _ O
the -X- _ O
training -X- _ O
used -X- _ O
to -X- _ O
produce -X- _ O
our -X- _ O
models -X- _ O
and -X- _ O
its -X- _ O
word -X- _ O
clusters -X- _ O
. -X- _ O

For -X- _ O
the -X- _ O
visual -X- _ O
encoder -X- _ O
, -X- _ O
we -X- _ O
used -X- _ O
the -X- _ O
ResNet50 -X- _ O
implementation -X- _ O
from -X- _ O
the -X- _ O
torchvision -X- _ O
package -X- _ O
with -X- _ O
ADAM -X- _ O
optimizer -X- _ O
and -X- _ O
a -X- _ O
learning -X- _ B-HyperparameterName
rate -X- _ I-HyperparameterName
of -X- _ O
10 -X- _ B-HyperparameterValue
−4 -X- _ I-HyperparameterValue
. -X- _ O

Morphological -X- _ B-MethodName
Reinflection -X- _ I-MethodName
with -X- _ O
Multiple -X- _ O
Arguments -X- _ O
: -X- _ O
An -X- _ O
Extended -X- _ O
Annotation -X- _ O
schema -X- _ O
and -X- _ O
a -X- _ O
Georgian -X- _ O
Case -X- _ O
Study -X- _ O

In -X- _ O
recent -X- _ O
years -X- _ O
, -X- _ O
a -X- _ O
flurry -X- _ O
of -X- _ O
morphological -X- _ O
datasets -X- _ O
had -X- _ O
emerged -X- _ O
, -X- _ O
most -X- _ O
notably -X- _ O
UniMorph -X- _ B-DatasetName
, -X- _ O
a -X- _ O
multi -X- _ O
- -X- _ O
lingual -X- _ O
repository -X- _ O
of -X- _ O
inflection -X- _ O
tables -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
the -X- _ O
flat -X- _ O
structure -X- _ O
of -X- _ O
the -X- _ O
current -X- _ O
morphological -X- _ O
annotation -X- _ O
schema -X- _ O
makes -X- _ O
the -X- _ O
treatment -X- _ O
of -X- _ O
some -X- _ O
languages -X- _ O
quirky -X- _ O
, -X- _ O
if -X- _ O
not -X- _ O
impossible -X- _ O
, -X- _ O
specifically -X- _ O
in -X- _ O
cases -X- _ O
of -X- _ O
polypersonal -X- _ O
agreement -X- _ O
, -X- _ O
where -X- _ O
verbs -X- _ O
agree -X- _ O
with -X- _ O
multiple -X- _ O
arguments -X- _ O
using -X- _ O
true -X- _ O
affixes -X- _ O
. -X- _ O
In -X- _ O
this -X- _ O
paper -X- _ O
we -X- _ O
propose -X- _ O
to -X- _ O
address -X- _ O
this -X- _ O
phenomenon -X- _ O
, -X- _ O
by -X- _ O
expanding -X- _ O
the -X- _ O
UniMorph -X- _ B-DatasetName
annotation -X- _ O
schema -X- _ O
to -X- _ O
hierarchical -X- _ B-MethodName
feature -X- _ I-MethodName
structure -X- _ I-MethodName
that -X- _ O
naturally -X- _ O
accommodates -X- _ O
complex -X- _ O
argument -X- _ O
marking -X- _ O
. -X- _ O
We -X- _ O
apply -X- _ O
this -X- _ O
extended -X- _ O
schema -X- _ O
to -X- _ O
one -X- _ O
such -X- _ O
language -X- _ O
, -X- _ O
Georgian -X- _ O
, -X- _ O
and -X- _ O
provide -X- _ O
a -X- _ O
human -X- _ O
- -X- _ O
verified -X- _ O
, -X- _ O
accurate -X- _ O
and -X- _ O
balanced -X- _ O
morphological -X- _ O
dataset -X- _ O
for -X- _ O
Georgian -X- _ O
verbs -X- _ O
. -X- _ O
The -X- _ O
dataset -X- _ O
has -X- _ O
4 -X- _ O
times -X- _ O
more -X- _ O
tables -X- _ O
and -X- _ O
6 -X- _ O
times -X- _ O
more -X- _ O
verb -X- _ O
forms -X- _ O
compared -X- _ O
to -X- _ O
the -X- _ O
existing -X- _ O
UniMorph -X- _ B-DatasetName
dataset -X- _ O
, -X- _ O
covering -X- _ O
all -X- _ O
possible -X- _ O
variants -X- _ O
of -X- _ O
argument -X- _ O
marking -X- _ O
, -X- _ O
demonstrating -X- _ O
the -X- _ O
adequacy -X- _ O
of -X- _ O
our -X- _ O
proposed -X- _ O
scheme -X- _ O
. -X- _ O
Experiments -X- _ O
with -X- _ O
a -X- _ O
standard -X- _ O
reinflection -X- _ B-TaskName
model -X- _ O
show -X- _ O
that -X- _ O
generalization -X- _ O
is -X- _ O
easy -X- _ O
when -X- _ O
the -X- _ O
data -X- _ O
is -X- _ O
split -X- _ O
at -X- _ O
the -X- _ O
form -X- _ O
level -X- _ O
, -X- _ O
but -X- _ O
extremely -X- _ O
hard -X- _ O
when -X- _ O
splitting -X- _ O
along -X- _ O
lemma -X- _ O
lines -X- _ O
. -X- _ O
Expanding -X- _ O
the -X- _ O
other -X- _ O
languages -X- _ O
in -X- _ O
UniMorph -X- _ B-DatasetName
to -X- _ O
this -X- _ O
schema -X- _ O
is -X- _ O
expected -X- _ O
to -X- _ O
improve -X- _ O
both -X- _ O
the -X- _ O
coverage -X- _ O
, -X- _ O
consistency -X- _ O
and -X- _ O
interpretability -X- _ O
of -X- _ O
this -X- _ O
benchmark -X- _ O
. -X- _ O

In -X- _ O
recent -X- _ O
years -X- _ O
, -X- _ O
morphological -X- _ B-TaskName
( -X- _ I-TaskName
re -X- _ I-TaskName
) -X- _ I-TaskName
inflection -X- _ I-TaskName
tasks -X- _ O
have -X- _ O
gained -X- _ O
a -X- _ O
lot -X- _ O
of -X- _ O
attention -X- _ O
in -X- _ O
NLP -X- _ O
. -X- _ O
1 -X- _ O
Subsequently -X- _ O
, -X- _ O
several -X- _ O
multi -X- _ O
- -X- _ O
lingual -X- _ O
morphological -X- _ O
datasets -X- _ O
have -X- _ O
emerged -X- _ O
to -X- _ O
allow -X- _ O
for -X- _ O
the -X- _ O
supervised -X- _ O
training -X- _ O
of -X- _ O
morphological -X- _ O
models -X- _ O
, -X- _ O
most -X- _ O
notably -X- _ O
UniMorph -X- _ B-DatasetName
( -X- _ O
McCarthy -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
, -X- _ O
that -X- _ O
organizes -X- _ O
words -X- _ O
into -X- _ O
inflectional -X- _ O
tables -X- _ O
, -X- _ O
annotating -X- _ O
each -X- _ O
inflected -X- _ O
word -X- _ O
- -X- _ O
form -X- _ O
with -X- _ O
its -X- _ O
respective -X- _ O
feature -X- _ O
- -X- _ O
set -X- _ O
. -X- _ O

While -X- _ O
western -X- _ O
languages -X- _ O
are -X- _ O
widely -X- _ O
represented -X- _ O
in -X- _ O
UniMorph -X- _ B-DatasetName
, -X- _ O
many -X- _ O
morphologically -X- _ O
rich -X- _ O
languages -X- _ O
( -X- _ O
Tsarfaty -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2010 -X- _ O
( -X- _ O
Tsarfaty -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
, -X- _ O
2020 -X- _ O
exhibit -X- _ O
rich -X- _ O
and -X- _ O
diverse -X- _ O
inflection -X- _ O
patterns -X- _ O
that -X- _ O
make -X- _ O
them -X- _ O
less -X- _ O
compatible -X- _ O
with -X- _ O
the -X- _ O
flat -X- _ O
feature -X- _ O
- -X- _ O
sets -X- _ O
in -X- _ O
the -X- _ O
Uni -X- _ B-DatasetName
- -X- _ I-DatasetName
Morph -X- _ I-DatasetName
schema -X- _ O
. -X- _ O
Concretely -X- _ O
, -X- _ O
in -X- _ O
some -X- _ O
cases -X- _ O
it -X- _ O
is -X- _ O
completely -X- _ O
impossible -X- _ O
to -X- _ O
annotate -X- _ O
parts -X- _ O
of -X- _ O
the -X- _ O
inflectional -X- _ O
paradigm -X- _ O
with -X- _ O
a -X- _ O
flat -X- _ O
bundle -X- _ O
, -X- _ O
as -X- _ O
is -X- _ O
the -X- _ O
case -X- _ O
with -X- _ O
case -X- _ O
stacking -X- _ O
, -X- _ O
and -X- _ O
in -X- _ O
other -X- _ O
cases -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
polypersonal -X- _ O
agreement -X- _ O
, -X- _ O
the -X- _ O
annotation -X- _ O
solutions -X- _ O
provided -X- _ O
are -X- _ O
unnatural -X- _ O
, -X- _ O
non -X- _ O
- -X- _ O
transparent -X- _ O
, -X- _ O
and -X- _ O
are -X- _ O
barely -X- _ O
used -X- _ O
in -X- _ O
practice -X- _ O
. -X- _ O
As -X- _ O
a -X- _ O
result -X- _ O
, -X- _ O
languages -X- _ O
exhibiting -X- _ O
such -X- _ O
phenomena -X- _ O
are -X- _ O
under -X- _ O
- -X- _ O
represented -X- _ O
in -X- _ O
UniMorph -X- _ B-DatasetName
, -X- _ O
and -X- _ O
when -X- _ O
they -X- _ O
are -X- _ O
, -X- _ O
the -X- _ O
inflection -X- _ O
tables -X- _ O
for -X- _ O
these -X- _ O
languages -X- _ O
are -X- _ O
often -X- _ O
incomplete -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
paper -X- _ O
we -X- _ O
propose -X- _ O
a -X- _ O
general -X- _ O
solution -X- _ O
for -X- _ O
annotating -X- _ O
such -X- _ O
structures -X- _ O
, -X- _ O
thus -X- _ O
extending -X- _ O
the -X- _ O
UniMorph -X- _ B-DatasetName
annotation -X- _ O
schema -X- _ O
to -X- _ O
fully -X- _ O
cover -X- _ O
a -X- _ O
wider -X- _ O
range -X- _ O
of -X- _ O
morphologically -X- _ O
- -X- _ O
complex -X- _ O
argumentmarking -X- _ O
phenomena -X- _ O
. -X- _ O
Following -X- _ O
Anderson -X- _ O
( -X- _ O
1992 -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
a -X- _ O
so -X- _ O
- -X- _ O
called -X- _ O
layered -X- _ B-MethodName
annotation -X- _ I-MethodName
of -X- _ O
features -X- _ O
, -X- _ O
where -X- _ O
the -X- _ O
inflectional -X- _ O
features -X- _ O
take -X- _ O
the -X- _ O
form -X- _ O
of -X- _ O
a -X- _ O
hierarchical -X- _ O
structure -X- _ O
, -X- _ O
in -X- _ O
the -X- _ O
spirit -X- _ O
of -X- _ O
formal -X- _ O
linguistic -X- _ O
frameworks -X- _ O
as -X- _ O
that -X- _ O
of -X- _ O
Johnson -X- _ O
( -X- _ O
1988 -X- _ O
) -X- _ O
; -X- _ O
Pollard -X- _ O
and -X- _ O
Sag -X- _ O
( -X- _ O
1994 -X- _ O
) -X- _ O
; -X- _ O
Shieber -X- _ O
( -X- _ O
2003 -X- _ O
) -X- _ O
; -X- _ O
Bresnan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2015 -X- _ O
) -X- _ O
. -X- _ O
We -X- _ O
organize -X- _ O
the -X- _ O
features -X- _ O
of -X- _ O
multiple -X- _ O
arguments -X- _ O
in -X- _ O
a -X- _ O
hierarchical -X- _ O
structure -X- _ O
, -X- _ O
rather -X- _ O
than -X- _ O
the -X- _ O
current -X- _ O
flat -X- _ O
structure -X- _ O
that -X- _ O
accommodates -X- _ O
only -X- _ O
subject -X- _ O
concords -X- _ O
. -X- _ O
This -X- _ O
schema -X- _ O
shift -X- _ O
allows -X- _ O
for -X- _ O
an -X- _ O
adequate -X- _ O
annotation -X- _ O
of -X- _ O
polypersonal -X- _ O
agreement -X- _ O
and -X- _ O
of -X- _ O
possessed -X- _ O
nominals -X- _ O
, -X- _ O
where -X- _ O
a -X- _ O
word -X- _ O
has -X- _ O
multiple -X- _ O
number -X- _ O
and -X- _ O
gender -X- _ O
features -X- _ O
, -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
forms -X- _ O
with -X- _ O
case -X- _ O
stacking -X- _ O
, -X- _ O
where -X- _ O
a -X- _ O
word -X- _ O
has -X- _ O
multiple -X- _ O
cases -X- _ O
. -X- _ O

We -X- _ O
apply -X- _ O
the -X- _ O
suggested -X- _ O
solution -X- _ O
to -X- _ O
Georgian -X- _ O
, -X- _ O
an -X- _ O
agglutinative -X- _ O
language -X- _ O
with -X- _ O
a -X- _ O
convoluted -X- _ O
verbal -X- _ O
system -X- _ O
, -X- _ O
that -X- _ O
indicates -X- _ O
both -X- _ O
subjects -X- _ O
and -X- _ O
objects -X- _ O
with -X- _ O
true -X- _ O
affixes -X- _ O
( -X- _ O
rather -X- _ O
than -X- _ O
clitics -X- _ O
that -X- _ O
are -X- _ O
omittable -X- _ O
from -X- _ O
the -X- _ O
inflection -X- _ O
tables -X- _ O
) -X- _ O
. -X- _ O
We -X- _ O
create -X- _ O
a -X- _ O
new -X- _ O
human -X- _ O
- -X- _ O
verified -X- _ O
dataset -X- _ O
for -X- _ O
Georgian -X- _ O
, -X- _ O
that -X- _ O
covers -X- _ O
most -X- _ O
of -X- _ O
the -X- _ O
grammatical -X- _ O
phenomena -X- _ O
in -X- _ O
Georgian -X- _ O
verbs -X- _ O
, -X- _ O
and -X- _ O
includes -X- _ O
118 -X- _ O
lemmas -X- _ O
, -X- _ O
adding -X- _ O
up -X- _ O
to -X- _ O
about -X- _ O
21k -X- _ O
verb -X- _ O
forms -X- _ O
, -X- _ O
compared -X- _ O
with -X- _ O
the -X- _ O
47 -X- _ O
lemmas -X- _ O
and -X- _ O
3.3k -X- _ O
verb -X- _ O
forms -X- _ O
, -X- _ O
some -X- _ O
of -X- _ O
which -X- _ O
are -X- _ O
erroneous -X- _ O
, -X- _ O
currently -X- _ O
available -X- _ O
in -X- _ O
the -X- _ O
Georgian -X- _ O
UniMorph -X- _ B-DatasetName
. -X- _ O
We -X- _ O
use -X- _ O
the -X- _ O
new -X- _ O
dataset -X- _ O
to -X- _ O
train -X- _ O
a -X- _ O
standard -X- _ O
morphological -X- _ B-TaskName
reinflection -X- _ I-TaskName
model -X- _ O
( -X- _ O
Silfverberg -X- _ O
and -X- _ O
Hulden -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
and -X- _ O
show -X- _ O
that -X- _ O
training -X- _ O
on -X- _ O
the -X- _ O
Georgian -X- _ O
inflections -X- _ O
currently -X- _ O
available -X- _ O
in -X- _ O
UniMorph -X- _ B-DatasetName
is -X- _ O
not -X- _ O
sufficient -X- _ O
for -X- _ O
generalizing -X- _ O
to -X- _ O
the -X- _ O
more -X- _ O
inclusive -X- _ O
set -X- _ O
of -X- _ O
inflections -X- _ O
that -X- _ O
are -X- _ O
allowed -X- _ O
by -X- _ O
the -X- _ O
new -X- _ O
scheme -X- _ O
. -X- _ O
We -X- _ O
conclude -X- _ O
that -X- _ O
our -X- _ O
annotation -X- _ O
approach -X- _ O
provides -X- _ O
a -X- _ O
more -X- _ O
complete -X- _ O
representation -X- _ O
of -X- _ O
linguistic -X- _ O
behaviors -X- _ O
, -X- _ O
and -X- _ O
that -X- _ O
our -X- _ O
proposed -X- _ O
Georgian -X- _ O
dataset -X- _ O
provides -X- _ O
a -X- _ O
much -X- _ O
better -X- _ O
depiction -X- _ O
of -X- _ O
the -X- _ O
morphological -X- _ O
phenomena -X- _ O
that -X- _ O
exist -X- _ O
in -X- _ O
the -X- _ O
data -X- _ O
and -X- _ O
the -X- _ O
computational -X- _ O
challenge -X- _ O
reflected -X- _ O
therein -X- _ O
. -X- _ O

We -X- _ O
therefore -X- _ O
call -X- _ O
to -X- _ O
apply -X- _ O
layered -X- _ B-MethodName
annotation -X- _ I-MethodName
to -X- _ O
all -X- _ O
currently -X- _ O
existing -X- _ O
morphological -X- _ O
data -X- _ O
in -X- _ O
Uni -X- _ B-DatasetName
- -X- _ I-DatasetName
Morph -X- _ I-DatasetName
, -X- _ O
to -X- _ O
more -X- _ O
consistently -X- _ O
and -X- _ O
transparently -X- _ O
capture -X- _ O
the -X- _ O
linguistic -X- _ O
reality -X- _ O
and -X- _ O
morphological -X- _ O
complexity -X- _ O
reflected -X- _ O
in -X- _ O
the -X- _ O
worlds -X- _ O
languages -X- _ O
. -X- _ O

The -X- _ O
Problem -X- _ O
: -X- _ O
Multiple -X- _ O
Arguments -X- _ O

Models -X- _ O
of -X- _ O
morphological -X- _ B-TaskName
reinfection -X- _ I-TaskName
are -X- _ O
trained -X- _ O
to -X- _ O
generate -X- _ O
forms -X- _ O
within -X- _ O
a -X- _ O
lemma -X- _ O
L -X- _ O
, -X- _ O
given -X- _ O
another -X- _ O
form -X- _ O
and -X- _ O
the -X- _ O
features -X- _ O
of -X- _ O
source -X- _ O
i -X- _ O
and -X- _ O
target -X- _ O
j -X- _ O
forms -X- _ O
: -X- _ O

⟨f -X- _ O
eat -X- _ O
L -X- _ O
i -X- _ O
, -X- _ O
f -X- _ O
orm -X- _ O
L -X- _ O
i -X- _ O
⟩ -X- _ O
, -X- _ O
⟨f -X- _ O
eat -X- _ O
L -X- _ O
j -X- _ O
, -X- _ O
_ -X- _ O
_ -X- _ O
_ -X- _ O
⟩ -X- _ O
→ -X- _ O
f -X- _ O
orm -X- _ O
L -X- _ O
j -X- _ O

For -X- _ O
example -X- _ O
, -X- _ O
for -X- _ O
the -X- _ O
Russian -X- _ O
lemma -X- _ O
ЛЕТЕТЬ -X- _ O
: -X- _ O
reinflecting -X- _ O
from -X- _ O
( -X- _ O
PRS -X- _ O
; -X- _ O
1 -X- _ O
; -X- _ O
SG -X- _ O
, -X- _ O
лечу -X- _ O
) -X- _ O
to -X- _ O
( -X- _ O
IMP -X- _ O
; -X- _ O
2 -X- _ O
; -X- _ O
SG -X- _ O
, -X- _ O
лети -X- _ O
) -X- _ O
will -X- _ O
be -X- _ O
represented -X- _ O
as -X- _ O
: -X- _ O
⟨PRS -X- _ O
; -X- _ O
1 -X- _ O
; -X- _ O
SG -X- _ O
, -X- _ O
лечу⟩ -X- _ O
, -X- _ O
⟨IMP -X- _ O
; -X- _ O
2 -X- _ O
; -X- _ O
SG -X- _ O
, -X- _ O
_ -X- _ O
_ -X- _ O
_ -X- _ O
⟩ -X- _ O
→ -X- _ O
лети -X- _ O
Standardly -X- _ O
, -X- _ O
the -X- _ O
data -X- _ O
for -X- _ O
training -X- _ O
morphological -X- _ O
models -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
Wu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
; -X- _ O
Makarov -X- _ O
and -X- _ O
Clematide -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
is -X- _ O
taken -X- _ O
from -X- _ O
UniMorph -X- _ B-DatasetName
( -X- _ O
Mc -X- _ O
- -X- _ O
Carthy -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
, -X- _ O
a -X- _ O
multilingual -X- _ O
morphological -X- _ O
dataset -X- _ O
in -X- _ O
which -X- _ O
words -X- _ O
are -X- _ O
grouped -X- _ O
by -X- _ O
lemma -X- _ O
into -X- _ O
inflection -X- _ O
tables -X- _ O
, -X- _ O
each -X- _ O
word -X- _ O
is -X- _ O
tagged -X- _ O
with -X- _ O
an -X- _ O
unordered -X- _ O
set -X- _ O
of -X- _ O
morphological -X- _ O
features -X- _ O
. -X- _ O
The -X- _ O
features -X- _ O
list -X- _ O
is -X- _ O
shared -X- _ O
across -X- _ O
languages -X- _ O
. -X- _ O
The -X- _ O
inflection -X- _ O
tables -X- _ O
are -X- _ O
meant -X- _ O
to -X- _ O
be -X- _ O
exhaustive -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
covering -X- _ O
all -X- _ O
possible -X- _ O
forms -X- _ O
of -X- _ O
a -X- _ O
lemma -X- _ O
, -X- _ O
regardless -X- _ O
of -X- _ O
usability -X- _ O
. -X- _ O

Although -X- _ O
the -X- _ O
features -X- _ O
were -X- _ O
designed -X- _ O
to -X- _ O
apply -X- _ O
cross -X- _ O
- -X- _ O
lingually -X- _ O
, -X- _ O
some -X- _ O
blind -X- _ O
- -X- _ O
spots -X- _ O
exist -X- _ O
. -X- _ O
Most -X- _ O
relevant -X- _ O
to -X- _ O
our -X- _ O
work -X- _ O
is -X- _ O
the -X- _ O
assumption -X- _ O
that -X- _ O
every -X- _ O
feature -X- _ O
set -X- _ O
includes -X- _ O
at -X- _ O
most -X- _ O
one -X- _ O
pronominal -X- _ O
feature -X- _ O
bundle -X- _ O
( -X- _ O
i.e. -X- _ O
, -X- _ O
person -X- _ O
- -X- _ O
gender -X- _ O
- -X- _ O
number -X- _ O
) -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
this -X- _ O
assumption -X- _ O
does -X- _ O
not -X- _ O
apply -X- _ O
to -X- _ O
verbs -X- _ O
with -X- _ O
object -X- _ O
concords -X- _ O
, -X- _ O
as -X- _ O
exhibited -X- _ O
in -X- _ O
Georgian -X- _ O
( -X- _ O
see -X- _ O
Table -X- _ O
1 -X- _ O
) -X- _ O
, -X- _ O
Inuit -X- _ O
and -X- _ O
many -X- _ O
Bantu -X- _ O
languages -X- _ O
inter -X- _ O
alia -X- _ O
, -X- _ O
nor -X- _ O
does -X- _ O
it -X- _ O
apply -X- _ O
to -X- _ O
possessed -X- _ O
nouns -X- _ O
that -X- _ O
mark -X- _ O
the -X- _ O
features -X- _ O
of -X- _ O
both -X- _ O
the -X- _ O
possessor -X- _ O
and -X- _ O
the -X- _ O
possessee -X- _ O
. -X- _ O
Examples -X- _ O
( -X- _ O
1a -X- _ O
) -X- _ O
- -X- _ O
( -X- _ O
1d -X- _ O
) -X- _ O
illustrate -X- _ O
this -X- _ O
: -X- _ O

( -X- _ O
1 -X- _ O
) -X- _ O
a. -X- _ O
Georgian -X- _ O
: -X- _ O
gagišvebt -X- _ O
' -X- _ O
We -X- _ O
will -X- _ O
let -X- _ O
you -X- _ O
go -X- _ O
' -X- _ O

( -X- _ O
The -X- _ O
solution -X- _ O
proposed -X- _ O
in -X- _ O
UniMorph -X- _ B-DatasetName
to -X- _ O
annotating -X- _ O
these -X- _ O
phenomena -X- _ O
is -X- _ O
via -X- _ O
concatenating -X- _ O
several -X- _ O
properties -X- _ O
into -X- _ O
a -X- _ O
single -X- _ O
string -X- _ O
, -X- _ O
lacking -X- _ O
any -X- _ O
internal -X- _ O
structure -X- _ O
; -X- _ O
e.g. -X- _ O
, -X- _ O
ARGAC2S -X- _ O
indicates -X- _ O
a -X- _ O
form -X- _ O
with -X- _ O
a -X- _ O
2nd -X- _ O
person -X- _ O
singular -X- _ O
accusative -X- _ O
argument -X- _ O
( -X- _ O
Sylak -X- _ O
- -X- _ O
Glassman -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
there -X- _ O
are -X- _ O
at -X- _ O
least -X- _ O
two -X- _ O
shortcomings -X- _ O
to -X- _ O
this -X- _ O
solution -X- _ O
. -X- _ O
First -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
not -X- _ O
sufficiently -X- _ O
transparent -X- _ O
. -X- _ O
ARGAC2S -X- _ O
is -X- _ O
an -X- _ O
opaque -X- _ O
string -X- _ O
, -X- _ O
that -X- _ O
does -X- _ O
not -X- _ O
decompose -X- _ O
into -X- _ O
the -X- _ O
known -X- _ O
features -X- _ O
licensed -X- _ O
by -X- _ O
the -X- _ O
UniMorph -X- _ B-DatasetName
features -X- _ O
list -X- _ O
( -X- _ O
i.e. -X- _ O
, -X- _ O
ACC -X- _ O
, -X- _ O
2 -X- _ O
, -X- _ O
SG -X- _ O
) -X- _ O
. -X- _ O
Secondly -X- _ O
, -X- _ O
and -X- _ O
possibly -X- _ O
due -X- _ O
to -X- _ O
this -X- _ O
lack -X- _ O
of -X- _ O
transparency -X- _ O
, -X- _ O
this -X- _ O
annotation -X- _ O
hack -X- _ O
is -X- _ O
hardly -X- _ O
ever -X- _ O
used -X- _ O
in -X- _ O
practice -X- _ O
. -X- _ O
Hence -X- _ O
, -X- _ O
from -X- _ O
all -X- _ O
examples -X- _ O
in -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
, -X- _ O
only -X- _ O
the -X- _ O
Hebrew -X- _ O
form -X- _ O
is -X- _ O
included -X- _ O
in -X- _ O
UniMorph -X- _ B-DatasetName
, -X- _ O
and -X- _ O
tagged -X- _ O
as -X- _ O
N -X- _ O
; -X- _ O
SG -X- _ O
; -X- _ O
FEM -X- _ O
; -X- _ O
PSS3S -X- _ O
with -X- _ O
multiple -X- _ O
possessor -X- _ O
features -X- _ O
merged -X- _ O
into -X- _ O
the -X- _ O
flat -X- _ O
string -X- _ O
PSS3S -X- _ O
. -X- _ O

The -X- _ O
crux -X- _ O
of -X- _ O
the -X- _ O
matter -X- _ O
is -X- _ O
that -X- _ O
in -X- _ O
the -X- _ O
current -X- _ O
annotation -X- _ O
schema -X- _ O
, -X- _ O
complex -X- _ O
features -X- _ O
assigned -X- _ O
to -X- _ O
additional -X- _ O
arguments -X- _ O
are -X- _ O
treated -X- _ O
as -X- _ O
a -X- _ O
single -X- _ O
nondecomposable -X- _ O
feature -X- _ O
, -X- _ O
that -X- _ O
lack -X- _ O
any -X- _ O
internal -X- _ O
structure -X- _ O
, -X- _ O
unlike -X- _ O
the -X- _ O
features -X- _ O
of -X- _ O
the -X- _ O
main -X- _ O
( -X- _ O
so -X- _ O
- -X- _ O
called -X- _ O
' -X- _ O
internal -X- _ O
' -X- _ O
) -X- _ O
argument -X- _ O
, -X- _ O
that -X- _ O
are -X- _ O
individually -X- _ O
spelled -X- _ O
out -X- _ O
. -X- _ O
We -X- _ O
argue -X- _ O
that -X- _ O
the -X- _ O
lack -X- _ O
of -X- _ O
transparency -X- _ O
and -X- _ O
usability -X- _ O
are -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
misrepresentation -X- _ O
of -X- _ O
the -X- _ O
inherently -X- _ O
hierarchical -X- _ O
and -X- _ O
compositional -X- _ O
structure -X- _ O
of -X- _ O
the -X- _ O
features -X- _ O
in -X- _ O
such -X- _ O
forms -X- _ O
. -X- _ O
We -X- _ O
suggest -X- _ O
to -X- _ O
explicitly -X- _ O
annotate -X- _ O
these -X- _ O
forms -X- _ O
with -X- _ O
features -X- _ O
that -X- _ O
are -X- _ O
all -X- _ O
explicitly -X- _ O
composed -X- _ O
of -X- _ O
the -X- _ O
same -X- _ O
primitive -X- _ O
features -X- _ O
. -X- _ O

All -X- _ O
in -X- _ O
all -X- _ O
, -X- _ O
the -X- _ O
lack -X- _ O
of -X- _ O
a -X- _ O
sufficiently -X- _ O
expressive -X- _ O
annotation -X- _ O
standard -X- _ O
leads -X- _ O
to -X- _ O
a -X- _ O
data -X- _ O
distribution -X- _ O
that -X- _ O
is -X- _ O
skewed -X- _ O
, -X- _ O
unrealistically -X- _ O
simple -X- _ O
, -X- _ O
and -X- _ O
, -X- _ O
when -X- _ O
languagespecific -X- _ O
annotation -X- _ O
solutions -X- _ O
are -X- _ O
painfully -X- _ O
needed -X- _ O
, -X- _ O
they -X- _ O
suffer -X- _ O
from -X- _ O
inconsistencies -X- _ O
and -X- _ O
ad -X- _ O
- -X- _ O
hoc -X- _ O
decisions -X- _ O
. -X- _ O
For -X- _ O
these -X- _ O
reasons -X- _ O
, -X- _ O
we -X- _ O
set -X- _ O
out -X- _ O
to -X- _ O
extend -X- _ O
the -X- _ O
UniMorph -X- _ B-DatasetName
annotation -X- _ O
schema -X- _ O
to -X- _ O
accommodate -X- _ O
all -X- _ O
such -X- _ O
cases -X- _ O
and -X- _ O
to -X- _ O
enable -X- _ O
a -X- _ O
proper -X- _ O
coverage -X- _ O
of -X- _ O
languages -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
Georgian -X- _ O
and -X- _ O
many -X- _ O
others -X- _ O
. -X- _ O

The -X- _ O
Proposed -X- _ O
Schema -X- _ O

We -X- _ O
propose -X- _ O
to -X- _ O
extend -X- _ O
the -X- _ O
UniMorph -X- _ B-DatasetName
annotation -X- _ O
schema -X- _ O
to -X- _ O
cover -X- _ O
multiple -X- _ O
pronominal -X- _ O
featurebundles -X- _ O
in -X- _ O
the -X- _ O
same -X- _ O
word -X- _ O
- -X- _ O
form -X- _ O
, -X- _ O
via -X- _ O
a -X- _ O
layering -X- _ O
approach -X- _ O
, -X- _ O
originally -X- _ O
proposed -X- _ O
for -X- _ O
morphological -X- _ O
systems -X- _ O
by -X- _ O
Anderson -X- _ O
( -X- _ O
1992 -X- _ O
) -X- _ O
. -X- _ O
Anderson -X- _ O
suggests -X- _ O
to -X- _ O
arrange -X- _ O
the -X- _ O
morphosyntactic -X- _ O
representation -X- _ O
( -X- _ O
MSR -X- _ O
) -X- _ O
of -X- _ O
words -X- _ O
in -X- _ O
a -X- _ O
hierarchy -X- _ O
( -X- _ O
dubbed -X- _ O
layers -X- _ O
) -X- _ O
of -X- _ O
features -X- _ O
, -X- _ O
in -X- _ O
the -X- _ O
sense -X- _ O
that -X- _ O
every -X- _ O
element -X- _ O
of -X- _ O
the -X- _ O
unordered -X- _ O
set -X- _ O
of -X- _ O
features -X- _ O
can -X- _ O
be -X- _ O
composed -X- _ O
of -X- _ O
another -X- _ O
unordered -X- _ O
set -X- _ O
of -X- _ O
features -X- _ O
. -X- _ O
That -X- _ O
is -X- _ O
, -X- _ O
a -X- _ O
general -X- _ O
feature -X- _ O
annotation -X- _ O
looks -X- _ O
as -X- _ O
in -X- _ O
( -X- _ O
2a -X- _ O
) -X- _ O
. -X- _ O
A -X- _ O
specific -X- _ O
transitive -X- _ O
verb -X- _ O
annotation -X- _ O
could -X- _ O
be -X- _ O
as -X- _ O
depicted -X- _ O
in -X- _ O
( -X- _ O
2b -X- _ O
) -X- _ O
: -X- _ O

( -X- _ O

2 -X- _ O
) -X- _ O
a. -X- _ O
[ -X- _ O
f -X- _ O
1 -X- _ O
, -X- _ O
f -X- _ O
2 -X- _ O
, -X- _ O
... -X- _ O
, -X- _ O
[ -X- _ O
F -X- _ O
i -X- _ O
: -X- _ O
f -X- _ O
i -X- _ O
1 -X- _ O
, -X- _ O
f -X- _ O
i -X- _ O
2 -X- _ O
, -X- _ O
... -X- _ O
[ -X- _ O
F -X- _ O
j -X- _ O
: -X- _ O
f -X- _ O
j -X- _ O
1 -X- _ O
.. -X- _ O
] -X- _ O
] -X- _ O
] -X- _ O
b. -X- _ O
[ -X- _ O
V -X- _ O
, -X- _ O
T -X- _ O
ense -X- _ O
, -X- _ O
[ -X- _ O
nom -X- _ O
: -X- _ O
P -X- _ O
er -X- _ O
, -X- _ O
N -X- _ O
um -X- _ O
, -X- _ O
Gen -X- _ O
] -X- _ O
, -X- _ O
[ -X- _ O
acc -X- _ O
: -X- _ O
P -X- _ O
er -X- _ O
, -X- _ O
N -X- _ O
um -X- _ O
, -X- _ O
Gen -X- _ O
] -X- _ O
] -X- _ O

This -X- _ O
hierarchical -X- _ B-MethodName
feature -X- _ I-MethodName
structure -X- _ I-MethodName
is -X- _ O
reminiscent -X- _ O
of -X- _ O
unification -X- _ O
grammars -X- _ O
or -X- _ O
attribute -X- _ O
- -X- _ O
value -X- _ O
grammars -X- _ O
( -X- _ O
Shieber -X- _ O
, -X- _ O
2003 -X- _ O
; -X- _ O
Johnson -X- _ O
, -X- _ O
1988 -X- _ O
) -X- _ O
that -X- _ O
are -X- _ O
extensively -X- _ O
used -X- _ O
in -X- _ O
syntactic -X- _ O
theories -X- _ O
such -X- _ O
as -X- _ O
GPSG -X- _ B-MethodName
, -X- _ O
HPSG -X- _ B-MethodName
, -X- _ O
and -X- _ O
resemble -X- _ O
the -X- _ O
f -X- _ O
- -X- _ O
structures -X- _ O
in -X- _ O
LFG -X- _ B-MethodName
( -X- _ O
Gazdar -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
1989 -X- _ O
; -X- _ O
Pollard -X- _ O
and -X- _ O
Sag -X- _ O
, -X- _ O
1994 -X- _ O
; -X- _ O
Bresnan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
. -X- _ O

Here -X- _ O
we -X- _ O
employ -X- _ O
these -X- _ O
structures -X- _ O
to -X- _ O
organize -X- _ O
the -X- _ O
features -X- _ O
of -X- _ O
morphologically -X- _ O
- -X- _ O
marked -X- _ O
arguments -X- _ O
hierarchically -X- _ O
, -X- _ O
so -X- _ O
an -X- _ O
argument -X- _ O
is -X- _ O
characterized -X- _ O
by -X- _ O
a -X- _ O
feature -X- _ O
composite -X- _ O
of -X- _ O
all -X- _ O
features -X- _ O
pertaining -X- _ O
to -X- _ O
that -X- _ O
argument -X- _ O
. -X- _ O
That -X- _ O
is -X- _ O
, -X- _ O
each -X- _ O
argument -X- _ O
's -X- _ O
featurebundle -X- _ O
os -X- _ O
specifically -X- _ O
marked -X- _ O
with -X- _ O
the -X- _ O
argument -X- _ O
it -X- _ O
belongs -X- _ O
to -X- _ O
, -X- _ O
and -X- _ O
is -X- _ O
decomposed -X- _ O
into -X- _ O
the -X- _ O
primitive -X- _ O
features -X- _ O
licensed -X- _ O
by -X- _ O
the -X- _ O
UniMorph -X- _ B-DatasetName
scheme -X- _ O
. -X- _ O
It -X- _ O
also -X- _ O
homogeneously -X- _ O
annotates -X- _ O
the -X- _ O
different -X- _ O
kinds -X- _ O
of -X- _ O
arguments -X- _ O
, -X- _ O
in -X- _ O
contrast -X- _ O
with -X- _ O
the -X- _ O
current -X- _ O
schema -X- _ O
where -X- _ O
the -X- _ O
subject -X- _ O
features -X- _ O
are -X- _ O
assigned -X- _ O
to -X- _ O
the -X- _ O
verb -X- _ O
directly -X- _ O
. -X- _ O
Thus -X- _ O
, -X- _ O
the -X- _ O
English -X- _ O
form -X- _ O
thinks -X- _ O
previously -X- _ O
annotated -X- _ O
as -X- _ O
V -X- _ O
; -X- _ O
PRS -X- _ O
; -X- _ O
3 -X- _ O
; -X- _ O
SG -X- _ O
will -X- _ O
be -X- _ O
annotated -X- _ O
as -X- _ O
V -X- _ O
; -X- _ O
PRS -X- _ O
; -X- _ O
NOM -X- _ O
( -X- _ O
3 -X- _ O
; -X- _ O
SG -X- _ O
) -X- _ O
. -X- _ O
In -X- _ O
languages -X- _ O
that -X- _ O
mark -X- _ O
multiple -X- _ O
arguments -X- _ O
, -X- _ O
different -X- _ O
kinds -X- _ O
of -X- _ O
arguments -X- _ O
can -X- _ O
be -X- _ O
marked -X- _ O
with -X- _ O
their -X- _ O
feature -X- _ O
- -X- _ O
bundles -X- _ O
without -X- _ O
conflicts -X- _ O
. -X- _ O
The -X- _ O
proposed -X- _ O
schema -X- _ O
thus -X- _ O
facilitates -X- _ O
the -X- _ O
annotation -X- _ O
of -X- _ O
the -X- _ O
poorly -X- _ O
- -X- _ O
treated -X- _ O
or -X- _ O
untreated -X- _ O
phenomena -X- _ O
as -X- _ O
illustrated -X- _ O
in -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
. -X- _ O
These -X- _ O
are -X- _ O
, -X- _ O
respectively -X- _ O
: -X- _ O

( -X- _ O
3 -X- _ O
) -X- _ O
a. -X- _ O
Georgian -X- _ O
: -X- _ O
gagišvebt -X- _ O
' -X- _ O
We -X- _ O
will -X- _ O
let -X- _ O
you -X- _ O
go -X- _ O
' -X- _ O
Table -X- _ O
2 -X- _ O
compares -X- _ O
the -X- _ O
annotation -X- _ O
of -X- _ O
these -X- _ O
examples -X- _ O
in -X- _ O
the -X- _ O
current -X- _ O
UniMorph -X- _ B-DatasetName
schema -X- _ O
compared -X- _ O
with -X- _ O
our -X- _ O
proposed -X- _ O
annotation -X- _ O
schema -X- _ O
. -X- _ O
2 -X- _ O
The -X- _ O
hierarchical -X- _ O
structures -X- _ O
, -X- _ O
beyond -X- _ O
being -X- _ O
more -X- _ O
transparent -X- _ O
, -X- _ O
opens -X- _ O
the -X- _ O
door -X- _ O
further -X- _ O
for -X- _ O
future -X- _ O
study -X- _ O
on -X- _ O
compositional -X- _ O
generalization -X- _ O
in -X- _ O
morphology -X- _ O
. -X- _ O

The -X- _ O
resemblance -X- _ O
of -X- _ O
our -X- _ O
proposed -X- _ O
schema -X- _ O
to -X- _ O
ideas -X- _ O
in -X- _ O
other -X- _ O
fields -X- _ O
of -X- _ O
theoretical -X- _ O
linguistics -X- _ O
, -X- _ O
most -X- _ O
prominently -X- _ O
to -X- _ O
the -X- _ O
f -X- _ O
- -X- _ O
structure -X- _ O
in -X- _ O
LFG -X- _ B-MethodName
( -X- _ O
Bresnan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
and -X- _ O
to -X- _ O
the -X- _ O
nested -X- _ O
Attribute -X- _ O
- -X- _ O
Value -X- _ O
matrices -X- _ O
in -X- _ O
HPSG -X- _ B-MethodName
( -X- _ O
Pollard -X- _ O
and -X- _ O
Sag -X- _ O
, -X- _ O
1994 -X- _ O
) -X- _ O
, -X- _ O
points -X- _ O
to -X- _ O
a -X- _ O
natural -X- _ O
interface -X- _ O
with -X- _ O
further -X- _ O
syntactic -X- _ O
and -X- _ O
semantic -X- _ O
annotations -X- _ O
downstream -X- _ O
. -X- _ O

A -X- _ O
Case -X- _ O
Study -X- _ O
from -X- _ O
Georgian -X- _ O

Linguistic -X- _ O
Background -X- _ O
Georgian -X- _ O
is -X- _ O
an -X- _ O
agglutinative -X- _ O
language -X- _ O
with -X- _ O
a -X- _ O
verbal -X- _ O
system -X- _ O
that -X- _ O
makes -X- _ O
a -X- _ O
vast -X- _ O
use -X- _ O
of -X- _ O
affixes -X- _ O
to -X- _ O
convey -X- _ O
a -X- _ O
wide -X- _ O
array -X- _ O
of -X- _ O
meanings -X- _ O
, -X- _ O
both -X- _ O
inflectional -X- _ O
and -X- _ O
derivational -X- _ O
( -X- _ O
see -X- _ O
Table -X- _ O
1 -X- _ O
) -X- _ O
. -X- _ O
The -X- _ O
Georgian -X- _ O
verbal -X- _ O
paradigm -X- _ O
is -X- _ O
divided -X- _ O
into -X- _ O
5 -X- _ O
classes -X- _ O
known -X- _ O
as -X- _ O
: -X- _ O
transitive -X- _ O
, -X- _ O
intransitive -X- _ O
, -X- _ O
medial -X- _ O
, -X- _ O
indirect -X- _ O
and -X- _ O
stative -X- _ O
( -X- _ O
Hewitt -X- _ O
, -X- _ O
1995 -X- _ O
) -X- _ O
. -X- _ O
The -X- _ O
verbs -X- _ O
are -X- _ O
inflected -X- _ O
to -X- _ O
reflect -X- _ O
12 -X- _ O
Tense -X- _ O
- -X- _ O
Aspect -X- _ O
- -X- _ O
Mood -X- _ O
( -X- _ O
TAM -X- _ O
) -X- _ O
combinations -X- _ O
( -X- _ O
traditionally -X- _ O
known -X- _ O
as -X- _ O
screeves -X- _ O
) -X- _ O
sorted -X- _ O
into -X- _ O
4 -X- _ O
series -X- _ O
: -X- _ O
present -X- _ O
and -X- _ O
future -X- _ O
, -X- _ O
aorist -X- _ O
, -X- _ O
perfective -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
imperative -X- _ O
. -X- _ O
Each -X- _ O
series -X- _ O
has -X- _ O
its -X- _ O
own -X- _ O
morpho -X- _ O
- -X- _ O
syntactic -X- _ O
characteristics -X- _ O
, -X- _ O
most -X- _ O
notably -X- _ O
split -X- _ O
- -X- _ O
ergativity -X- _ O
is -X- _ O
manifested -X- _ O
in -X- _ O
the -X- _ O
aorist -X- _ O
. -X- _ O

The -X- _ O
characteristic -X- _ O
most -X- _ O
essential -X- _ O
to -X- _ O
this -X- _ O
work -X- _ O
is -X- _ O
that -X- _ O
Georgian -X- _ O
verbs -X- _ O
always -X- _ O
agree -X- _ O
on -X- _ O
person -X- _ O
and -X- _ O
number -X- _ O
with -X- _ O
the -X- _ O
direct -X- _ O
and -X- _ O
indirect -X- _ O
objects -X- _ O
, -X- _ O
on -X- _ O
top -X- _ O
of -X- _ O
the -X- _ O
subject -X- _ O
- -X- _ O
verb -X- _ O
agreement -X- _ O
. -X- _ O
The -X- _ O
Georgian -X- _ O
data -X- _ O
in -X- _ O
UniMorph -X- _ B-DatasetName
follows -X- _ O
the -X- _ O
convention -X- _ O
of -X- _ O
including -X- _ O
objects -X- _ O
only -X- _ O
in -X- _ O
third -X- _ O
person -X- _ O
singular -X- _ O
-thus -X- _ O
failing -X- _ O
to -X- _ O
provide -X- _ O
a -X- _ O
comprehensive -X- _ O
coverage -X- _ O
of -X- _ O
the -X- _ O
wordforms -X- _ O
that -X- _ O
can -X- _ O
be -X- _ O
attested -X- _ O
in -X- _ O
the -X- _ O
language -X- _ O
. -X- _ O

Additional -X- _ O
issues -X- _ O
with -X- _ O
the -X- _ O
current -X- _ O
morphological -X- _ O
data -X- _ O
in -X- _ O
UniMorph -X- _ B-DatasetName
for -X- _ O
Georgian -X- _ O
verbs -X- _ O
are -X- _ O
: -X- _ O
sparsity -X- _ O
, -X- _ O
as -X- _ O
it -X- _ O
includes -X- _ O
only -X- _ O
47 -X- _ O
inflection -X- _ O
tables -X- _ O
; -X- _ O
lack -X- _ O
of -X- _ O
diversity -X- _ O
, -X- _ O
as -X- _ O
all -X- _ O
table -X- _ O
are -X- _ O
from -X- _ O
the -X- _ O
transitive -X- _ O
class -X- _ O
; -X- _ O
and -X- _ O
lack -X- _ O
of -X- _ O
accuracy -X- _ O
, -X- _ O
as -X- _ O
the -X- _ O
data -X- _ O
was -X- _ O
produced -X- _ O
automatically -X- _ O
without -X- _ O
verification -X- _ O
by -X- _ O
native -X- _ O
speakers -X- _ O
. -X- _ O

Data -X- _ O
Annotation -X- _ O

A -X- _ O
key -X- _ O
contribution -X- _ O
of -X- _ O
this -X- _ O
work -X- _ O
is -X- _ O
the -X- _ O
creation -X- _ O
of -X- _ O
a -X- _ O
new -X- _ O
dataset -X- _ O
for -X- _ O
Georgian -X- _ O
that -X- _ O
follows -X- _ O
the -X- _ O
layered -X- _ B-MethodName
annotation -X- _ I-MethodName
schema -X- _ O
and -X- _ O
addresses -X- _ O
the -X- _ O
other -X- _ O
shortcomings -X- _ O
just -X- _ O
described -X- _ O
. -X- _ O
We -X- _ O
selected -X- _ O
a -X- _ O
list -X- _ O
of -X- _ O
118 -X- _ O
verb -X- _ O
lemmata -X- _ O
from -X- _ O
all -X- _ O
differ- -X- _ O
ent -X- _ O
classes -X- _ O
. -X- _ O
3 -X- _ O
Every -X- _ O
verb -X- _ O
was -X- _ O
manually -X- _ O
annotated -X- _ O
with -X- _ O
its -X- _ O
stem -X- _ O
, -X- _ O
its -X- _ O
thematic -X- _ O
affix -X- _ O
and -X- _ O
principal -X- _ O
parts -X- _ O
, -X- _ O
to -X- _ O
automatically -X- _ O
generate -X- _ O
the -X- _ O
full -X- _ O
inflection -X- _ O
tables -X- _ O
. -X- _ O

This -X- _ O
automatic -X- _ O
generation -X- _ O
of -X- _ O
Georgian -X- _ O
verbs -X- _ O
is -X- _ O
prone -X- _ O
to -X- _ O
some -X- _ O
errors -X- _ O
, -X- _ O
for -X- _ O
instance -X- _ O
, -X- _ O
in -X- _ O
accounting -X- _ O
for -X- _ O
idiosyncratic -X- _ O
phonologically -X- _ O
- -X- _ O
conditioned -X- _ O
stem -X- _ O
changes -X- _ O
. -X- _ O
Hence -X- _ O
, -X- _ O
we -X- _ O
ran -X- _ O
our -X- _ O
data -X- _ O
through -X- _ O
3 -X- _ O
native -X- _ O
Georgian -X- _ O
speakers -X- _ O
to -X- _ O
assert -X- _ O
its -X- _ O
correctness -X- _ O
, -X- _ O
or -X- _ O
fix -X- _ O
when -X- _ O
needed -X- _ O
. -X- _ O
In -X- _ O
cases -X- _ O
where -X- _ O
speakers -X- _ O
were -X- _ O
unsure -X- _ O
we -X- _ O
used -X- _ O
a -X- _ O
Georgian -X- _ O
morphological -X- _ O
analyzer -X- _ O
( -X- _ O
Doborjginidze -X- _ O
and -X- _ O
Lobzhanidze -X- _ O
, -X- _ O
2012 -X- _ O
) -X- _ O
for -X- _ O
consultation -X- _ O
. -X- _ O
In -X- _ O
cases -X- _ O
of -X- _ O
disagreement -X- _ O
, -X- _ O
we -X- _ O
used -X- _ O
a -X- _ O
majority -X- _ B-MetricName
vote -X- _ I-MetricName
among -X- _ O
the -X- _ O
speakers -X- _ O
. -X- _ O
On -X- _ O
average -X- _ O
, -X- _ O
at -X- _ O
least -X- _ O
one -X- _ O
speaker -X- _ O
was -X- _ O
uncertain -X- _ O
in -X- _ O
about -X- _ O
5 -X- _ B-MetricValue
% -X- _ I-MetricValue
of -X- _ O
the -X- _ O
forms -X- _ O
, -X- _ O
but -X- _ O
a -X- _ O
disagreement -X- _ O
that -X- _ O
necessitated -X- _ O
a -X- _ O
majority -X- _ B-MetricName
vote -X- _ I-MetricName
occurred -X- _ O
only -X- _ O
on -X- _ O
about -X- _ O
0.7 -X- _ B-MetricValue
% -X- _ I-MetricValue
of -X- _ O
the -X- _ O
cases -X- _ O
. -X- _ O

Experiments -X- _ O

To -X- _ O
assess -X- _ O
the -X- _ O
usability -X- _ O
of -X- _ O
our -X- _ O
dataset -X- _ O
, -X- _ O
we -X- _ O
trained -X- _ O
a -X- _ O
standard -X- _ O
reinflection -X- _ O
model -X- _ O
, -X- _ O
the -X- _ O
character -X- _ O
- -X- _ O
level -X- _ O
LSTM -X- _ O
of -X- _ O
Silfverberg -X- _ O
and -X- _ O
Hulden -X- _ O
( -X- _ O
2018 -X- _ O
) -X- _ O
, -X- _ O
on -X- _ O
our -X- _ O
data -X- _ O
. -X- _ O
5 -X- _ O
We -X- _ O
sampled -X- _ O
from -X- _ O
our -X- _ O
data -X- _ O
2 -X- _ O
datasets -X- _ O
for -X- _ O
training -X- _ O
morphological -X- _ B-TaskName
reinflection -X- _ I-TaskName
models -X- _ O
, -X- _ O
containing -X- _ O
train -X- _ O
, -X- _ O
validation -X- _ O
and -X- _ O
test -X- _ O
sets -X- _ O
in -X- _ O
sizes -X- _ O
8k -X- _ O
, -X- _ O
1k -X- _ O
and -X- _ O
1k -X- _ O
examples -X- _ O
, -X- _ O
respectively -X- _ O
. -X- _ O
Following -X- _ O
Goldman -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
, -X- _ O
one -X- _ O
dataset -X- _ O
employed -X- _ O
an -X- _ O
easier -X- _ O
formsplit -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
no -X- _ O
forms -X- _ O
appear -X- _ O
in -X- _ O
both -X- _ O
train -X- _ O
and -X- _ O
test -X- _ O
, -X- _ O
6 -X- _ O
and -X- _ O
the -X- _ O
other -X- _ O
with -X- _ O
the -X- _ O
more -X- _ O
challenging -X- _ O
lemmasplit -X- _ O
, -X- _ O
where -X- _ O
lemmas -X- _ O
from -X- _ O
train -X- _ O
, -X- _ O
dev -X- _ O
and -X- _ O
test -X- _ O
are -X- _ O
disjoint -X- _ O
. -X- _ O
To -X- _ O
assess -X- _ O
the -X- _ O
generalization -X- _ O
capacity -X- _ O
we -X- _ O
varied -X- _ O
the -X- _ O
sources -X- _ O
of -X- _ O
both -X- _ O
the -X- _ O
train -X- _ O
and -X- _ O
test -X- _ O
sets -X- _ O
. -X- _ O
7 -X- _ O
We -X- _ O
report -X- _ O
2 -X- _ O
evaluation -X- _ O
metrics -X- _ O
: -X- _ O
accuracy -X- _ B-MetricName
over -X- _ O
exact -X- _ O
matches -X- _ O
, -X- _ O
and -X- _ O
average -X- _ B-MetricName
edit -X- _ I-MetricName
distance -X- _ I-MetricName
from -X- _ O
gold -X- _ O
. -X- _ O
bination -X- _ O
) -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
the -X- _ O
model -X- _ O
generalizes -X- _ O
poorly -X- _ O
from -X- _ O
the -X- _ O
original -X- _ O
partial -X- _ O
data -X- _ O
to -X- _ O
the -X- _ O
forms -X- _ O
in -X- _ O
our -X- _ O
test -X- _ O
set -X- _ O
which -X- _ O
reflect -X- _ O
the -X- _ O
entire -X- _ O
Georgian -X- _ O
inflectional -X- _ O
system -X- _ O
. -X- _ O
Generalization -X- _ O
from -X- _ O
our -X- _ O
data -X- _ O
to -X- _ O
Uni -X- _ B-DatasetName
- -X- _ I-DatasetName
Morph -X- _ I-DatasetName
's -X- _ O
set -X- _ O
is -X- _ O
a -X- _ O
lot -X- _ O
better -X- _ O
. -X- _ O
The -X- _ O
results -X- _ O
also -X- _ O
show -X- _ O
that -X- _ O
the -X- _ O
splitting -X- _ O
method -X- _ O
is -X- _ O
crucial -X- _ O
for -X- _ O
success -X- _ O
of -X- _ O
the -X- _ O
model -X- _ O
, -X- _ O
as -X- _ O
it -X- _ O
inflects -X- _ O
easily -X- _ O
to -X- _ O
unseen -X- _ O
forms -X- _ O
, -X- _ O
but -X- _ O
much -X- _ O
harder -X- _ O
when -X- _ O
inflecting -X- _ O
forms -X- _ O
in -X- _ O
a -X- _ O
previously -X- _ O
unseen -X- _ O
lemma -X- _ O
. -X- _ O
8 -X- _ O
These -X- _ O
results -X- _ O
corroborate -X- _ O
the -X- _ O
results -X- _ O
of -X- _ O
Goldman -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
regarding -X- _ O
the -X- _ O
difficulty -X- _ O
of -X- _ O
lemma -X- _ O
- -X- _ O
split -X- _ O
data -X- _ O
. -X- _ O
Although -X- _ O
the -X- _ O
accuracy -X- _ B-MetricName
over -X- _ O
the -X- _ O
lemma -X- _ O
split -X- _ O
data -X- _ O
is -X- _ O
negligible -X- _ O
, -X- _ O
the -X- _ O
average -X- _ B-MetricName
edit -X- _ I-MetricName
distance -X- _ I-MetricName
in -X- _ O
that -X- _ O
case -X- _ O
points -X- _ O
again -X- _ O
to -X- _ O
the -X- _ O
conclusion -X- _ O
that -X- _ O
generalization -X- _ O
from -X- _ O
UniMorph -X- _ B-DatasetName
to -X- _ O
our -X- _ O
data -X- _ O
is -X- _ O
harder -X- _ O
that -X- _ O
the -X- _ O
other -X- _ O
way -X- _ O
around -X- _ O
. -X- _ O

Error -X- _ O
Analysis -X- _ O
To -X- _ O
provide -X- _ O
insights -X- _ O
into -X- _ O
the -X- _ O
challenge -X- _ O
of -X- _ O
reinflecting -X- _ O
morphologically -X- _ O
complex -X- _ O
forms -X- _ O
, -X- _ O
we -X- _ O
manually -X- _ O
sampled -X- _ O
the -X- _ O
erroneous -X- _ O
output -X- _ O
of -X- _ O
the -X- _ O
model -X- _ O
trained -X- _ O
and -X- _ O
tested -X- _ O
over -X- _ O
our -X- _ O
lemmasplit -X- _ O
data -X- _ O
, -X- _ O
to -X- _ O
draw -X- _ O
insights -X- _ O
on -X- _ O
the -X- _ O
points -X- _ O
of -X- _ O
failure -X- _ O
. -X- _ O

In -X- _ O
many -X- _ O
cases -X- _ O
the -X- _ O
model -X- _ O
succeeded -X- _ O
in -X- _ O
copying -X- _ O
and -X- _ O
modifying -X- _ O
the -X- _ O
verb -X- _ O
stem -X- _ O
, -X- _ O
but -X- _ O
failed -X- _ O
to -X- _ O
output -X- _ O
the -X- _ O
other -X- _ O
morphemes -X- _ O
correctly -X- _ O
. -X- _ O
Sometimes -X- _ O
the -X- _ O
errors -X- _ O
were -X- _ O
due -X- _ O
to -X- _ O
inflection -X- _ O
to -X- _ O
an -X- _ O
incorrect -X- _ O
TAM -X- _ O
combination -X- _ O
of -X- _ O
the -X- _ O
same -X- _ O
lexeme -X- _ O
, -X- _ O
and -X- _ O
sometimes -X- _ O
the -X- _ O
inflection -X- _ O
was -X- _ O
done -X- _ O
to -X- _ O
the -X- _ O
correct -X- _ O
TAM -X- _ O
but -X- _ O
to -X- _ O
a -X- _ O
different -X- _ O
derivationally -X- _ O
- -X- _ O
related -X- _ O
lemma -X- _ O
( -X- _ O
e.g. -X- _ O
change -X- _ O
of -X- _ O
voice -X- _ O
in -X- _ O
addition -X- _ O
to -X- _ O
the -X- _ O
change -X- _ O
of -X- _ O
TAM -X- _ O
) -X- _ O
. -X- _ O
We -X- _ O
conclude -X- _ O
that -X- _ O
the -X- _ O
fact -X- _ O
that -X- _ O
our -X- _ O
datasets -X- _ O
include -X- _ O
lemmas -X- _ O
from -X- _ O
diverse -X- _ O
classes -X- _ O
that -X- _ O
may -X- _ O
have -X- _ O
derivational -X- _ O
relations -X- _ O
makes -X- _ O
the -X- _ O
inflection -X- _ O
task -X- _ O
significantly -X- _ O
harder -X- _ O
. -X- _ O
Interestingly -X- _ O
, -X- _ O
the -X- _ O
model -X- _ O
managed -X- _ O
to -X- _ O
predict -X- _ O
the -X- _ O
correct -X- _ O
subject -X- _ O
and -X- _ O
object -X- _ O
affixes -X- _ O
most -X- _ O
of -X- _ O
the -X- _ O
time -X- _ O
. -X- _ O

8 -X- _ O
For -X- _ O
learning -X- _ O
curves -X- _ O
on -X- _ O
the -X- _ O
splits -X- _ O
see -X- _ O
Appendix -X- _ O
A -X- _ O
. -X- _ O

Conclusion -X- _ O

This -X- _ O
paper -X- _ O
proposes -X- _ O
a -X- _ O
transition -X- _ O
of -X- _ O
the -X- _ O
UniMorph -X- _ B-DatasetName
annotation -X- _ O
standard -X- _ O
to -X- _ O
a -X- _ O
layered -X- _ B-MethodName
hierarchical -X- _ I-MethodName
annotation -X- _ I-MethodName
of -X- _ O
features -X- _ O
. -X- _ O
This -X- _ O
revised -X- _ O
schema -X- _ O
caters -X- _ O
for -X- _ O
complex -X- _ O
marking -X- _ O
phenomena -X- _ O
including -X- _ O
multiple -X- _ O
pronominal -X- _ O
agreement -X- _ O
. -X- _ O
We -X- _ O
apply -X- _ O
it -X- _ O
to -X- _ O
Georgian -X- _ O
, -X- _ O
and -X- _ O
construct -X- _ O
a -X- _ O
corresponding -X- _ O
new -X- _ O
dataset -X- _ O
that -X- _ O
is -X- _ O
large -X- _ O
, -X- _ O
balanced -X- _ O
, -X- _ O
complete -X- _ O
with -X- _ O
respect -X- _ O
to -X- _ O
grammatical -X- _ O
phenomena -X- _ O
in -X- _ O
the -X- _ O
Georgian -X- _ O
verb -X- _ O
system -X- _ O
and -X- _ O
verified -X- _ O
by -X- _ O
native -X- _ O
- -X- _ O
speakers -X- _ O
. -X- _ O
Our -X- _ O
experiments -X- _ O
with -X- _ O
a -X- _ O
standard -X- _ O
reinflection -X- _ B-TaskName
model -X- _ O
on -X- _ O
the -X- _ O
old -X- _ O
and -X- _ O
new -X- _ O
Georgian -X- _ O
datasets -X- _ O
shows -X- _ O
that -X- _ O
the -X- _ O
old -X- _ O
UniMorph -X- _ B-DatasetName
dataset -X- _ O
does -X- _ O
not -X- _ O
generalize -X- _ O
well -X- _ O
to -X- _ O
the -X- _ O
new -X- _ O
testset -X- _ O
, -X- _ O
due -X- _ O
to -X- _ O
its -X- _ O
partial -X- _ O
coverage -X- _ O
. -X- _ O
This -X- _ O
work -X- _ O
is -X- _ O
intended -X- _ O
to -X- _ O
encourage -X- _ O
the -X- _ O
community -X- _ O
to -X- _ O
extend -X- _ O
the -X- _ O
annotation -X- _ O
of -X- _ O
different -X- _ O
languages -X- _ O
to -X- _ O
include -X- _ O
phenomena -X- _ O
such -X- _ O
as -X- _ O
polypersonal -X- _ O
agreement -X- _ O
and -X- _ O
others -X- _ O
that -X- _ O
can -X- _ O
be -X- _ O
dealt -X- _ O
with -X- _ O
using -X- _ O
a -X- _ O
hierarchical -X- _ B-MethodName
annotation -X- _ I-MethodName
, -X- _ O
ultimately -X- _ O
leading -X- _ O
to -X- _ O
more -X- _ O
complete -X- _ O
and -X- _ O
consistent -X- _ O
benchmarks -X- _ O
for -X- _ O
studying -X- _ O
non -X- _ O
- -X- _ O
trivial -X- _ O
and -X- _ O
less -X- _ O
- -X- _ O
explored -X- _ O
areas -X- _ O
of -X- _ O
computational -X- _ O
morphology -X- _ O
. -X- _ O

A -X- _ O
Learning -X- _ O
Curves -X- _ O

Fig -X- _ O
. -X- _ O
1 -X- _ O
exemplifies -X- _ O
the -X- _ O
sufficiency -X- _ O
of -X- _ O
our -X- _ O
dataset -X- _ O
for -X- _ O
training -X- _ O
an -X- _ O
inflection -X- _ B-TaskName
model -X- _ O
on -X- _ O
form -X- _ O
- -X- _ O
split -X- _ O
data -X- _ O
as -X- _ O
doubling -X- _ O
the -X- _ O
data -X- _ O
amount -X- _ O
from -X- _ O
4,000 -X- _ O
to -X- _ O
8,000 -X- _ O
yields -X- _ O
relatively -X- _ O
minor -X- _ O
improvement -X- _ O
. -X- _ O
It -X- _ O
also -X- _ O
shows -X- _ O
that -X- _ O
for -X- _ O
the -X- _ O
lemma -X- _ O
- -X- _ O
split -X- _ O
data -X- _ O
, -X- _ O
the -X- _ O
model -X- _ O
completely -X- _ O
fails -X- _ O
. -X- _ O
It -X- _ O
starts -X- _ O
improve -X- _ O
marginally -X- _ O
with -X- _ O
more -X- _ O
than -X- _ O
2,000 -X- _ O
examples -X- _ O
, -X- _ O
although -X- _ O
its -X- _ O
performance -X- _ O
remains -X- _ O
far -X- _ O
from -X- _ O
satisfactory -X- _ O
. -X- _ O
This -X- _ O
leaves -X- _ O
room -X- _ O
for -X- _ O
exploration -X- _ O
of -X- _ O
bootstrapping -X- _ O
and -X- _ O
augmentation -X- _ O
methods -X- _ O
or -X- _ O
more -X- _ O
sophisticated -X- _ O
modeling -X- _ O
to -X- _ O
improve -X- _ O
results -X- _ O
. -X- _ O

B -X- _ O
Tech -X- _ O
- -X- _ O
Spec -X- _ O

All -X- _ O
algorithms -X- _ O
described -X- _ O
in -X- _ O
the -X- _ O
paper -X- _ O
were -X- _ O
executed -X- _ O
on -X- _ O
a -X- _ O
single -X- _ O
machine -X- _ O
equipped -X- _ O
with -X- _ O
one -X- _ O
NVIDIA -X- _ O
TI -X- _ O
- -X- _ O
TAN -X- _ O
Xp -X- _ O
GPU -X- _ O
, -X- _ O
16 -X- _ O
Intel -X- _ O
i7 -X- _ O
- -X- _ O
6900K -X- _ O
( -X- _ O
3.20GHz -X- _ O
) -X- _ O
CPUs -X- _ O
and -X- _ O
126 -X- _ O
GB -X- _ O
RAM -X- _ O
. -X- _ O
Since -X- _ O
the -X- _ O
LSTM -X- _ O
algorithm -X- _ O
was -X- _ O
implemented -X- _ O
on -X- _ O
DyNet -X- _ O
, -X- _ O
there -X- _ O
was -X- _ O
no -X- _ O
need -X- _ O
of -X- _ O
the -X- _ O
GPU -X- _ O
, -X- _ O
and -X- _ O
all -X- _ O
the -X- _ O
calculations -X- _ O
were -X- _ O
done -X- _ O
using -X- _ O
only -X- _ O
the -X- _ O
CPU -X- _ O
. -X- _ O
During -X- _ O
training -X- _ O
, -X- _ O
we -X- _ O
experimented -X- _ O
with -X- _ O
several -X- _ O
values -X- _ O
for -X- _ O
the -X- _ O
hyper -X- _ O
- -X- _ O
parameters -X- _ O
detailed -X- _ O
above -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
for -X- _ O
all -X- _ O
the -X- _ O
combinations -X- _ O
we -X- _ O
tried -X- _ O
, -X- _ O
the -X- _ O
results -X- _ O
barely -X- _ O
changed -X- _ O
both -X- _ O
at -X- _ O
the -X- _ O
form -X- _ O
- -X- _ O
split -X- _ O
setting -X- _ O
and -X- _ O
the -X- _ O
lemma -X- _ O
- -X- _ O
split -X- _ O
setting -X- _ O
. -X- _ O

Acknowledgements -X- _ O

The -X- _ O
first -X- _ O
author -X- _ O
would -X- _ O
like -X- _ O
to -X- _ O
thank -X- _ O
the -X- _ O
native -X- _ O
Georgian -X- _ O
speakers -X- _ O
: -X- _ O
Simon -X- _ O
Guriel -X- _ O
, -X- _ O
Silvia -X- _ O
Guriel -X- _ O
- -X- _ O
Agiashvili -X- _ O
and -X- _ O
Nona -X- _ O
Atanelov -X- _ O
for -X- _ O
their -X- _ O
invaluable -X- _ O
help -X- _ O
in -X- _ O
the -X- _ O
data -X- _ O
annotation -X- _ O
process -X- _ O
. -X- _ O
This -X- _ O
research -X- _ O
was -X- _ O
funded -X- _ O
by -X- _ O
the -X- _ O
European -X- _ O
Research -X- _ O
Council -X- _ O
under -X- _ O
the -X- _ O
European -X- _ O
Union -X- _ O
's -X- _ O
Horizon -X- _ O
2020 -X- _ O
research -X- _ O
and -X- _ O
innovation -X- _ O
programme -X- _ O
( -X- _ O
grant -X- _ O
agreement -X- _ O
No -X- _ O
. -X- _ O
677352 -X- _ O
) -X- _ O
and -X- _ O
by -X- _ O
a -X- _ O
research -X- _ O
grant -X- _ O
from -X- _ O
the -X- _ O
Ministry -X- _ O
of -X- _ O
Science -X- _ O
and -X- _ O
Technology -X- _ O
( -X- _ O
MOST -X- _ O
) -X- _ O
of -X- _ O
the -X- _ O
Israeli -X- _ O
Government -X- _ O
, -X- _ O
for -X- _ O
which -X- _ O
we -X- _ O
are -X- _ O
grateful -X- _ O

Transformed -X- _ O
Protoform -X- _ B-TaskName
Reconstruction -X- _ I-TaskName

Protoform -X- _ B-TaskName
reconstruction -X- _ I-TaskName
is -X- _ O
the -X- _ O
task -X- _ O
of -X- _ O
inferring -X- _ O
how -X- _ O
morphemes -X- _ O
or -X- _ O
words -X- _ O
sounded -X- _ O
in -X- _ O
ancestral -X- _ O
languages -X- _ O
of -X- _ O
a -X- _ O
set -X- _ O
of -X- _ O
daughter -X- _ O
languages -X- _ O
. -X- _ O
Meloni -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
achieved -X- _ O
the -X- _ O
stateof -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
on -X- _ O
Latin -X- _ O
protoform -X- _ B-TaskName
reconstruction -X- _ I-TaskName
with -X- _ O
an -X- _ O
RNN -X- _ B-MethodName
- -X- _ O
based -X- _ O
encoder -X- _ O
- -X- _ O
decoder -X- _ O
with -X- _ O
attention -X- _ O
model -X- _ O
. -X- _ O
We -X- _ O
update -X- _ O
their -X- _ O
model -X- _ O
with -X- _ O
the -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
seq2seq -X- _ O
model -X- _ O
- -X- _ O
the -X- _ O
Transformer -X- _ B-MethodName
. -X- _ O
Our -X- _ O
model -X- _ O
outperforms -X- _ O
their -X- _ O
model -X- _ O
on -X- _ O
a -X- _ O
suite -X- _ O
of -X- _ O
different -X- _ O
metrics -X- _ O
on -X- _ O
two -X- _ O
different -X- _ O
datasets -X- _ O
: -X- _ O
Meloni -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
's -X- _ O
Romance -X- _ B-DatasetName
data -X- _ O
of -X- _ O
8,000 -X- _ O
+ -X- _ O
cognates -X- _ O
( -X- _ O
spanning -X- _ O
5 -X- _ O
languages -X- _ O
) -X- _ O
and -X- _ O
a -X- _ O
Chinese -X- _ B-DatasetName
dataset -X- _ O
( -X- _ O
Hóu -X- _ O
, -X- _ O
2004 -X- _ O
) -X- _ O
of -X- _ O
800 -X- _ O
+ -X- _ O
cognates -X- _ O
( -X- _ O
spanning -X- _ O
39 -X- _ O
varieties -X- _ O
) -X- _ O
. -X- _ O
We -X- _ O
also -X- _ O
probe -X- _ O
our -X- _ O
model -X- _ O
for -X- _ O
potential -X- _ O
phylogenetic -X- _ O
signal -X- _ O
contained -X- _ O
in -X- _ O
the -X- _ O
model -X- _ O
. -X- _ O
Our -X- _ O
code -X- _ O
is -X- _ O
publicly -X- _ O
available -X- _ O
1 -X- _ O
. -X- _ O

Introduction -X- _ O

Languages -X- _ O
change -X- _ O
over -X- _ O
time -X- _ O
and -X- _ O
sometimes -X- _ O
diverge -X- _ O
into -X- _ O
multiple -X- _ O
daughter -X- _ O
languages -X- _ O
. -X- _ O
The -X- _ O
common -X- _ O
ancestor -X- _ O
of -X- _ O
a -X- _ O
set -X- _ O
of -X- _ O
genetically -X- _ O
related -X- _ O
languages -X- _ O
is -X- _ O
their -X- _ O
proto -X- _ O
- -X- _ O
language -X- _ O
. -X- _ O
While -X- _ O
there -X- _ O
are -X- _ O
proto -X- _ O
- -X- _ O
languages -X- _ O
such -X- _ O
as -X- _ O
Latin -X- _ O
that -X- _ O
are -X- _ O
attested -X- _ O
, -X- _ O
they -X- _ O
are -X- _ O
the -X- _ O
exception -X- _ O
2 -X- _ O
. -X- _ O
Reconstructed -X- _ O
words -X- _ O
and -X- _ O
morphemes -X- _ O
in -X- _ O
proto -X- _ O
- -X- _ O
languages -X- _ O
are -X- _ O
called -X- _ O
protoforms -X- _ O
. -X- _ O
The -X- _ O
task -X- _ O
of -X- _ O
reconstructing -X- _ O
unattested -X- _ O
protolanguages -X- _ O
is -X- _ O
called -X- _ O
protoform -X- _ B-TaskName
reconstruction -X- _ I-TaskName
. -X- _ O

Historical -X- _ O
linguists -X- _ O
reconstruct -X- _ O
proto -X- _ O
- -X- _ O
languages -X- _ O
by -X- _ O
identifying -X- _ O
systematic -X- _ O
sound -X- _ O
changes -X- _ O
that -X- _ O
can -X- _ O
be -X- _ O
inferred -X- _ O
from -X- _ O
correspondences -X- _ O
between -X- _ O
attested -X- _ O
daughter -X- _ O
languages -X- _ O
( -X- _ O
see -X- _ O
Table -X- _ O
1 -X- _ O
) -X- _ O
. -X- _ O
They -X- _ O
compare -X- _ O
the -X- _ O
sounds -X- _ O
between -X- _ O
a -X- _ O
set -X- _ O
of -X- _ O
cognates -X- _ O
, -X- _ O
or -X- _ O
words -X- _ O
with -X- _ O
a -X- _ O
common -X- _ O
ancestor -X- _ O
, -X- _ O
to -X- _ O
develop -X- _ O
hypotheses -X- _ O
about -X- _ O
the -X- _ O
types -X- _ O
and -X- _ O
chronologies -X- _ O
of -X- _ O
sound -X- _ O
changes -X- _ O
. -X- _ O
This -X- _ O
task -X- _ O
is -X- _ O
inherently -X- _ O
data -X- _ O
- -X- _ O
constrained -X- _ O
, -X- _ O
especially -X- _ O
for -X- _ O
under -X- _ O
- -X- _ O
documented -X- _ O
languages -X- _ O
. -X- _ O
Such -X- _ O
data -X- _ O
scarcity -X- _ O
makes -X- _ O
it -X- _ O
a -X- _ O
particularly -X- _ O
difficult -X- _ O
task -X- _ O
for -X- _ O
contemporary -X- _ O
neural -X- _ O
network -X- _ O
architectures -X- _ O
such -X- _ O
as -X- _ O
the -X- _ O
Transformer -X- _ O
( -X- _ O
Vaswani -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
, -X- _ O
which -X- _ O
are -X- _ O
data -X- _ O
hungry -X- _ O
. -X- _ O

The -X- _ O
contributions -X- _ O
of -X- _ O
this -X- _ O
paper -X- _ O
are -X- _ O
as -X- _ O
follows -X- _ O
: -X- _ O

• -X- _ O
Application -X- _ O
of -X- _ O
the -X- _ O
Transformer -X- _ B-MethodName
architecture -X- _ O
to -X- _ O
the -X- _ O
protoform -X- _ B-TaskName
reconstruction -X- _ I-TaskName
task -X- _ O
, -X- _ O
achieving -X- _ O
state -X- _ O
of -X- _ O
the -X- _ O
art -X- _ O
performance -X- _ O
, -X- _ O
contrary -X- _ O
to -X- _ O
expectation -X- _ O
. -X- _ O

• -X- _ O
Expansion -X- _ O
of -X- _ O
prior -X- _ O
digital -X- _ O
versions -X- _ O
of -X- _ O
Hóu -X- _ O
( -X- _ O
2004 -X- _ O
) -X- _ O
's -X- _ O
Chinese -X- _ B-DatasetName
dataset -X- _ O
to -X- _ O
include -X- _ O
a -X- _ O
total -X- _ O
of -X- _ O
804 -X- _ O
cognate -X- _ O
sets -X- _ O
across -X- _ O
39 -X- _ O
modern -X- _ O
varieties -X- _ O
and -X- _ O
Middle -X- _ O
Chinese -X- _ O
. -X- _ O

Related -X- _ O
Work -X- _ O

Applying -X- _ O
machine -X- _ O
learning -X- _ O
to -X- _ O
protoform -X- _ B-TaskName
reconstruction -X- _ I-TaskName
is -X- _ O
not -X- _ O
new -X- _ O
. -X- _ O
Bouchard -X- _ O
- -X- _ O
Côté -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2013 -X- _ O
) -X- _ O
learn -X- _ O
an -X- _ O
unsupervised -X- _ O
protoform -X- _ O
reconstruction -X- _ O
model -X- _ O
for -X- _ O
the -X- _ O
large -X- _ O
Oceanic -X- _ O
language -X- _ O
family -X- _ O
using -X- _ O
Monte -X- _ O
Carlo -X- _ O
Expectation -X- _ O
Maximization -X- _ O
( -X- _ O
Dempster -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
1977; -X- _ O
Bouchard -X- _ O
- -X- _ O
Côté -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2008 -X- _ O
, -X- _ O
supervising -X- _ O
the -X- _ O
model -X- _ O
with -X- _ O
a -X- _ O
gold -X- _ O
phylogeny -X- _ O
and -X- _ O
using -X- _ O
a -X- _ O
probabilistic -X- _ O
, -X- _ O
generative -X- _ O
model -X- _ O
of -X- _ O
sound -X- _ O
change -X- _ O
. -X- _ O
He -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2022 -X- _ O
) -X- _ O
modernize -X- _ O
an -X- _ O
earlier -X- _ O
version -X- _ O
of -X- _ O
Bouchard -X- _ O
- -X- _ O
Côté -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2013 -X- _ O
) -X- _ O
's -X- _ O
model -X- _ O
with -X- _ O
RNNs -X- _ B-MethodName
for -X- _ O
a -X- _ O
4 -X- _ O
language -X- _ O
subset -X- _ O
of -X- _ O
Romance -X- _ B-DatasetName
, -X- _ O
but -X- _ O
they -X- _ O
rely -X- _ O
on -X- _ O
a -X- _ O
bigram -X- _ O
language -X- _ O
model -X- _ O
of -X- _ O
Latin -X- _ O
, -X- _ O
making -X- _ O
their -X- _ O
model -X- _ O
technically -X- _ O
not -X- _ O
unsupervised -X- _ O
. -X- _ O
List -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2022 -X- _ O
) -X- _ O
apply -X- _ O
an -X- _ O
SVM -X- _ B-MethodName
classifier -X- _ O
to -X- _ O
supervised -X- _ O
reconstruction -X- _ O
by -X- _ O
treating -X- _ O
sound -X- _ O
correspondences -X- _ O
as -X- _ O
training -X- _ O
examples -X- _ O
. -X- _ O
Note -X- _ O
that -X- _ O
there -X- _ O
were -X- _ O
no -X- _ O
word -X- _ O
boundaries -X- _ O
in -X- _ O
the -X- _ O
input -X- _ O
matrix; -X- _ O
that -X- _ O
is -X- _ O
, -X- _ O
all -X- _ O
sound -X- _ O
correspondences -X- _ O
across -X- _ O
the -X- _ O
training -X- _ O
set -X- _ O
are -X- _ O
flattened -X- _ O
into -X- _ O
one -X- _ O
matrix -X- _ O
. -X- _ O
Furthermore -X- _ O
, -X- _ O
each -X- _ O
language -X- _ O
has -X- _ O
an -X- _ O
independent -X- _ O
phonemic -X- _ O
inventory -X- _ O
. -X- _ O
To -X- _ O
learn -X- _ O
contextual -X- _ O
information -X- _ O
, -X- _ O
the -X- _ O
authors -X- _ O
experiment -X- _ O
with -X- _ O
adding -X- _ O
features -X- _ O
encoding -X- _ O
the -X- _ O
position -X- _ O
of -X- _ O
phonemes -X- _ O
, -X- _ O
among -X- _ O
others -X- _ O
. -X- _ O
Ciobanu -X- _ O
and -X- _ O
Dinu -X- _ O
( -X- _ O
2018 -X- _ O
) -X- _ O
learn -X- _ O
a -X- _ O
conditional -X- _ O
random -X- _ O
field -X- _ O
( -X- _ O
Lafferty -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2001 -X- _ O
) -X- _ O
using -X- _ O
n -X- _ O
- -X- _ O
gram -X- _ O
features -X- _ O
for -X- _ O
supervised -X- _ O
reconstruction -X- _ O
and -X- _ O
ensemble -X- _ O
5 -X- _ O
daughter -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
protoform -X- _ O
models -X- _ O
. -X- _ O
They -X- _ O
use -X- _ O
a -X- _ O
dataset -X- _ O
of -X- _ O
3,218 -X- _ O
complete -X- _ O
cognate -X- _ O
sets -X- _ O
spanning -X- _ O
Latin -X- _ O
( -X- _ O
the -X- _ O
proto -X- _ O
- -X- _ O
language -X- _ O
) -X- _ O
and -X- _ O
5 -X- _ O
Romance -X- _ O
languages -X- _ O
: -X- _ O
Romanian -X- _ O
, -X- _ O
French -X- _ O
, -X- _ O
Italian -X- _ O
, -X- _ O
Spanish -X- _ O
, -X- _ O
Portuguese -X- _ O
. -X- _ O
Meloni -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
employ -X- _ O
a -X- _ O
GRU -X- _ O
- -X- _ O
based -X- _ O
seq2seq -X- _ O
approach -X- _ O
( -X- _ O
Cho -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2014 -X- _ O
) -X- _ O
to -X- _ O
Latin -X- _ O
protoform -X- _ O
reconstruction -X- _ O
and -X- _ O
achieve -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
theart -X- _ O
character -X- _ O
edit -X- _ O
distances -X- _ O
. -X- _ O
They -X- _ O
extend -X- _ O
Dinu -X- _ O
and -X- _ O
Ciobanu -X- _ O
( -X- _ O
2014 -X- _ O
) -X- _ O
's -X- _ O
Romance -X- _ B-DatasetName
data -X- _ O
using -X- _ O
data -X- _ O
from -X- _ O
Wiktionary -X- _ O
- -X- _ O
for -X- _ O
a -X- _ O
total -X- _ O
of -X- _ O
8,799 -X- _ O
cognate -X- _ O
sets -X- _ O
across -X- _ O
5 -X- _ O
Romance -X- _ O
languages -X- _ O
plus -X- _ O
Latin -X- _ O
- -X- _ O
in -X- _ O
both -X- _ O
orthographic -X- _ O
and -X- _ O
phonetic -X- _ O
( -X- _ O
IPA -X- _ O
) -X- _ O
representations -X- _ O
. -X- _ O

Applying -X- _ O
machine -X- _ O
learning -X- _ O
to -X- _ O
protoform -X- _ B-TaskName
reconstruction -X- _ I-TaskName
is -X- _ O
not -X- _ O
new -X- _ O
. -X- _ O
Bouchard -X- _ O
- -X- _ O
Côté -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2013 -X- _ O
) -X- _ O
learn -X- _ O
an -X- _ O
unsupervised -X- _ O
protoform -X- _ O
reconstruction -X- _ O
model -X- _ O
for -X- _ O
the -X- _ O
large -X- _ O
Oceanic -X- _ O
language -X- _ O
family -X- _ O
using -X- _ O
Monte -X- _ O
Carlo -X- _ O
Expectation -X- _ O
Maximization -X- _ O
( -X- _ O
Dempster -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
1977; -X- _ O
Bouchard -X- _ O
- -X- _ O
Côté -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2008 -X- _ O
, -X- _ O
supervising -X- _ O
the -X- _ O
model -X- _ O
with -X- _ O
a -X- _ O
gold -X- _ O
phylogeny -X- _ O
and -X- _ O
using -X- _ O
a -X- _ O
probabilistic -X- _ O
, -X- _ O
generative -X- _ O
model -X- _ O
of -X- _ O
sound -X- _ O
change -X- _ O
. -X- _ O
He -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2022 -X- _ O
) -X- _ O
modernize -X- _ O
an -X- _ O
earlier -X- _ O
version -X- _ O
of -X- _ O
Bouchard -X- _ O
- -X- _ O
Côté -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2013 -X- _ O
) -X- _ O
's -X- _ O
model -X- _ O
with -X- _ O
RNNs -X- _ B-MethodName
for -X- _ O
a -X- _ O
4 -X- _ O
language -X- _ O
subset -X- _ O
of -X- _ O
Romance -X- _ B-DatasetName
, -X- _ O
but -X- _ O
they -X- _ O
rely -X- _ O
on -X- _ O
a -X- _ O
bigram -X- _ O
language -X- _ O
model -X- _ O
of -X- _ O
Latin -X- _ O
, -X- _ O
making -X- _ O
their -X- _ O
model -X- _ O
technically -X- _ O
not -X- _ O
unsupervised -X- _ O
. -X- _ O
List -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2022 -X- _ O
) -X- _ O
apply -X- _ O
an -X- _ O
SVM -X- _ B-MethodName
classifier -X- _ O
to -X- _ O
supervised -X- _ O
reconstruction -X- _ O
by -X- _ O
treating -X- _ O
sound -X- _ O
correspondences -X- _ O
as -X- _ O
training -X- _ O
examples -X- _ O
. -X- _ O
Note -X- _ O
that -X- _ O
there -X- _ O
were -X- _ O
no -X- _ O
word -X- _ O
boundaries -X- _ O
in -X- _ O
the -X- _ O
input -X- _ O
matrix; -X- _ O
that -X- _ O
is -X- _ O
, -X- _ O
all -X- _ O
sound -X- _ O
correspondences -X- _ O
across -X- _ O
the -X- _ O
training -X- _ O
set -X- _ O
are -X- _ O
flattened -X- _ O
into -X- _ O
one -X- _ O
matrix -X- _ O
. -X- _ O
Furthermore -X- _ O
, -X- _ O
each -X- _ O
language -X- _ O
has -X- _ O
an -X- _ O
independent -X- _ O
phonemic -X- _ O
inventory -X- _ O
. -X- _ O
To -X- _ O
learn -X- _ O
contextual -X- _ O
information -X- _ O
, -X- _ O
the -X- _ O
authors -X- _ O
experiment -X- _ O
with -X- _ O
adding -X- _ O
features -X- _ O
encoding -X- _ O
the -X- _ O
position -X- _ O
of -X- _ O
phonemes -X- _ O
, -X- _ O
among -X- _ O
others -X- _ O
. -X- _ O
Ciobanu -X- _ O
and -X- _ O
Dinu -X- _ O
( -X- _ O
2018 -X- _ O
) -X- _ O
learn -X- _ O
a -X- _ O
conditional -X- _ O
random -X- _ O
field -X- _ O
( -X- _ O
Lafferty -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2001 -X- _ O
) -X- _ O
using -X- _ O
n -X- _ O
- -X- _ O
gram -X- _ O
features -X- _ O
for -X- _ O
supervised -X- _ O
reconstruction -X- _ O
and -X- _ O
ensemble -X- _ O
5 -X- _ O
daughter -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
protoform -X- _ O
models -X- _ O
. -X- _ O
They -X- _ O
use -X- _ O
a -X- _ O
dataset -X- _ O
of -X- _ O
3,218 -X- _ O
complete -X- _ O
cognate -X- _ O
sets -X- _ O
spanning -X- _ O
Latin -X- _ O
( -X- _ O
the -X- _ O
proto -X- _ O
- -X- _ O
language -X- _ O
) -X- _ O
and -X- _ O
5 -X- _ O
Romance -X- _ O
languages -X- _ O
: -X- _ O
Romanian -X- _ O
, -X- _ O
French -X- _ O
, -X- _ O
Italian -X- _ O
, -X- _ O
Spanish -X- _ O
, -X- _ O
Portuguese -X- _ O
. -X- _ O
Meloni -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
employ -X- _ O
a -X- _ O
GRU -X- _ O
- -X- _ O
based -X- _ O
seq2seq -X- _ O
approach -X- _ O
( -X- _ O
Cho -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2014 -X- _ O
) -X- _ O
to -X- _ O
Latin -X- _ O
protoform -X- _ O
reconstruction -X- _ O
and -X- _ O
achieve -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
theart -X- _ O
character -X- _ O
edit -X- _ O
distances -X- _ O
. -X- _ O
They -X- _ O
extend -X- _ O
Dinu -X- _ O
and -X- _ O
Ciobanu -X- _ O
( -X- _ O
2014 -X- _ O
) -X- _ O
's -X- _ O
Romance -X- _ B-DatasetName
data -X- _ O
using -X- _ O
data -X- _ O
from -X- _ O
Wiktionary -X- _ O
- -X- _ O
for -X- _ O
a -X- _ O
total -X- _ O
of -X- _ O
8,799 -X- _ O
cognate -X- _ O
sets -X- _ O
across -X- _ O
5 -X- _ O
Romance -X- _ O
languages -X- _ O
plus -X- _ O
Latin -X- _ O
- -X- _ O
in -X- _ O
both -X- _ O
orthographic -X- _ O
and -X- _ O
phonetic -X- _ O
( -X- _ O
IPA -X- _ O
) -X- _ O
representations -X- _ O
. -X- _ O

In -X- _ O
their -X- _ O
model -X- _ O
, -X- _ O
all -X- _ O
entries -X- _ O
comprising -X- _ O
the -X- _ O
cognate -X- _ O
set -X- _ O
are -X- _ O
concatenated -X- _ O
together -X- _ O
in -X- _ O
a -X- _ O
fixed -X- _ O
order -X- _ O
to -X- _ O
form -X- _ O
a -X- _ O
training -X- _ O
example -X- _ O
. -X- _ O
Chang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2022 -X- _ O
) -X- _ O
applied -X- _ O
Meloni -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
's -X- _ O
architecture -X- _ O
to -X- _ O
the -X- _ O
reconstruction -X- _ O
of -X- _ O
Middle -X- _ O
Chinese -X- _ O
on -X- _ O
a -X- _ O
dataset -X- _ O
of -X- _ O
5000 -X- _ O
+ -X- _ O
cognate -X- _ O
sets -X- _ O
spanning -X- _ O
8 -X- _ O
languages -X- _ O
they -X- _ O
compiled -X- _ O
from -X- _ O
Wiktionary -X- _ O
. -X- _ O
3 -X- _ O
Fourrier -X- _ O
( -X- _ O
2022 -X- _ O
) -X- _ O
compares -X- _ O
statistical -X- _ O
machine -X- _ O
translation -X- _ O
, -X- _ O
RNN -X- _ B-MethodName
, -X- _ O
and -X- _ O
Transformer -X- _ B-MethodName
architectures -X- _ O
for -X- _ O
protoform -X- _ B-TaskName
reconstruction -X- _ I-TaskName
, -X- _ O
but -X- _ O
they -X- _ O
evaluate -X- _ O
their -X- _ O
results -X- _ O
using -X- _ O
BLEU -X- _ B-MetricName
scores -X- _ O
( -X- _ O
Papineni -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2002 -X- _ O
) -X- _ O
instead -X- _ O
of -X- _ O
edit -X- _ B-MetricName
distance -X- _ I-MetricName
. -X- _ O
They -X- _ O
find -X- _ O
that -X- _ O
their -X- _ O
Transformer -X- _ B-MethodName
model -X- _ O
did -X- _ O
not -X- _ O
outperform -X- _ O
the -X- _ O
RNN -X- _ B-MethodName
models -X- _ O
on -X- _ O
protoform -X- _ B-TaskName
reconstruction -X- _ I-TaskName
. -X- _ O
In -X- _ O
addition -X- _ O
, -X- _ O
their -X- _ O
multilingual -X- _ O
NMT -X- _ O
( -X- _ O
neural -X- _ O
machine -X- _ O
translation -X- _ O
) -X- _ O
model -X- _ O
predicts -X- _ O
many -X- _ O
languages -X- _ O
instead -X- _ O
of -X- _ O
one -X- _ O
target -X- _ O
language -X- _ O
and -X- _ O
is -X- _ O
trained -X- _ O
on -X- _ O
bilingual -X- _ O
pairs -X- _ O
for -X- _ O
protoform -X- _ B-TaskName
reconstruction -X- _ I-TaskName
( -X- _ O
e.g. -X- _ O
Italian -X- _ O
- -X- _ O
Latin -X- _ O
and -X- _ O
Spanish -X- _ O
- -X- _ O
Latin -X- _ O
) -X- _ O
, -X- _ O
unlike -X- _ O
comparative -X- _ O
reconstruction -X- _ O
. -X- _ O
In -X- _ O
contrast -X- _ O
, -X- _ O
we -X- _ O
encode -X- _ O
the -X- _ O
entire -X- _ O
cognate -X- _ O
set -X- _ O
consisting -X- _ O
of -X- _ O
multiple -X- _ O
daughter -X- _ O
languages -X- _ O
( -X- _ O
5 -X- _ O
for -X- _ O
the -X- _ O
Romance -X- _ B-DatasetName
dataset; -X- _ O
39 -X- _ O
for -X- _ O
Chinese -X- _ B-DatasetName
) -X- _ O
and -X- _ O
predict -X- _ O
the -X- _ O
corresponding -X- _ O
protoform -X- _ O
. -X- _ O

Datasets -X- _ O

We -X- _ O
train -X- _ O
and -X- _ O
test -X- _ O
our -X- _ O
model -X- _ O
on -X- _ O
Romance -X- _ B-DatasetName
and -X- _ O
Sinitic -X- _ B-DatasetName
( -X- _ I-DatasetName
Chinese -X- _ I-DatasetName
) -X- _ I-DatasetName
language -X- _ O
datasets -X- _ O
. -X- _ O
For -X- _ O
Romance -X- _ O
languages -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
Meloni -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
's -X- _ O
dataset -X- _ O
which -X- _ O
consists -X- _ O
of -X- _ O
8,799 -X- _ O
cognate -X- _ O
sets -X- _ O
of -X- _ O
Romanian -X- _ O
, -X- _ O
French -X- _ O
, -X- _ O
Italian -X- _ O
, -X- _ O
Spanish -X- _ O
, -X- _ O
Portuguese -X- _ O
words -X- _ O
and -X- _ O
the -X- _ O
corresponding -X- _ O
Latin -X- _ O
form -X- _ O
( -X- _ O
approximately -X- _ O
, -X- _ O
a -X- _ O
protoform -X- _ O
) -X- _ O
. -X- _ O
There -X- _ O
are -X- _ O
two -X- _ O
versions -X- _ O
of -X- _ O
this -X- _ O
dataset -X- _ O
: -X- _ O
phonetic -X- _ O
and -X- _ O
orthographic -X- _ O
. -X- _ O
The -X- _ O
phonetic -X- _ O
dataset -X- _ O
( -X- _ O
Rom -X- _ B-DatasetName
- -X- _ I-DatasetName
phon -X- _ I-DatasetName
) -X- _ O
represents -X- _ O
words -X- _ O
with -X- _ O
IPA -X- _ O
symbols -X- _ O
whereas -X- _ O
the -X- _ O
orthographic -X- _ O
dataset -X- _ O
( -X- _ O
Rom -X- _ B-DatasetName
- -X- _ I-DatasetName
orth -X- _ I-DatasetName
) -X- _ O
represents -X- _ O
words -X- _ O
in -X- _ O
the -X- _ O
orthographic -X- _ O
form -X- _ O
of -X- _ O
each -X- _ O
language -X- _ O
. -X- _ O
We -X- _ O
preserved -X- _ O
all -X- _ O
diacritics -X- _ O
, -X- _ O
except -X- _ O
for -X- _ O
vowel -X- _ O
length -X- _ O
. -X- _ O
This -X- _ O
dataset -X- _ O
is -X- _ O
an -X- _ O
extension -X- _ O
of -X- _ O
Dinu -X- _ O
and -X- _ O
Ciobanu -X- _ O
( -X- _ O
2014 -X- _ O
) -X- _ O
's -X- _ O
original -X- _ O
dataset -X- _ O
of -X- _ O
3,218 -X- _ O
cognate -X- _ O
sets -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
not -X- _ O
publicly -X- _ O
available -X- _ O
. -X- _ O
Refer -X- _ O
to -X- _ O
Table -X- _ O
2 -X- _ O
for -X- _ O
more -X- _ O
information -X- _ O
. -X- _ O

Expanding -X- _ O
digital -X- _ O
versions -X- _ O
of -X- _ O
Hóu -X- _ O
( -X- _ O
2004 -X- _ O
) -X- _ O

For -X- _ O
Sinitic -X- _ O
languages -X- _ O
, -X- _ O
we -X- _ O
created -X- _ O
a -X- _ O
dataset -X- _ O
of -X- _ O
Middle -X- _ O
Chinese -X- _ O
and -X- _ O
its -X- _ O
modern -X- _ O
daughter -X- _ O
languages -X- _ O
. -X- _ O
Middle -X- _ O
Chinese -X- _ O
is -X- _ O
an -X- _ O
unattested -X- _ O
language -X- _ O
, -X- _ O
and -X- _ O
we -X- _ O
thus -X- _ O
have -X- _ O
to -X- _ O
rely -X- _ O
on -X- _ O
Baxter -X- _ O
and -X- _ O
Sagart -X- _ O
( -X- _ O
2014 -X- _ O
) -X- _ O
's -X- _ O
reconstructions -X- _ O
of -X- _ O
forms -X- _ O
corresponding -X- _ O
to -X- _ O
4,967 -X- _ O
Chinese -X- _ O
characters -X- _ O
. -X- _ O
We -X- _ O
scraped -X- _ O
Wiktionary -X- _ O
to -X- _ O
obtain -X- _ O
Hóu -X- _ O
( -X- _ O
2004 -X- _ O
) -X- _ O
's -X- _ O
phonetic -X- _ O
representations -X- _ O
of -X- _ O
their -X- _ O
modern -X- _ O
reflexes -X- _ O
. -X- _ O
4 -X- _ O
The -X- _ O
resulting -X- _ O
dataset -X- _ O
contains -X- _ O
804 -X- _ O
cognate -X- _ O
sets -X- _ O
of -X- _ O
39 -X- _ O
modern -X- _ O
Sinitic -X- _ O
languages -X- _ O
and -X- _ O
the -X- _ O
corresponding -X- _ O
reconstructed -X- _ O
Middle -X- _ O
Chinese -X- _ O
word -X- _ O
. -X- _ O
List -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
's -X- _ O
version -X- _ O
previously -X- _ O
had -X- _ O
894 -X- _ O
cognate -X- _ O
sets -X- _ O
across -X- _ O
15 -X- _ O
varieties -X- _ O
. -X- _ O

Model -X- _ O

We -X- _ O
propose -X- _ O
a -X- _ O
Transformer -X- _ B-MethodName
- -X- _ O
based -X- _ O
encoder -X- _ O
- -X- _ O
decoder -X- _ O
architecture -X- _ O
( -X- _ O
Vaswani -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
because -X- _ O
such -X- _ O
models -X- _ O
have -X- _ O
produced -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
results -X- _ O
on -X- _ O
many -X- _ O
sequence -X- _ O
processing -X- _ O
tasks -X- _ O
. -X- _ O
Transformers -X- _ O
are -X- _ O
by -X- _ O
reputation -X- _ O
data -X- _ O
hungry -X- _ O
, -X- _ O
though -X- _ O
, -X- _ O
which -X- _ O
poses -X- _ O
a -X- _ O
challenge -X- _ O
to -X- _ O
our -X- _ O
problem -X- _ O
setting -X- _ O
, -X- _ O
where -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
available -X- _ O
training -X- _ O
examples -X- _ O
is -X- _ O
often -X- _ O
very -X- _ O
small -X- _ O
. -X- _ O
Additive -X- _ O
positional -X- _ O
encoding -X- _ O
and -X- _ O
language -X- _ O
embedding -X- _ O
are -X- _ O
applied -X- _ O
to -X- _ O
each -X- _ O
daughter -X- _ O
sequence -X- _ O
before -X- _ O
all -X- _ O
daughter -X- _ O
sequences -X- _ O
are -X- _ O
concatenated -X- _ O
into -X- _ O
a -X- _ O
single -X- _ O
sequence -X- _ O
. -X- _ O

We -X- _ O
modify -X- _ O
the -X- _ O
standard -X- _ O
encoder -X- _ O
- -X- _ O
decoder -X- _ O
architecture -X- _ O
to -X- _ O
accommodate -X- _ O
the -X- _ O
structure -X- _ O
of -X- _ O
our -X- _ O
datasets -X- _ O
, -X- _ O
where -X- _ O
multiple -X- _ O
daughter -X- _ O
sequences -X- _ O
correspond -X- _ O
to -X- _ O
a -X- _ O
single -X- _ O
protoform -X- _ O
sequence -X- _ O
. -X- _ O
Like -X- _ O
Meloni -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
, -X- _ O
the -X- _ O
daughter -X- _ O
sequences -X- _ O
are -X- _ O
concatenated -X- _ O
into -X- _ O
a -X- _ O
single -X- _ O
sequence -X- _ O
before -X- _ O
being -X- _ O
fed -X- _ O
into -X- _ O
the -X- _ O
encoder -X- _ O
. -X- _ O
Because -X- _ O
we -X- _ O
only -X- _ O
care -X- _ O
about -X- _ O
the -X- _ O
relative -X- _ O
position -X- _ O
between -X- _ O
tokens -X- _ O
within -X- _ O
each -X- _ O
daughter -X- _ O
sequence -X- _ O
but -X- _ O
not -X- _ O
across -X- _ O
daughter -X- _ O
sequences -X- _ O
, -X- _ O
positional -X- _ O
encoding -X- _ O
is -X- _ O
applied -X- _ O
to -X- _ O
each -X- _ O
individual -X- _ O
daughter -X- _ O
sequence -X- _ O
before -X- _ O
concatenation -X- _ O
. -X- _ O
Along -X- _ O
with -X- _ O
positional -X- _ O
encoding -X- _ O
, -X- _ O
an -X- _ O
additive -X- _ O
language -X- _ O
embedding -X- _ O
is -X- _ O
applied -X- _ O
to -X- _ O
the -X- _ O
token -X- _ O
embeddings -X- _ O
to -X- _ O
differentiate -X- _ O
between -X- _ O
input -X- _ O
tokens -X- _ O
of -X- _ O
different -X- _ O
daughter -X- _ O
languages -X- _ O
. -X- _ O

Experiments -X- _ O


Baselines -X- _ O

We -X- _ O
compare -X- _ O
our -X- _ O
Transformer -X- _ B-MethodName
model -X- _ O
to -X- _ O
a -X- _ O
variety -X- _ O
of -X- _ O
baselines -X- _ O
. -X- _ O
For -X- _ O
Meloni -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
Chang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2022 -X- _ O
) -X- _ O
's -X- _ O
PyTorch -X- _ O
re -X- _ O
- -X- _ O
implementation -X- _ O
and -X- _ O
reran -X- _ O
a -X- _ O
Bayesian -X- _ O
hyperparameter -X- _ O
search -X- _ O
using -X- _ O
WandB -X- _ O
( -X- _ O
Biewald -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
to -X- _ O
ensure -X- _ O
a -X- _ O
more -X- _ O
fair -X- _ O
comparison -X- _ O
( -X- _ O
since -X- _ O
our -X- _ O
model -X- _ O
is -X- _ O
tuned -X- _ O
with -X- _ O
WandB -X- _ O
as -X- _ O
well -X- _ O
) -X- _ O
. -X- _ O
We -X- _ O
also -X- _ O
include -X- _ O
the -X- _ O
random -X- _ B-MethodName
daughter -X- _ I-MethodName
( -X- _ O
randomly -X- _ O
designate -X- _ O
a -X- _ O
daughter -X- _ O
form -X- _ O
as -X- _ O
the -X- _ O
protoform -X- _ O
and -X- _ O
assume -X- _ O
no -X- _ O
sound -X- _ O
change -X- _ O
) -X- _ O
and -X- _ O
the -X- _ O
majority -X- _ B-MethodName
constituent -X- _ I-MethodName
baselines -X- _ O
( -X- _ O
predict -X- _ O
the -X- _ O
most -X- _ O
common -X- _ O
phoneme -X- _ O
in -X- _ O
each -X- _ O
syllable -X- _ O
constituent -X- _ O
) -X- _ O
from -X- _ O
Chang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2022 -X- _ O
) -X- _ O
. -X- _ O
For -X- _ O
the -X- _ O
SVM -X- _ B-MethodName
and -X- _ O
CoRPaR -X- _ B-MethodName
classifiers -X- _ O
( -X- _ O
List -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
experiment -X- _ O
with -X- _ O
different -X- _ O
contextual -X- _ O
features -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
Pos -X- _ O
( -X- _ O
position -X- _ O
) -X- _ O
, -X- _ O
Str -X- _ O
( -X- _ O
prosodic -X- _ O
structure -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
Ini -X- _ O
( -X- _ O
whether -X- _ O
or -X- _ O
not -X- _ O
the -X- _ O
phoneme -X- _ O
appears -X- _ O
word -X- _ O
- -X- _ O
initially -X- _ O
or -X- _ O
word -X- _ O
- -X- _ O
finally -X- _ O
) -X- _ O
. -X- _ O

We -X- _ O
publish -X- _ O
results -X- _ O
on -X- _ O
Meloni -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
's -X- _ O
full -X- _ O
set -X- _ O
of -X- _ O
8,799 -X- _ O
cognates -X- _ O
but -X- _ O
can -X- _ O
not -X- _ O
redistribute -X- _ O
this -X- _ O
set -X- _ O
due -X- _ O
to -X- _ O
Dinu -X- _ O
and -X- _ O
Ciobanu -X- _ O
( -X- _ O
2014 -X- _ O
) -X- _ O
's -X- _ O
restrictions -X- _ O
. -X- _ O
For -X- _ O
reproducibility -X- _ O
, -X- _ O
we -X- _ O
include -X- _ O
results -X- _ O
on -X- _ O
Meloni -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
's -X- _ O
public -X- _ O
subset -X- _ O
of -X- _ O
5,419 -X- _ O
cognates -X- _ O
in -X- _ O
the -X- _ O
Appendix -X- _ O
( -X- _ O
Table -X- _ O
7 -X- _ O
) -X- _ O
, -X- _ O
both -X- _ O
of -X- _ O
which -X- _ O
include -X- _ O
vowel -X- _ O
length -X- _ O
. -X- _ O
Observe -X- _ O
that -X- _ O
these -X- _ O
results -X- _ O
are -X- _ O
worse -X- _ O
than -X- _ O
those -X- _ O
obtained -X- _ O
on -X- _ O
the -X- _ O
full -X- _ O
set -X- _ O
, -X- _ O
suggesting -X- _ O
that -X- _ O
the -X- _ O
RNN -X- _ B-MethodName
and -X- _ O
Transformer -X- _ B-MethodName
are -X- _ O
dependent -X- _ O
on -X- _ O
a -X- _ O
wealth -X- _ O
of -X- _ O
training -X- _ O
data -X- _ O
. -X- _ O

Preprocessing -X- _ O

In -X- _ O
all -X- _ O
our -X- _ O
datasets -X- _ O
, -X- _ O
we -X- _ O
merge -X- _ O
diacritics -X- _ O
to -X- _ O
their -X- _ O
base -X- _ O
segments -X- _ O
to -X- _ O
form -X- _ O
a -X- _ O
multi -X- _ O
- -X- _ O
character -X- _ O
token -X- _ O
. -X- _ O
For -X- _ O
instance -X- _ O
, -X- _ O
the -X- _ O
sequence -X- _ O
[ -X- _ O
t -X- _ O
, -X- _ O
ʰ -X- _ O
] -X- _ O
is -X- _ O
concatenated -X- _ O
to -X- _ O
[ -X- _ O
tʰ -X- _ O
] -X- _ O
. -X- _ O
This -X- _ O
ensures -X- _ O
that -X- _ O
phonemes -X- _ O
are -X- _ O
treated -X- _ O
as -X- _ O
one -X- _ O
token -X- _ O
. -X- _ O
For -X- _ O
Chinese -X- _ B-DatasetName
, -X- _ O
tone -X- _ O
contours -X- _ O
( -X- _ O
a -X- _ O
sequence -X- _ O
of -X- _ O
tones -X- _ O
) -X- _ O
are -X- _ O
treated -X- _ O
as -X- _ O
one -X- _ O
token -X- _ O
. -X- _ O
When -X- _ O
multiple -X- _ O
pronunciation -X- _ O
variants -X- _ O
are -X- _ O
listed -X- _ O
for -X- _ O
a -X- _ O
single -X- _ O
Chinese -X- _ O
character -X- _ O
, -X- _ O
we -X- _ O
arbitrarily -X- _ O
pick -X- _ O
the -X- _ O
first -X- _ O
one -X- _ O
. -X- _ O

Results -X- _ O
and -X- _ O
Discussion -X- _ O

Evaluation -X- _ O
criteria -X- _ O

We -X- _ O
evaluate -X- _ O
the -X- _ O
predicted -X- _ O
protoforms -X- _ O
using -X- _ O
edit -X- _ B-MetricName
distance -X- _ I-MetricName
( -X- _ O
Levenshtein -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
1966 -X- _ O
) -X- _ O
, -X- _ O
normalized -X- _ B-MetricName
edit -X- _ I-MetricName
distance -X- _ I-MetricName
( -X- _ O
edit -X- _ B-MetricName
distance -X- _ I-MetricName
normalized -X- _ O
by -X- _ O
the -X- _ O
length -X- _ O
of -X- _ O
the -X- _ O
target -X- _ O
) -X- _ O
and -X- _ O
accuracy -X- _ B-MetricName
( -X- _ O
the -X- _ O
percentage -X- _ O
of -X- _ O
protoforms -X- _ O
that -X- _ O
are -X- _ O
reconstructed -X- _ O
without -X- _ O
any -X- _ O
mistakes -X- _ O
) -X- _ O
. -X- _ O
Like -X- _ O
Chang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2022 -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
also -X- _ O
use -X- _ O
feature -X- _ B-MetricName
error -X- _ I-MetricName
rate -X- _ I-MetricName
calculated -X- _ O
using -X- _ O
articulatory -X- _ O
feature -X- _ O
vectors -X- _ O
from -X- _ O
PanPhon -X- _ O
( -X- _ O
Mortensen -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
because -X- _ O
it -X- _ O
reflects -X- _ O
the -X- _ O
phonetic -X- _ O
similarity -X- _ O
between -X- _ O
the -X- _ O
prediction -X- _ O
and -X- _ O
the -X- _ O
gold -X- _ O
protoform -X- _ O
. -X- _ O
For -X- _ O
datasets -X- _ O
with -X- _ O
phonetic -X- _ O
transcriptions -X- _ O
( -X- _ O
Romancephonetic -X- _ B-DatasetName
and -X- _ O
Chinese -X- _ B-DatasetName
) -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
phoneme -X- _ B-MetricName
edit -X- _ I-MetricName
distance -X- _ I-MetricName
and -X- _ O
normalized -X- _ B-MetricName
phoneme -X- _ I-MetricName
edit -X- _ I-MetricName
distance -X- _ I-MetricName
. -X- _ O
As -X- _ O
List -X- _ O
( -X- _ O
2019 -X- _ O
) -X- _ O
suggests -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
B -X- _ B-MetricName
- -X- _ I-MetricName
Cubed -X- _ I-MetricName
F -X- _ I-MetricName
Scores -X- _ I-MetricName
( -X- _ O
Amigó -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2009 -X- _ O
) -X- _ O
to -X- _ O
capture -X- _ O
the -X- _ O
structural -X- _ O
similarity -X- _ O
between -X- _ O
the -X- _ O
gold -X- _ O
and -X- _ O
predicted -X- _ O
protoforms -X- _ O
( -X- _ O
0 -X- _ O
: -X- _ O
structurally -X- _ O
dissimilar -X- _ O
, -X- _ O
1 -X- _ O
: -X- _ O
similar -X- _ O
) -X- _ O
. -X- _ O
With -X- _ O
the -X- _ O
exception -X- _ O
of -X- _ O
character -X- _ B-MetricName
and -X- _ O
phoneme -X- _ B-MetricName
edit -X- _ I-MetricName
distance -X- _ I-MetricName
, -X- _ O
the -X- _ O
metrics -X- _ O
enable -X- _ O
fair -X- _ O
comparison -X- _ O
across -X- _ O
different -X- _ O
language -X- _ O
families -X- _ O
, -X- _ O
which -X- _ O
will -X- _ O
differ -X- _ O
in -X- _ O
the -X- _ O
average -X- _ O
word -X- _ O
length -X- _ O
. -X- _ O

Results -X- _ O

Table -X- _ O
3 -X- _ O
shows -X- _ O
that -X- _ O
our -X- _ O
model -X- _ O
consistently -X- _ O
has -X- _ O
the -X- _ O
best -X- _ O
performance -X- _ O
on -X- _ O
all -X- _ O
datasets -X- _ O
with -X- _ O
regards -X- _ O
to -X- _ O
most -X- _ O
metrics -X- _ O
. -X- _ O
The -X- _ O
results -X- _ O
were -X- _ O
averaged -X- _ O
across -X- _ O
5 -X- _ O
runs -X- _ O
. -X- _ O
Out -X- _ O
of -X- _ O
all -X- _ O
datasets -X- _ O
, -X- _ O
our -X- _ O
model -X- _ O
performs -X- _ O
best -X- _ O
on -X- _ O
the -X- _ O
Rom -X- _ B-DatasetName
- -X- _ I-DatasetName
orth -X- _ I-DatasetName
dataset -X- _ O
, -X- _ O
where -X- _ O
we -X- _ O
achieve -X- _ O
a -X- _ O
7.0 -X- _ B-MetricValue
% -X- _ I-MetricValue
decrease -X- _ O
in -X- _ O
phoneme -X- _ B-MetricName
edit -X- _ I-MetricName
distance -X- _ I-MetricName
and -X- _ O
a -X- _ O
1.43p.p -X- _ B-MetricValue
improvement -X- _ O
in -X- _ O
accuracy -X- _ B-MetricName
relative -X- _ O
to -X- _ O
the -X- _ O
RNN -X- _ B-MethodName
baseline -X- _ O
. -X- _ O
We -X- _ O
observe -X- _ O
the -X- _ O
most -X- _ O
dramatic -X- _ O
performance -X- _ O
difference -X- _ O
with -X- _ O
the -X- _ O
RNN -X- _ B-MethodName
baseline -X- _ O
on -X- _ O
the -X- _ O
Sinitic -X- _ B-DatasetName
dataset -X- _ O
: -X- _ O
a -X- _ O
10.48 -X- _ B-MetricValue
% -X- _ I-MetricValue
decrease -X- _ O
in -X- _ O
phoneme -X- _ B-MetricName
edit -X- _ I-MetricName
distance -X- _ I-MetricName
and -X- _ O
a -X- _ O
5.47p.p -X- _ B-MetricValue
increase -X- _ O
in -X- _ O
accuracy -X- _ B-MetricName
. -X- _ O
For -X- _ O
reproducibility -X- _ O
, -X- _ O
results -X- _ O
on -X- _ O
the -X- _ O
publicly -X- _ O
available -X- _ O
portion -X- _ O
of -X- _ O
the -X- _ O
Rom -X- _ B-DatasetName
- -X- _ I-DatasetName
phon -X- _ I-DatasetName
and -X- _ O
Rom -X- _ B-DatasetName
- -X- _ I-DatasetName
orth -X- _ I-DatasetName
datasets -X- _ O
are -X- _ O
provided -X- _ O
in -X- _ O
Table -X- _ O
7 -X- _ O
in -X- _ O
the -X- _ O
Appendix -X- _ O
. -X- _ O

Analysis -X- _ O

We -X- _ O
observe -X- _ O
that -X- _ O
the -X- _ O
BCFS -X- _ B-MetricName
is -X- _ O
relatively -X- _ O
high -X- _ O
for -X- _ O
the -X- _ O
Romance -X- _ B-DatasetName
non -X- _ O
- -X- _ O
neural -X- _ O
baselines -X- _ O
compared -X- _ O
to -X- _ O
those -X- _ O
of -X- _ O
the -X- _ O
Chinese -X- _ B-DatasetName
ones -X- _ O
. -X- _ O
This -X- _ O
suggests -X- _ O
that -X- _ O
the -X- _ O
sound -X- _ O
changes -X- _ O
in -X- _ O
the -X- _ O
Romance -X- _ B-DatasetName
datasets -X- _ O
are -X- _ O
more -X- _ O
regular -X- _ O
than -X- _ O
that -X- _ O
of -X- _ O
Chinese -X- _ B-DatasetName
, -X- _ O
which -X- _ O
corroborates -X- _ O
List -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2014 -X- _ O
) -X- _ O
's -X- _ O
results -X- _ O
that -X- _ O
more -X- _ O
than -X- _ O
half -X- _ O
of -X- _ O
the -X- _ O
Chinese -X- _ O
characters -X- _ O
in -X- _ O
their -X- _ O
dataset -X- _ O
could -X- _ O
not -X- _ O
be -X- _ O
explained -X- _ O
by -X- _ O
a -X- _ O
tree -X- _ O
model -X- _ O
. -X- _ O
We -X- _ O
examine -X- _ O
the -X- _ O
errors -X- _ O
made -X- _ O
by -X- _ O
the -X- _ O
Transformer -X- _ B-MethodName
model -X- _ O
on -X- _ O
the -X- _ O
Rom -X- _ B-DatasetName
- -X- _ I-DatasetName
phon -X- _ I-DatasetName
datasest -X- _ O
. -X- _ O
Substitutions -X- _ O
constitute -X- _ O
around -X- _ O
61 -X- _ B-MetricValue
% -X- _ I-MetricValue
of -X- _ O
the -X- _ O
errors -X- _ O
made -X- _ O
by -X- _ O
the -X- _ O
Transformer; -X- _ B-MethodName
deletions -X- _ O
, -X- _ O
21 -X- _ B-MetricValue
% -X- _ I-MetricValue
, -X- _ O
and -X- _ O
insertions -X- _ O
, -X- _ O
18 -X- _ B-MetricValue
% -X- _ I-MetricValue
. -X- _ O
The -X- _ O
highest -X- _ O
number -X- _ O
of -X- _ O
substitution -X- _ O
errors -X- _ O
occur -X- _ O
between -X- _ O
[ -X- _ O
i -X- _ O
, -X- _ O
ɪ -X- _ O
] -X- _ O
, -X- _ O
[ -X- _ O
e -X- _ O
, -X- _ O
ɛ -X- _ O
] -X- _ O
, -X- _ O
[ -X- _ O
o -X- _ O
, -X- _ O
ɔ -X- _ O
] -X- _ O
and -X- _ O
[ -X- _ O
u -X- _ O
, -X- _ O
ʊ -X- _ O
] -X- _ O
-vowel -X- _ O
pairs -X- _ O
that -X- _ O
contrast -X- _ O
only -X- _ O
in -X- _ O
tenseness -X- _ O
. -X- _ O
This -X- _ O
is -X- _ O
consistent -X- _ O
with -X- _ O
the -X- _ O
analysis -X- _ O
of -X- _ O
Meloni -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
, -X- _ O
where -X- _ O
substitutions -X- _ O
between -X- _ O
tense -X- _ O
- -X- _ O
lax -X- _ O
vowel -X- _ O
pairs -X- _ O
take -X- _ O
up -X- _ O
the -X- _ O
largest -X- _ O
portion -X- _ O
of -X- _ O
errors -X- _ O
. -X- _ O

We -X- _ O
observe -X- _ O
that -X- _ O
other -X- _ O
common -X- _ O
substitution -X- _ O
errors -X- _ O
also -X- _ O
happen -X- _ O
between -X- _ O
phonemes -X- _ O
that -X- _ O
share -X- _ O
major -X- _ O
phonetic -X- _ O
features -X- _ O
. -X- _ O
This -X- _ O
demonstrates -X- _ O
that -X- _ O
al -X- _ O
- -X- _ O
though -X- _ O
no -X- _ O
explicit -X- _ O
phonetic -X- _ O
information -X- _ O
is -X- _ O
fed -X- _ O
directly -X- _ O
into -X- _ O
the -X- _ O
model -X- _ O
, -X- _ O
the -X- _ O
model -X- _ O
makes -X- _ O
mistakes -X- _ O
motivated -X- _ O
by -X- _ O
phonetic -X- _ O
similarity -X- _ O
, -X- _ O
like -X- _ O
Meloni -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
. -X- _ O

We -X- _ O
do -X- _ O
not -X- _ O
observe -X- _ O
notable -X- _ O
differences -X- _ O
in -X- _ O
the -X- _ O
error -X- _ O
statistics -X- _ O
between -X- _ O
the -X- _ O
Transformer -X- _ B-MethodName
and -X- _ O
the -X- _ O
RNN -X- _ B-MethodName
. -X- _ O

Language -X- _ B-TaskName
relatedness -X- _ I-TaskName

Inspired -X- _ O
by -X- _ O
Fourrier -X- _ O
( -X- _ O
2022 -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
probe -X- _ O
our -X- _ O
model -X- _ O
for -X- _ O
diachronic -X- _ O
information -X- _ O
on -X- _ O
how -X- _ O
genetically -X- _ O
related -X- _ O
each -X- _ O
Romance -X- _ O
language -X- _ O
is -X- _ O
to -X- _ O
each -X- _ O
other -X- _ O
. -X- _ O
We -X- _ O
create -X- _ O
a -X- _ O
distance -X- _ O
matrix -X- _ O
between -X- _ O
every -X- _ O
pair -X- _ O
of -X- _ O
languages -X- _ O
in -X- _ O
a -X- _ O
dataset -X- _ O
by -X- _ O
taking -X- _ O
the -X- _ O
cosine -X- _ B-MetricName
similarity -X- _ I-MetricName
between -X- _ O
a -X- _ O
pair -X- _ O
's -X- _ O
language -X- _ O
embeddings -X- _ O
. -X- _ O
We -X- _ O
then -X- _ O
use -X- _ O
sklearn -X- _ O
( -X- _ O
Pedregosa -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2011 -X- _ O
) -X- _ O
's -X- _ O
implementation -X- _ O
of -X- _ O
the -X- _ O
Ward -X- _ O
variance -X- _ O
minimization -X- _ O
algorithm -X- _ O
( -X- _ O
Ward -X- _ O
Jr -X- _ O
, -X- _ O
1963 -X- _ O
) -X- _ O
to -X- _ O
perform -X- _ O
hierarchical -X- _ O
clustering -X- _ O
on -X- _ O
the -X- _ O
distance -X- _ O
matrix -X- _ O
. -X- _ O
We -X- _ O
take -X- _ O
a -X- _ O
consensus -X- _ O
of -X- _ O
the -X- _ O
dendrograms -X- _ O
from -X- _ O
5 -X- _ O
different -X- _ O
runs -X- _ O
using -X- _ O
the -X- _ O
consense -X- _ O
program -X- _ O
from -X- _ O
PHYLIP -X- _ O
( -X- _ O
Felsenstein -X- _ O
, -X- _ O
2013 -X- _ O
) -X- _ O
. -X- _ O

As -X- _ O
we -X- _ O
see -X- _ O
in -X- _ O
Figure -X- _ O
2 -X- _ O
, -X- _ O
the -X- _ O
Transformer -X- _ B-MethodName
captures -X- _ O
more -X- _ O
of -X- _ O
the -X- _ O
phylogenetic -X- _ O
relationships -X- _ O
among -X- _ O
the -X- _ O
languages -X- _ O
correctly -X- _ O
for -X- _ O
the -X- _ O
Rom -X- _ B-DatasetName
- -X- _ I-DatasetName
phon -X- _ I-DatasetName
dataset -X- _ O
. -X- _ O
Indeed -X- _ O
, -X- _ O
the -X- _ O
Generalized -X- _ B-MetricName
Quartet -X- _ I-MetricName
Distance -X- _ I-MetricName
( -X- _ O
GQD -X- _ B-MetricName
) -X- _ O
( -X- _ O
Sand -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2013; -X- _ O
Pompei -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2011; -X- _ O
Rama -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
between -X- _ O
the -X- _ O
gold -X- _ O
and -X- _ O
predicted -X- _ O
tree -X- _ O
, -X- _ O
calculated -X- _ O
using -X- _ O
quartetDist -X- _ O
from -X- _ O
the -X- _ O
tqDist -X- _ O
library -X- _ O
( -X- _ O
Sand -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2014 -X- _ O
) -X- _ O
, -X- _ O
is -X- _ O
0.4 -X- _ B-MetricValue
for -X- _ O
the -X- _ O
Transformer -X- _ B-MethodName
but -X- _ O
0.8 -X- _ B-MetricValue
for -X- _ O
the -X- _ O
RNN -X- _ B-MethodName
. -X- _ O
See -X- _ O
Figure -X- _ O
5 -X- _ O
in -X- _ O
the -X- _ O
Appendix -X- _ O
for -X- _ O
the -X- _ O
results -X- _ O
of -X- _ O
the -X- _ O
orthographic -X- _ O
dataset -X- _ O
. -X- _ O
Since -X- _ O
the -X- _ O
Romance -X- _ B-DatasetName
dataset -X- _ O
only -X- _ O
includes -X- _ O
5 -X- _ O
daughter -X- _ O
languages -X- _ O
, -X- _ O
our -X- _ O
results -X- _ O
are -X- _ O
insufficient -X- _ O
to -X- _ O
corroborate -X- _ O
or -X- _ O
contradict -X- _ O
Cathcart -X- _ O
and -X- _ O
Wandl -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
's -X- _ O
findings -X- _ O
: -X- _ O
the -X- _ O
more -X- _ O
accurate -X- _ O
the -X- _ O
protoforms -X- _ O
, -X- _ O
the -X- _ O
less -X- _ O
accurate -X- _ O
the -X- _ O
phylogeny -X- _ O
will -X- _ O
be -X- _ O
. -X- _ O
It -X- _ O
is -X- _ O
not -X- _ O
clear -X- _ O
if -X- _ O
the -X- _ O
model -X- _ O
's -X- _ O
language -X- _ O
embeddings -X- _ O
are -X- _ O
learning -X- _ O
information -X- _ O
that -X- _ O
reflects -X- _ O
shared -X- _ O
innovations -X- _ O
( -X- _ O
sound -X- _ O
changes -X- _ O
that -X- _ O
if -X- _ O
shared -X- _ O
among -X- _ O
a -X- _ O
set -X- _ O
of -X- _ O
daughter -X- _ O
languages -X- _ O
, -X- _ O
would -X- _ O
be -X- _ O
acceptable -X- _ O
justification -X- _ O
for -X- _ O
grouping -X- _ O
them -X- _ O
) -X- _ O
-the -X- _ O
only -X- _ O
acceptable -X- _ O
criterion -X- _ O
for -X- _ O
phylogenetic -X- _ O
inference -X- _ O
in -X- _ O
historical -X- _ O
linguistics -X- _ O
( -X- _ O
Campbell -X- _ O
, -X- _ O
2013 -X- _ O
) -X- _ O
-or -X- _ O
if -X- _ O
the -X- _ O
model -X- _ O
is -X- _ O
learning -X- _ O
superficial -X- _ O
phonetic -X- _ O
similarity -X- _ O
. -X- _ O

Conclusion -X- _ O

By -X- _ O
showing -X- _ O
that -X- _ O
Transformers -X- _ B-MethodName
can -X- _ O
outperform -X- _ O
previous -X- _ O
architectures -X- _ O
in -X- _ O
protoform -X- _ B-TaskName
reconstruction -X- _ I-TaskName
despite -X- _ O
the -X- _ O
inherent -X- _ O
data -X- _ O
scarcity -X- _ O
of -X- _ O
the -X- _ O
task -X- _ O
, -X- _ O
our -X- _ O
work -X- _ O
motivates -X- _ O
future -X- _ O
research -X- _ O
in -X- _ O
this -X- _ O
area -X- _ O
to -X- _ O
take -X- _ O
full -X- _ O
advantage -X- _ O
of -X- _ O
the -X- _ O
recent -X- _ O
advancements -X- _ O
in -X- _ O
the -X- _ O
Transformer -X- _ O
space -X- _ O
. -X- _ O

Accurate -X- _ O
supervised -X- _ O
reconstruction -X- _ O
can -X- _ O
help -X- _ O
pre -X- _ O
- -X- _ O
dict -X- _ O
protoforms -X- _ O
for -X- _ O
cognate -X- _ O
sets -X- _ O
where -X- _ O
linguists -X- _ O
have -X- _ O
not -X- _ O
reconstructed -X- _ O
one -X- _ O
yet -X- _ O
. -X- _ O
Future -X- _ O
work -X- _ O
could -X- _ O
reconstruct -X- _ O
proto -X- _ O
- -X- _ O
languages -X- _ O
whose -X- _ O
linguist -X- _ O
reconstructions -X- _ O
are -X- _ O
not -X- _ O
available -X- _ O
, -X- _ O
by -X- _ O
transferring -X- _ O
knowledge -X- _ O
learned -X- _ O
from -X- _ O
languages -X- _ O
with -X- _ O
already -X- _ O
reconstructed -X- _ O
protoforms -X- _ O
. -X- _ O
Furthermore -X- _ O
, -X- _ O
future -X- _ O
work -X- _ O
can -X- _ O
leverage -X- _ O
the -X- _ O
abundance -X- _ O
of -X- _ O
work -X- _ O
in -X- _ O
unsupervised -X- _ O
NMT -X- _ O
to -X- _ O
adapt -X- _ O
our -X- _ O
Transformer -X- _ B-MethodName
model -X- _ O
for -X- _ O
the -X- _ O
unsupervised -X- _ O
setting -X- _ O
, -X- _ O
a -X- _ O
more -X- _ O
realistic -X- _ O
scenario -X- _ O
for -X- _ O
the -X- _ O
historical -X- _ O
linguist -X- _ O
. -X- _ O

Limitations -X- _ O

One -X- _ O
limitation -X- _ O
of -X- _ O
our -X- _ O
work -X- _ O
is -X- _ O
that -X- _ O
the -X- _ O
RNN -X- _ B-MethodName
( -X- _ O
Meloni -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
actually -X- _ O
outperforms -X- _ O
our -X- _ O
Transformer -X- _ B-MethodName
on -X- _ O
the -X- _ O
Chinese -X- _ B-DatasetName
dataset -X- _ O
in -X- _ O
Chang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2022 -X- _ O
) -X- _ O
. -X- _ O
In -X- _ O
addition -X- _ O
, -X- _ O
as -X- _ O
with -X- _ O
other -X- _ O
neural -X- _ O
approaches -X- _ O
, -X- _ O
our -X- _ O
model -X- _ O
requires -X- _ O
significant -X- _ O
amounts -X- _ O
of -X- _ O
data -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
often -X- _ O
not -X- _ O
available -X- _ O
to -X- _ O
historical -X- _ O
linguists -X- _ O
researching -X- _ O
less -X- _ O
well -X- _ O
- -X- _ O
studied -X- _ O
language -X- _ O
families -X- _ O
based -X- _ O
on -X- _ O
field -X- _ O
reports -X- _ O
. -X- _ O
Romance -X- _ B-DatasetName
and -X- _ O
Chinese -X- _ B-DatasetName
have -X- _ O
relatively -X- _ O
many -X- _ O
cognate -X- _ O
sets -X- _ O
because -X- _ O
the -X- _ O
protoforms -X- _ O
are -X- _ O
documented -X- _ O
5 -X- _ O
, -X- _ O
but -X- _ O
a -X- _ O
low -X- _ O
resource -X- _ O
setup -X- _ O
with -X- _ O
200 -X- _ O
cognate -X- _ O
sets -X- _ O
would -X- _ O
not -X- _ O
fare -X- _ O
well -X- _ O
on -X- _ O
our -X- _ O
datahungrier -X- _ O
Transformer -X- _ B-MethodName
model -X- _ O
. -X- _ O
Furthermore -X- _ O
, -X- _ O
concatenating -X- _ O
the -X- _ O
entire -X- _ O
cognate -X- _ O
set -X- _ O
may -X- _ O
not -X- _ O
work -X- _ O
on -X- _ O
language -X- _ O
families -X- _ O
with -X- _ O
hundreds -X- _ O
of -X- _ O
languages -X- _ O
such -X- _ O
as -X- _ O
Oceanic -X- _ O
because -X- _ O
the -X- _ O
input -X- _ O
sequence -X- _ O
would -X- _ O
be -X- _ O
too -X- _ O
long -X- _ O
compared -X- _ O
to -X- _ O
the -X- _ O
output -X- _ O
protoform -X- _ O
sequence -X- _ O
. -X- _ O

Finally -X- _ O
, -X- _ O
we -X- _ O
obtain -X- _ O
our -X- _ O
Chinese -X- _ B-DatasetName
gold -X- _ O
protoforms -X- _ O
from -X- _ O
Baxter -X- _ O
and -X- _ O
Sagart -X- _ O
( -X- _ O
2014 -X- _ O
) -X- _ O
's -X- _ O
Middle -X- _ O
Chinese -X- _ O
reconstruction -X- _ O
, -X- _ O
which -X- _ O
was -X- _ O
actually -X- _ O
a -X- _ O
transcription -X- _ O
of -X- _ O
the -X- _ O
Qieyun -X- _ O
, -X- _ O
a -X- _ O
rhyme -X- _ O
dictionary -X- _ O
. -X- _ O
Norman -X- _ O
and -X- _ O
Coblin -X- _ O
( -X- _ O
1995 -X- _ O
) -X- _ O
disagree -X- _ O
with -X- _ O
relying -X- _ O
on -X- _ O
such -X- _ O
a -X- _ O
philological -X- _ O
source -X- _ O
and -X- _ O
prefer -X- _ O
comparative -X- _ O
reconstructions -X- _ O
that -X- _ O
begin -X- _ O
from -X- _ O
daughter -X- _ O
data -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
there -X- _ O
is -X- _ O
no -X- _ O
available -X- _ O
comparative -X- _ O
reconstruction -X- _ O
of -X- _ O
Middle -X- _ O
Chinese -X- _ O
with -X- _ O
protoforms -X- _ O
corresponding -X- _ O
to -X- _ O
thousands -X- _ O
of -X- _ O
characters -X- _ O
to -X- _ O
use -X- _ O
as -X- _ O
a -X- _ O
gold -X- _ O
standard -X- _ O
. -X- _ O
Be -X- _ O
that -X- _ O
as -X- _ O
it -X- _ O
may -X- _ O
, -X- _ O
it -X- _ O
seems -X- _ O
clear -X- _ O
that -X- _ O
Middle -X- _ O
Chinese -X- _ O
as -X- _ O
recorded -X- _ O
in -X- _ O
the -X- _ O
Qieyun -X- _ O
is -X- _ O
not -X- _ O
identical -X- _ O
to -X- _ O
the -X- _ O
most -X- _ O
recent -X- _ O
ancestor -X- _ O
of -X- _ O
the -X- _ O
Chinese -X- _ O
languages -X- _ O
. -X- _ O
Its -X- _ O
preface -X- _ O
concedes -X- _ O
that -X- _ O
it -X- _ O
is -X- _ O
a -X- _ O
compromise -X- _ O
between -X- _ O
Tang -X- _ O
Dynasty -X- _ O
dialects -X- _ O
. -X- _ O
The -X- _ O
situation -X- _ O
with -X- _ O
Romance -X- _ B-DatasetName
is -X- _ O
, -X- _ O
in -X- _ O
some -X- _ O
ways -X- _ O
, -X- _ O
comparable -X- _ O
. -X- _ O
Classical -X- _ O
Latin -X- _ O
- -X- _ O
the -X- _ O
variety -X- _ O
on -X- _ O
which -X- _ O
we -X- _ O
trainis -X- _ O
not -X- _ O
the -X- _ O
direct -X- _ O
ancestor -X- _ O
of -X- _ O
modern -X- _ O
Romance -X- _ O
languages -X- _ O
. -X- _ O
Instead -X- _ O
, -X- _ O
they -X- _ O
are -X- _ O
descended -X- _ O
from -X- _ O
Vulgar -X- _ O
Latin -X- _ O
or -X- _ O
Proto -X- _ O
- -X- _ O
Romance -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
not -X- _ O
well -X- _ O
- -X- _ O
attested -X- _ O
and -X- _ O
is -X- _ O
primarily -X- _ O
through -X- _ O
graffiti -X- _ O
and -X- _ O
other -X- _ O
informal -X- _ O
inscriptions -X- _ O
. -X- _ O
Proto -X- _ O
- -X- _ O
Romance -X- _ O
reconstructions -X- _ O
are -X- _ O
also -X- _ O
not -X- _ O
exhaustive -X- _ O
. -X- _ O
As -X- _ O
a -X- _ O
result -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
difficult -X- _ O
to -X- _ O
find -X- _ O
a -X- _ O
dataset -X- _ O
like -X- _ O
Meloni -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
with -X- _ O
thousands -X- _ O
of -X- _ O
such -X- _ O
ancestor -X- _ O
forms -X- _ O
. -X- _ O
We -X- _ O
are -X- _ O
also -X- _ O
limited -X- _ O
to -X- _ O
the -X- _ O
faithfulness -X- _ O
of -X- _ O
espeak -X- _ O
- -X- _ O
ng -X- _ O
's -X- _ O
Latin -X- _ O
G2P -X- _ O
, -X- _ O
from -X- _ O
which -X- _ O
Meloni -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
obtain -X- _ O
their -X- _ O
phonetic -X- _ B-DatasetName
Romance -X- _ I-DatasetName
dataset -X- _ O
. -X- _ O

For -X- _ O
most -X- _ O
language -X- _ O
families -X- _ O
, -X- _ O
protoforms -X- _ O
are -X- _ O
not -X- _ O
attested -X- _ O
. -X- _ O
In -X- _ O
fact -X- _ O
, -X- _ O
as -X- _ O
the -X- _ O
term -X- _ O
is -X- _ O
often -X- _ O
used -X- _ O
, -X- _ O
protoform -X- _ O
refers -X- _ O
to -X- _ O
a -X- _ O
form -X- _ O
that -X- _ O
is -X- _ O
inferred -X- _ O
only -X- _ O
through -X- _ O
linguists -X- _ O
' -X- _ O
comparative -X- _ O
method -X- _ O
. -X- _ O
We -X- _ O
adopt -X- _ O
the -X- _ O
other -X- _ O
usage -X- _ O
for -X- _ O
simplicity -X- _ O
. -X- _ O
In -X- _ O
practice -X- _ O
, -X- _ O
our -X- _ O
approach -X- _ O
would -X- _ O
require -X- _ O
reconstructions -X- _ O
made -X- _ O
by -X- _ O
a -X- _ O
linguist -X- _ O
to -X- _ O
serve -X- _ O
as -X- _ O
training -X- _ O
labels -X- _ O
for -X- _ O
cognate -X- _ O
sets -X- _ O
. -X- _ O

A -X- _ O
Training -X- _ O

We -X- _ O
split -X- _ O
70 -X- _ B-HyperparameterValue
% -X- _ I-HyperparameterValue
, -X- _ O
10 -X- _ B-HyperparameterValue
% -X- _ I-HyperparameterValue
, -X- _ O
and -X- _ O
20 -X- _ B-HyperparameterValue
% -X- _ I-HyperparameterValue
of -X- _ O
our -X- _ O
dataset -X- _ O
into -X- _ O
train -X- _ B-HyperparameterName
, -X- _ O
validation -X- _ B-HyperparameterName
, -X- _ O
and -X- _ O
test -X- _ B-HyperparameterName
sets -X- _ I-HyperparameterName
, -X- _ O
respectively -X- _ O
. -X- _ O
We -X- _ O
conduct -X- _ O
hyperparameter -X- _ O
searches -X- _ O
using -X- _ O
WandB -X- _ O
( -X- _ O
Biewald -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
and -X- _ O
use -X- _ O
early -X- _ O
stopping -X- _ O
, -X- _ O
picking -X- _ O
the -X- _ O
epoch -X- _ B-HyperparameterName
with -X- _ O
lowest -X- _ O
edit -X- _ B-MetricName
distance -X- _ I-MetricName
on -X- _ O
validation -X- _ O
data -X- _ O
. -X- _ O
All -X- _ O
experiments -X- _ O
are -X- _ O
performed -X- _ O
on -X- _ O
a -X- _ O
Ubuntu -X- _ O
server -X- _ O
with -X- _ O
4 -X- _ O
GPUs -X- _ O
and -X- _ O
20 -X- _ O
CPUs -X- _ O
. -X- _ O
For -X- _ O
both -X- _ O
the -X- _ O
RNN -X- _ B-MethodName
and -X- _ O
the -X- _ O
Transformer -X- _ B-MethodName
, -X- _ O
Meloni -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
's -X- _ O
dataset -X- _ O
takes -X- _ O
less -X- _ O
than -X- _ O
7 -X- _ O
GPU -X- _ O
hours -X- _ O
to -X- _ O
run -X- _ O
, -X- _ O
while -X- _ O
Hóu -X- _ O
( -X- _ O
2004 -X- _ O
) -X- _ O
takes -X- _ O
less -X- _ O
than -X- _ O
1 -X- _ O
GPU -X- _ O
hour -X- _ O
. -X- _ O
For -X- _ O
the -X- _ O
large -X- _ O
Romance -X- _ B-DatasetName
orthographic -X- _ I-DatasetName
dataset -X- _ O
, -X- _ O
the -X- _ O
RNN -X- _ B-MethodName
model -X- _ O
has -X- _ O
around -X- _ O
480,000 -X- _ B-HyperparameterValue
parameters -X- _ B-HyperparameterName
, -X- _ O
while -X- _ O
the -X- _ O
Transformer -X- _ B-MethodName
has -X- _ O
around -X- _ O
800,000 -X- _ B-HyperparameterValue
parameters -X- _ B-HyperparameterName
. -X- _ O

B -X- _ O
Hyper -X- _ O
- -X- _ O
parameters -X- _ O

Refer -X- _ O
to -X- _ O
Table -X- _ O
5 -X- _ O
and -X- _ O
Table -X- _ O
6 -X- _ O
for -X- _ O
the -X- _ O
best -X- _ O
hyperparameters -X- _ O
we -X- _ O
found -X- _ O
during -X- _ O
hyperparameter -X- _ O
search -X- _ O
via -X- _ O
WandB -X- _ O
. -X- _ O

C -X- _ O
Supplementary -X- _ O
Results -X- _ O

In -X- _ O
order -X- _ O
to -X- _ O
compare -X- _ O
our -X- _ O
model -X- _ O
to -X- _ O
earlier -X- _ O
work -X- _ O
, -X- _ O
we -X- _ O
used -X- _ O
the -X- _ O
Rom -X- _ B-DatasetName
- -X- _ I-DatasetName
phon -X- _ I-DatasetName
and -X- _ O
Rom -X- _ B-DatasetName
- -X- _ I-DatasetName
orth -X- _ I-DatasetName
datasets -X- _ O
from -X- _ O
Meloni -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
this -X- _ O
set -X- _ O
includes -X- _ O
a -X- _ O
subset -X- _ O
from -X- _ O
Ciobanu -X- _ O
and -X- _ O
Dinu -X- _ O
( -X- _ O
2018 -X- _ O
) -X- _ O
which -X- _ O
is -X- _ O
not -X- _ O
freely -X- _ O
redistributable -X- _ O
. -X- _ O
So -X- _ O
that -X- _ O
our -X- _ O
results -X- _ O
can -X- _ O
be -X- _ O
reproduced -X- _ O
, -X- _ O
we -X- _ O
also -X- _ O
computed -X- _ O
them -X- _ O
on -X- _ O
the -X- _ O
publicly -X- _ O
available -X- _ O
subset -X- _ O
of -X- _ O
Meloni -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
's -X- _ O
dataset -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
presented -X- _ O
in -X- _ O
Table -X- _ O
7 -X- _ O
. -X- _ O

Phylogenetic -X- _ O
trees -X- _ O
for -X- _ O
Chinese -X- _ O
were -X- _ O
also -X- _ O
extracted -X- _ O
from -X- _ O
the -X- _ O
RNN -X- _ B-MethodName
and -X- _ O
Transformer -X- _ B-MethodName
models -X- _ O
. -X- _ O
These -X- _ O
are -X- _ O
shown -X- _ O
in -X- _ O
Figures -X- _ O
3 -X- _ O
and -X- _ O
4 -X- _ O
. -X- _ O

We -X- _ O
also -X- _ O
plot -X- _ O
the -X- _ O
dendrograms -X- _ O
derived -X- _ O
from -X- _ O
the -X- _ O
Rom -X- _ B-DatasetName
- -X- _ I-DatasetName
orto -X- _ I-DatasetName
dataset -X- _ O
in -X- _ O
Figure -X- _ O
5 -X- _ O
. -X- _ O
C1 -X- _ O
. -X- _ O
Did -X- _ O
you -X- _ O
report -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
parameters -X- _ O
in -X- _ O
the -X- _ O
models -X- _ O
used -X- _ O
, -X- _ O
the -X- _ O
total -X- _ O
computational -X- _ O
budget -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
GPU -X- _ O
hours -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
computing -X- _ O
infrastructure -X- _ O
used -X- _ O
? -X- _ O
Appendix -X- _ O
A -X- _ O

Acknowledgements -X- _ O

We -X- _ O
would -X- _ O
like -X- _ O
to -X- _ O
thank -X- _ O
Liang -X- _ O
( -X- _ O
Leon -X- _ O
) -X- _ O
Lu -X- _ O
for -X- _ O
finding -X- _ O
a -X- _ O
bug -X- _ O
in -X- _ O
our -X- _ O
implementation -X- _ O
, -X- _ O
Ying -X- _ O
Chen -X- _ O
for -X- _ O
writing -X- _ O
the -X- _ O
code -X- _ O
for -X- _ O
the -X- _ O
baselines -X- _ O
, -X- _ O
and -X- _ O
Brendon -X- _ O
Boldt -X- _ O
and -X- _ O
Graham -X- _ O
Neubig -X- _ O
for -X- _ O
providing -X- _ O
useful -X- _ O
feedba -X- _ O

DiffusionNER -X- _ B-MethodName
: -X- _ O
Boundary -X- _ O
Diffusion -X- _ O
for -X- _ O
Named -X- _ B-TaskName
Entity -X- _ I-TaskName
Recognition -X- _ I-TaskName

In -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
DIFFUSIONNER -X- _ B-MethodName
, -X- _ O
which -X- _ O
formulates -X- _ O
the -X- _ O
named -X- _ B-TaskName
entity -X- _ I-TaskName
recognition -X- _ I-TaskName
task -X- _ I-TaskName
as -X- _ O
a -X- _ O
boundary -X- _ O
- -X- _ O
denoising -X- _ O
diffusion -X- _ O
process -X- _ O
and -X- _ O
thus -X- _ O
generates -X- _ O
named -X- _ O
entities -X- _ O
from -X- _ O
noisy -X- _ O
spans -X- _ O
. -X- _ O
During -X- _ O
training -X- _ O
, -X- _ O
DIFFUSIONNER -X- _ B-MethodName
gradually -X- _ O
adds -X- _ O
noises -X- _ O
to -X- _ O
the -X- _ O
golden -X- _ O
entity -X- _ O
boundaries -X- _ O
by -X- _ O
a -X- _ O
fixed -X- _ O
forward -X- _ O
diffusion -X- _ O
process -X- _ O
and -X- _ O
learns -X- _ O
a -X- _ O
reverse -X- _ O
diffusion -X- _ O
process -X- _ O
to -X- _ O
recover -X- _ O
the -X- _ O
entity -X- _ O
boundaries -X- _ O
. -X- _ O
In -X- _ O
inference -X- _ O
, -X- _ O
DIFFU -X- _ B-MethodName
- -X- _ I-MethodName
SIONNER -X- _ I-MethodName
first -X- _ O
randomly -X- _ O
samples -X- _ O
some -X- _ O
noisy -X- _ O
spans -X- _ O
from -X- _ O
a -X- _ O
standard -X- _ O
Gaussian -X- _ O
distribution -X- _ O
and -X- _ O
then -X- _ O
generates -X- _ O
the -X- _ O
named -X- _ O
entities -X- _ O
by -X- _ O
denoising -X- _ O
them -X- _ O
with -X- _ O
the -X- _ O
learned -X- _ O
reverse -X- _ O
diffusion -X- _ O
process -X- _ O
. -X- _ O
The -X- _ O
proposed -X- _ O
boundary -X- _ O
- -X- _ O
denoising -X- _ O
diffusion -X- _ O
process -X- _ O
allows -X- _ O
progressive -X- _ O
refinement -X- _ O
and -X- _ O
dynamic -X- _ O
sampling -X- _ O
of -X- _ O
entities -X- _ O
, -X- _ O
empowering -X- _ O
DIFFUSIONNER -X- _ B-MethodName
with -X- _ O
efficient -X- _ O
and -X- _ O
flexible -X- _ O
entity -X- _ O
generation -X- _ O
capability -X- _ O
. -X- _ O
Experiments -X- _ O
on -X- _ O
multiple -X- _ O
flat -X- _ O
and -X- _ O
nested -X- _ O
NER -X- _ B-TaskName
datasets -X- _ O
demonstrate -X- _ O
that -X- _ O
DIFFUSIONNER -X- _ B-MethodName
achieves -X- _ O
comparable -X- _ O
or -X- _ O
even -X- _ O
better -X- _ O
performance -X- _ O
than -X- _ O
previous -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
models -X- _ O
1 -X- _ O
. -X- _ O

Named -X- _ B-TaskName
Entity -X- _ I-TaskName
Recognition -X- _ I-TaskName
( -X- _ O
NER -X- _ O
) -X- _ O
is -X- _ O
a -X- _ O
basic -X- _ O
task -X- _ O
of -X- _ O
information -X- _ O
extraction -X- _ O
( -X- _ O
Tjong -X- _ O
Kim -X- _ O
Sang -X- _ O
and -X- _ O
De -X- _ O
Meulder -X- _ O
, -X- _ O
2003 -X- _ O
) -X- _ O
, -X- _ O
which -X- _ O
aims -X- _ O
to -X- _ O
locate -X- _ O
entity -X- _ O
mentions -X- _ O
and -X- _ O
label -X- _ O
specific -X- _ O
entity -X- _ O
types -X- _ O
such -X- _ O
as -X- _ O
person -X- _ O
, -X- _ O
location -X- _ O
, -X- _ O
and -X- _ O
organization -X- _ O
. -X- _ O
It -X- _ O
is -X- _ O
fundamental -X- _ O
to -X- _ O
many -X- _ O
structured -X- _ O
information -X- _ O
extraction -X- _ O
tasks -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
relation -X- _ O
extraction -X- _ O
( -X- _ O
Li -X- _ O
and -X- _ O
Ji -X- _ O
, -X- _ O
2014 -X- _ O
; -X- _ O
Miwa -X- _ O
and -X- _ O
Bansal -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
and -X- _ O
event -X- _ O
extraction -X- _ O
( -X- _ O
McClosky -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2011 -X- _ O
; -X- _ O
Wadden -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
. -X- _ O

Most -X- _ O
traditional -X- _ O
methods -X- _ O
( -X- _ O
Chiu -X- _ O
and -X- _ O
Nichols -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
formulate -X- _ O
the -X- _ O
NER -X- _ B-TaskName
task -X- _ O
into -X- _ O
a -X- _ O
sequence -X- _ O
labeling -X- _ O
task -X- _ O
by -X- _ O
assigning -X- _ O
a -X- _ O
single -X- _ O
label -X- _ O
to -X- _ O
each -X- _ O
token -X- _ O
. -X- _ O
To -X- _ O
accommodate -X- _ O
the -X- _ O
nested -X- _ O
structure -X- _ O
between -X- _ O
entities -X- _ O
, -X- _ O
some -X- _ O
methods -X- _ O
( -X- _ O
Ju -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
; -X- _ O
Figure -X- _ O
1 -X- _ O
: -X- _ O
Boundary -X- _ O
diffusion -X- _ O
in -X- _ O
named -X- _ O
entity -X- _ O
recognition -X- _ O
. -X- _ O
The -X- _ O
fixed -X- _ O
forward -X- _ O
diffusion -X- _ O
process -X- _ O
adds -X- _ O
Gaussian -X- _ O
noise -X- _ O
to -X- _ O
the -X- _ O
entity -X- _ O
boundaries -X- _ O
at -X- _ O
each -X- _ O
timestep -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
noisy -X- _ O
boundaries -X- _ O
recover -X- _ O
their -X- _ O
original -X- _ O
state -X- _ O
by -X- _ O
denoising -X- _ O
with -X- _ O
the -X- _ O
learnable -X- _ O
reverse -X- _ O
diffusion -X- _ O
process -X- _ O
. -X- _ O
For -X- _ O
inference -X- _ O
, -X- _ O
the -X- _ O
reverse -X- _ O
diffusion -X- _ O
process -X- _ O
generates -X- _ O
entity -X- _ O
boundaries -X- _ O
and -X- _ O
performs -X- _ O
entity -X- _ O
typing -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
noisy -X- _ O
spans -X- _ O
sampled -X- _ O
from -X- _ O
the -X- _ O
Gaussian -X- _ O
distribution -X- _ O
. -X- _ O
2020 -X- _ O
) -X- _ O
further -X- _ O
devise -X- _ O
cascaded -X- _ O
or -X- _ O
stacked -X- _ O
tagging -X- _ O
strategies -X- _ O
. -X- _ O
Another -X- _ O
class -X- _ O
of -X- _ O
methods -X- _ O
treat -X- _ O
NER -X- _ B-TaskName
as -X- _ O
a -X- _ O
classification -X- _ O
task -X- _ O
on -X- _ O
text -X- _ O
spans -X- _ O
( -X- _ O
Sohrab -X- _ O
and -X- _ O
Eberts -X- _ O
and -X- _ O
Ulges -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
assign -X- _ O
labels -X- _ O
to -X- _ O
word -X- _ O
pairs -X- _ O
( -X- _ O
Yu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
; -X- _ O
Li -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022a -X- _ O
) -X- _ O
or -X- _ O
potential -X- _ O
spans -X- _ O
( -X- _ O
Lin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Shen -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021a -X- _ O
) -X- _ O
. -X- _ O
In -X- _ O
contrast -X- _ O
to -X- _ O
the -X- _ O
above -X- _ O
works -X- _ O
, -X- _ O
some -X- _ O
pioneer -X- _ O
works -X- _ O
( -X- _ O
Paolini -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
; -X- _ O
Yan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021b -X- _ O
; -X- _ O
propose -X- _ O
generative -X- _ O
NER -X- _ B-TaskName
methods -X- _ O
that -X- _ O
formulate -X- _ O
NER -X- _ B-TaskName
as -X- _ O
a -X- _ O
sequence -X- _ O
generation -X- _ O
task -X- _ O
by -X- _ O
translating -X- _ O
structured -X- _ O
entities -X- _ O
into -X- _ O
a -X- _ O
linearized -X- _ O
text -X- _ O
sequence -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
autoregressive -X- _ O
manner -X- _ O
, -X- _ O
the -X- _ O
generation -X- _ O
- -X- _ O
based -X- _ O
methods -X- _ O
suffer -X- _ O
from -X- _ O
inefficient -X- _ O
decoding -X- _ O
. -X- _ O
In -X- _ O
addition -X- _ O
, -X- _ O
the -X- _ O
discrepancy -X- _ O
between -X- _ O
training -X- _ O
and -X- _ O
evaluation -X- _ O
leads -X- _ O
to -X- _ O
exposure -X- _ O
bias -X- _ O
that -X- _ O
impairs -X- _ O
the -X- _ O
model -X- _ O
performance -X- _ O
. -X- _ O

We -X- _ O
move -X- _ O
to -X- _ O
another -X- _ O
powerful -X- _ O
generative -X- _ O
model -X- _ O
for -X- _ O
NER -X- _ B-TaskName
, -X- _ O
namely -X- _ O
the -X- _ O
diffusion -X- _ O
model -X- _ O
. -X- _ O
As -X- _ O
a -X- _ O
class -X- _ O
of -X- _ O
deep -X- _ O
latent -X- _ O
generative -X- _ O
models -X- _ O
, -X- _ O
diffusion -X- _ O
models -X- _ O
have -X- _ O
achieved -X- _ O
impressive -X- _ O
results -X- _ O
on -X- _ O
image -X- _ O
, -X- _ O
audio -X- _ O
and -X- _ O
text -X- _ O
generation -X- _ O
( -X- _ O
Rombach -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
; -X- _ O
Ramesh -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
; -X- _ O
Kong -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
; -X- _ O
Li -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022b -X- _ O
; -X- _ O
Gong -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
. -X- _ O
The -X- _ O
core -X- _ O
idea -X- _ O
of -X- _ O
diffusion -X- _ O
models -X- _ O
is -X- _ O
to -X- _ O
systematically -X- _ O
perturb -X- _ O
the -X- _ O
data -X- _ O
through -X- _ O
a -X- _ O
forward -X- _ O
diffusion -X- _ O
process -X- _ O
, -X- _ O
and -X- _ O
then -X- _ O
recover -X- _ O
the -X- _ O
data -X- _ O
by -X- _ O
learning -X- _ O
a -X- _ O
reverse -X- _ O
diffusion -X- _ O
process -X- _ O
. -X- _ O

Inspired -X- _ O
by -X- _ O
this -X- _ O
, -X- _ O
we -X- _ O
present -X- _ O
DIFFUSIONNER -X- _ B-MethodName
, -X- _ O
a -X- _ O
new -X- _ O
generative -X- _ O
framework -X- _ O
for -X- _ O
named -X- _ B-TaskName
entity -X- _ I-TaskName
recognition -X- _ I-TaskName
, -X- _ O
which -X- _ O
formulates -X- _ O
NER -X- _ B-TaskName
as -X- _ O
a -X- _ O
denoising -X- _ O
diffusion -X- _ O
process -X- _ O
( -X- _ O
Sohl -X- _ O
- -X- _ O
Dickstein -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
; -X- _ O
Ho -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
on -X- _ O
entity -X- _ O
boundaries -X- _ O
and -X- _ O
generates -X- _ O
entities -X- _ O
from -X- _ O
noisy -X- _ O
spans -X- _ O
. -X- _ O
As -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
1 -X- _ O
, -X- _ O
during -X- _ O
training -X- _ O
, -X- _ O
we -X- _ O
add -X- _ O
Gaussian -X- _ O
noise -X- _ O
to -X- _ O
the -X- _ O
entity -X- _ O
boundaries -X- _ O
step -X- _ O
by -X- _ O
step -X- _ O
in -X- _ O
the -X- _ O
forward -X- _ O
diffusion -X- _ O
process -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
noisy -X- _ O
spans -X- _ O
are -X- _ O
progressively -X- _ O
denoised -X- _ O
by -X- _ O
a -X- _ O
reverse -X- _ O
diffusion -X- _ O
process -X- _ O
to -X- _ O
recover -X- _ O
the -X- _ O
original -X- _ O
entity -X- _ O
boundaries -X- _ O
. -X- _ O
The -X- _ O
forward -X- _ O
process -X- _ O
is -X- _ O
fixed -X- _ O
and -X- _ O
determined -X- _ O
by -X- _ O
the -X- _ O
variance -X- _ O
schedule -X- _ O
of -X- _ O
the -X- _ O
Gaussian -X- _ O
Markov -X- _ O
chains -X- _ O
, -X- _ O
while -X- _ O
the -X- _ O
reverse -X- _ O
process -X- _ O
requires -X- _ O
learning -X- _ O
a -X- _ O
denoising -X- _ O
network -X- _ O
that -X- _ O
progressively -X- _ O
refines -X- _ O
the -X- _ O
entity -X- _ O
boundaries -X- _ O
. -X- _ O
For -X- _ O
inference -X- _ O
, -X- _ O
we -X- _ O
first -X- _ O
sample -X- _ O
noisy -X- _ O
spans -X- _ O
from -X- _ O
a -X- _ O
prior -X- _ O
Gaussian -X- _ O
distribution -X- _ O
and -X- _ O
then -X- _ O
generate -X- _ O
entity -X- _ O
boundaries -X- _ O
using -X- _ O
the -X- _ O
learned -X- _ O
reverse -X- _ O
diffusion -X- _ O
process -X- _ O
. -X- _ O

Empowered -X- _ O
by -X- _ O
the -X- _ O
diffusion -X- _ O
model -X- _ O
, -X- _ O
DIFFUSION -X- _ B-MethodName
- -X- _ I-MethodName
NER -X- _ I-MethodName
presents -X- _ O
three -X- _ O
advantages -X- _ O
. -X- _ O
First -X- _ O
, -X- _ O
the -X- _ O
iterative -X- _ O
denoising -X- _ O
process -X- _ O
of -X- _ O
the -X- _ O
diffusion -X- _ O
model -X- _ O
gives -X- _ O
DIFFUSIONNER -X- _ B-MethodName
the -X- _ O
ability -X- _ O
to -X- _ O
progressively -X- _ O
refine -X- _ O
the -X- _ O
entity -X- _ O
boundaries -X- _ O
, -X- _ O
thus -X- _ O
improve -X- _ O
performance -X- _ O
. -X- _ O
Second -X- _ O
, -X- _ O
independent -X- _ O
of -X- _ O
the -X- _ O
predefined -X- _ O
number -X- _ O
of -X- _ O
noisy -X- _ O
spans -X- _ O
in -X- _ O
the -X- _ O
training -X- _ O
stage -X- _ O
, -X- _ O
DIF -X- _ B-MethodName
- -X- _ I-MethodName
FUSIONNER -X- _ I-MethodName
can -X- _ O
sample -X- _ O
a -X- _ O
different -X- _ O
number -X- _ O
of -X- _ O
noisy -X- _ O
spans -X- _ O
to -X- _ O
decode -X- _ O
entities -X- _ O
during -X- _ O
evaluation -X- _ O
. -X- _ O
Such -X- _ O
dynamic -X- _ O
entity -X- _ O
sampling -X- _ O
makes -X- _ O
more -X- _ O
sense -X- _ O
in -X- _ O
real -X- _ O
scenarios -X- _ O
where -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
entities -X- _ O
is -X- _ O
arbitrary -X- _ O
. -X- _ O
Third -X- _ O
, -X- _ O
different -X- _ O
from -X- _ O
the -X- _ O
autoregressive -X- _ O
manner -X- _ O
in -X- _ O
generation -X- _ O
- -X- _ O
based -X- _ O
methods -X- _ O
, -X- _ O
DIFFUSION -X- _ B-MethodName
- -X- _ I-MethodName
NER -X- _ I-MethodName
can -X- _ O
generate -X- _ O
all -X- _ O
entities -X- _ O
in -X- _ O
parallel -X- _ O
within -X- _ O
several -X- _ O
denoising -X- _ O
timesteps -X- _ O
. -X- _ O
In -X- _ O
addition -X- _ O
, -X- _ O
the -X- _ O
shared -X- _ O
encoder -X- _ O
across -X- _ O
timesteps -X- _ O
can -X- _ O
further -X- _ O
speed -X- _ O
up -X- _ O
inference -X- _ O
. -X- _ O
We -X- _ O
will -X- _ O
further -X- _ O
analyze -X- _ O
these -X- _ O
advantages -X- _ O
of -X- _ O
DIFFUSIONNER -X- _ B-MethodName
in -X- _ O
§ -X- _ O
6.2 -X- _ O
. -X- _ O
In -X- _ O
summary -X- _ O
, -X- _ O
our -X- _ O
main -X- _ O
contributions -X- _ O
are -X- _ O
as -X- _ O
follows -X- _ O
: -X- _ O

• -X- _ O
DIFFUSIONNER -X- _ B-MethodName
is -X- _ O
the -X- _ O
first -X- _ O
to -X- _ O
use -X- _ O
the -X- _ O
diffusion -X- _ O
model -X- _ O
for -X- _ O
NER -X- _ B-DatasetName
, -X- _ O
an -X- _ O
extractive -X- _ O
task -X- _ O
on -X- _ O
discrete -X- _ O
text -X- _ O
sequences -X- _ O
. -X- _ O
Our -X- _ O
exploration -X- _ O
provides -X- _ O
a -X- _ O
new -X- _ O
perspective -X- _ O
on -X- _ O
diffusion -X- _ O
models -X- _ O
in -X- _ O
natural -X- _ O
language -X- _ O
understanding -X- _ O
tasks -X- _ O
. -X- _ O

• -X- _ O
DIFFUSIONNER -X- _ B-MethodName
formulates -X- _ O
named -X- _ B-TaskName
entity -X- _ I-TaskName
recognition -X- _ I-TaskName
as -X- _ O
a -X- _ O
boundary -X- _ O
denoising -X- _ O
diffusion -X- _ O
process -X- _ O
from -X- _ O
the -X- _ O
noisy -X- _ O
spans -X- _ O
. -X- _ O
DIFFUSION -X- _ B-MethodName
- -X- _ I-MethodName
NER -X- _ I-MethodName
is -X- _ O
a -X- _ O
novel -X- _ O
generative -X- _ O
NER -X- _ B-TaskName
method -X- _ O
that -X- _ O
generates -X- _ O
entities -X- _ O
by -X- _ O
progressive -X- _ O
boundary -X- _ O
refinement -X- _ O
over -X- _ O
the -X- _ O
noisy -X- _ O
spans -X- _ O
. -X- _ O

• -X- _ O
We -X- _ O
conduct -X- _ O
experiments -X- _ O
on -X- _ O
both -X- _ O
nested -X- _ O
and -X- _ O
flat -X- _ O
NER -X- _ B-TaskName
to -X- _ O
show -X- _ O
the -X- _ O
generality -X- _ O
of -X- _ O
DIFFU -X- _ B-MethodName
- -X- _ I-MethodName
SIONNER -X- _ I-MethodName
. -X- _ O
Experimental -X- _ O
results -X- _ O
show -X- _ O
that -X- _ O
our -X- _ O
model -X- _ O
achieves -X- _ O
better -X- _ O
or -X- _ O
competitive -X- _ O
performance -X- _ O
against -X- _ O
the -X- _ O
previous -X- _ O
SOTA -X- _ O
models -X- _ O
. -X- _ O

Named -X- _ B-TaskName
Entity -X- _ I-TaskName
Recognition -X- _ I-TaskName

Named -X- _ B-TaskName
entity -X- _ I-TaskName
recognition -X- _ I-TaskName
is -X- _ O
a -X- _ O
long -X- _ O
- -X- _ O
standing -X- _ O
study -X- _ O
in -X- _ O
natural -X- _ O
language -X- _ O
processing -X- _ O
. -X- _ O
Traditional -X- _ O
methods -X- _ O
can -X- _ O
be -X- _ O
divided -X- _ O
into -X- _ O
two -X- _ O
folders -X- _ O
: -X- _ O
tagging -X- _ O
- -X- _ O
based -X- _ O
and -X- _ O
span -X- _ O
- -X- _ O
based -X- _ O
. -X- _ O
For -X- _ O
tagging -X- _ O
- -X- _ O
based -X- _ O
methods -X- _ O
( -X- _ O
Chiu -X- _ O
and -X- _ O
Nichols -X- _ O
, -X- _ O
2016 -X- _ O
; -X- _ O
Ju -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
; -X- _ O
, -X- _ O
they -X- _ O
usually -X- _ O
perform -X- _ O
sequence -X- _ O
labeling -X- _ O
at -X- _ O
the -X- _ O
token -X- _ O
level -X- _ O
and -X- _ O
then -X- _ O
translate -X- _ O
into -X- _ O
predictions -X- _ O
at -X- _ O
the -X- _ O
span -X- _ O
level -X- _ O
. -X- _ O
Meanwhile -X- _ O
, -X- _ O
the -X- _ O
span -X- _ O
- -X- _ O
based -X- _ O
methods -X- _ O
( -X- _ O
Sohrab -X- _ O
and -X- _ O
Eberts -X- _ O
and -X- _ O
Ulges -X- _ O
, -X- _ O
2020 -X- _ O
; -X- _ O
Shen -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021a -X- _ O
, -X- _ O
b -X- _ O
; -X- _ O
Li -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022a -X- _ O
) -X- _ O
directly -X- _ O
perform -X- _ O
entity -X- _ O
classification -X- _ O
on -X- _ O
potential -X- _ O
spans -X- _ O
for -X- _ O
prediction -X- _ O
. -X- _ O
Besides -X- _ O
, -X- _ O
some -X- _ O
methods -X- _ O
attempt -X- _ O
to -X- _ O
formulate -X- _ O
NER -X- _ B-TaskName
as -X- _ O
sequence -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
set -X- _ O
( -X- _ O
Tan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
or -X- _ O
reading -X- _ O
comprehension -X- _ O
tasks -X- _ O
for -X- _ O
prediction -X- _ O
. -X- _ O
In -X- _ O
addition -X- _ O
, -X- _ O
autoregressive -X- _ O
generative -X- _ O
NER -X- _ B-TaskName
works -X- _ O
( -X- _ O
Athiwaratkun -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
; -X- _ O
De -X- _ O
Cao -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
; -X- _ O
Yan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021b -X- _ O
; -X- _ O
linearize -X- _ O
structured -X- _ O
named -X- _ O
entities -X- _ O
into -X- _ O
a -X- _ O
sequence -X- _ O
, -X- _ O
relying -X- _ O
on -X- _ O
sequence -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
sequence -X- _ O
language -X- _ O
models -X- _ O
( -X- _ O
Lewis -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
; -X- _ O
Raffel -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
to -X- _ O
decode -X- _ O
entities -X- _ O
. -X- _ O
These -X- _ O
works -X- _ O
designed -X- _ O
various -X- _ O
translation -X- _ O
schemas -X- _ O
, -X- _ O
including -X- _ O
from -X- _ O
word -X- _ O
index -X- _ O
sequence -X- _ O
to -X- _ O
entities -X- _ O
( -X- _ O
Yan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021b -X- _ O
) -X- _ O
and -X- _ O
from -X- _ O
label -X- _ O
- -X- _ O
enhanced -X- _ O
sequence -X- _ O
to -X- _ O
entities -X- _ O
( -X- _ O
Paolini -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
, -X- _ O
to -X- _ O
unify -X- _ O
NER -X- _ B-TaskName
to -X- _ O
the -X- _ O
text -X- _ O
generation -X- _ O
task -X- _ O
and -X- _ O
achieved -X- _ O
promising -X- _ O
performance -X- _ O
and -X- _ O
generalizability -X- _ O
. -X- _ O
Other -X- _ O
works -X- _ O
focus -X- _ O
on -X- _ O
the -X- _ O
disorder -X- _ O
of -X- _ O
the -X- _ O
entities -X- _ O
and -X- _ O
mitigate -X- _ O
incorrect -X- _ O
decoding -X- _ O
bias -X- _ O
from -X- _ O
a -X- _ O
causal -X- _ O
inference -X- _ O
perspective -X- _ O
. -X- _ O
Different -X- _ O
from -X- _ O
previous -X- _ O
works -X- _ O
, -X- _ O
our -X- _ O
proposed -X- _ O
DIFFUSIONNER -X- _ B-MethodName
is -X- _ O
the -X- _ O
first -X- _ O
one -X- _ O
to -X- _ O
explore -X- _ O
the -X- _ O
utilization -X- _ O
of -X- _ O
the -X- _ O
generative -X- _ O
diffusion -X- _ O
model -X- _ O
on -X- _ O
NER -X- _ B-TaskName
, -X- _ O
which -X- _ O
enables -X- _ O
progressive -X- _ O
refinement -X- _ O
and -X- _ O
dynamic -X- _ O
sampling -X- _ O
of -X- _ O
entities -X- _ O
. -X- _ O
Furthermore -X- _ O
, -X- _ O
compared -X- _ O
with -X- _ O
previous -X- _ O
generation -X- _ O
- -X- _ O
based -X- _ O
methods -X- _ O
, -X- _ O
our -X- _ O
DIFFUSIONNER -X- _ B-MethodName
can -X- _ O
also -X- _ O
decode -X- _ O
entities -X- _ O
in -X- _ O
a -X- _ O
nonautoregressive -X- _ O
manner -X- _ O
, -X- _ O
and -X- _ O
thus -X- _ O
result -X- _ O
in -X- _ O
a -X- _ O
faster -X- _ O
inference -X- _ O
speed -X- _ O
with -X- _ O
better -X- _ O
performance -X- _ O
. -X- _ O

Diffusion -X- _ O
model -X- _ O
is -X- _ O
a -X- _ O
deep -X- _ O
latent -X- _ O
generative -X- _ O
model -X- _ O
proposed -X- _ O
by -X- _ O
( -X- _ O
Sohl -X- _ O
- -X- _ O
Dickstein -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
. -X- _ O
With -X- _ O
the -X- _ O
development -X- _ O
of -X- _ O
recent -X- _ O
work -X- _ O
( -X- _ O
Ho -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
, -X- _ O
diffusion -X- _ O
model -X- _ O
has -X- _ O
achieved -X- _ O
impressive -X- _ O
results -X- _ O
on -X- _ O
image -X- _ O
and -X- _ O
audio -X- _ O
generation -X- _ O
( -X- _ O
Rombach -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
; -X- _ O
Ramesh -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
; -X- _ O
Kong -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
. -X- _ O
Diffusion -X- _ O
model -X- _ O
consists -X- _ O
of -X- _ O
the -X- _ O
forward -X- _ O
diffusion -X- _ O
process -X- _ O
and -X- _ O
the -X- _ O
reverse -X- _ O
diffusion -X- _ O
process -X- _ O
. -X- _ O
The -X- _ O
former -X- _ O
progressively -X- _ O
disturbs -X- _ O
the -X- _ O
data -X- _ O
distribution -X- _ O
by -X- _ O
adding -X- _ O
noise -X- _ O
with -X- _ O
a -X- _ O
fixed -X- _ O
variance -X- _ O
schedule -X- _ O
( -X- _ O
Ho -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
latter -X- _ O
learns -X- _ O
to -X- _ O
recover -X- _ O
the -X- _ O
data -X- _ O
structure -X- _ O
. -X- _ O
Despite -X- _ O
the -X- _ O
success -X- _ O
of -X- _ O
the -X- _ O
diffusion -X- _ O
model -X- _ O
in -X- _ O
continuous -X- _ O
state -X- _ O
spaces -X- _ O
( -X- _ O
image -X- _ O
or -X- _ O
waveform -X- _ O
) -X- _ O
, -X- _ O
the -X- _ O
application -X- _ O
to -X- _ O
natural -X- _ O
language -X- _ O
still -X- _ O
remains -X- _ O
some -X- _ O
open -X- _ O
challenges -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
discrete -X- _ O
nature -X- _ O
of -X- _ O
text -X- _ O
( -X- _ O
Austin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
; -X- _ O
Hoogeboom -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
; -X- _ O
Strudel -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
; -X- _ O
He -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
. -X- _ O
Diffusion -X- _ O
- -X- _ O
LM -X- _ O
( -X- _ O
Li -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022b -X- _ O
) -X- _ O
models -X- _ O
discrete -X- _ O
text -X- _ O
in -X- _ O
continuous -X- _ O
space -X- _ O
through -X- _ O
embedding -X- _ O
and -X- _ O
rounding -X- _ O
operations -X- _ O
and -X- _ O
proposes -X- _ O
an -X- _ O
extra -X- _ O
classifier -X- _ O
as -X- _ O
a -X- _ O
guidance -X- _ O
to -X- _ O
impose -X- _ O
constraints -X- _ O
on -X- _ O
controllable -X- _ O
text -X- _ O
generation -X- _ O
. -X- _ O
DiffuSeq -X- _ O
( -X- _ O
Gong -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
and -X- _ O
SeqDiffuSeq -X- _ O
( -X- _ O
Yuan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022a -X- _ O
) -X- _ O
extend -X- _ O
diffusionbased -X- _ O
text -X- _ O
generation -X- _ O
to -X- _ O
a -X- _ O
more -X- _ O
generalized -X- _ O
setting -X- _ O
. -X- _ O
They -X- _ O
propose -X- _ O
classifier -X- _ O
- -X- _ O
free -X- _ O
sequence -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
sequence -X- _ O
diffusion -X- _ O
frameworks -X- _ O
based -X- _ O
on -X- _ O
encoder -X- _ O
- -X- _ O
only -X- _ O
and -X- _ O
encoder -X- _ O
- -X- _ O
decoder -X- _ O
architectures -X- _ O
, -X- _ O
respectively -X- _ O
. -X- _ O

Although -X- _ O
diffusion -X- _ O
models -X- _ O
have -X- _ O
shown -X- _ O
their -X- _ O
generative -X- _ O
capability -X- _ O
on -X- _ O
images -X- _ O
and -X- _ O
audio -X- _ O
, -X- _ O
its -X- _ O
potential -X- _ O
on -X- _ O
discriminative -X- _ O
tasks -X- _ O
has -X- _ O
not -X- _ O
been -X- _ O
explored -X- _ O
thoroughly -X- _ O
. -X- _ O
Several -X- _ O
pioneer -X- _ O
works -X- _ O
( -X- _ O
Amit -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
; -X- _ O
Baranchuk -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
; -X- _ O
Chen -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
have -X- _ O
made -X- _ O
some -X- _ O
attempts -X- _ O
on -X- _ O
diffusion -X- _ O
models -X- _ O
for -X- _ O
object -X- _ O
detection -X- _ O
and -X- _ O
semantic -X- _ O
segmentation -X- _ O
. -X- _ O
Our -X- _ O
proposed -X- _ O
DIFFUSIONNER -X- _ B-MethodName
aims -X- _ O
to -X- _ O
solve -X- _ O
an -X- _ O
extractive -X- _ O
task -X- _ O
on -X- _ O
discrete -X- _ O
text -X- _ O
sequences -X- _ O
. -X- _ O

In -X- _ O
diffusion -X- _ O
models -X- _ O
, -X- _ O
both -X- _ O
the -X- _ O
forward -X- _ O
and -X- _ O
reverse -X- _ O
processes -X- _ O
can -X- _ O
be -X- _ O
considered -X- _ O
a -X- _ O
Markov -X- _ O
chain -X- _ O
with -X- _ O
progressive -X- _ O
Gaussian -X- _ O
transitions -X- _ O
. -X- _ O
Formally -X- _ O
, -X- _ O
given -X- _ O
a -X- _ O
data -X- _ O
distribution -X- _ O
x -X- _ O
0 -X- _ O
∼ -X- _ O
q -X- _ O
( -X- _ O
x -X- _ O
0 -X- _ O
) -X- _ O
and -X- _ O
a -X- _ O
predefined -X- _ O
variance -X- _ O
schedule -X- _ O
{ -X- _ O
β -X- _ O
1 -X- _ O
, -X- _ O
. -X- _ O
. -X- _ O
. -X- _ O
, -X- _ O
β -X- _ O
T -X- _ O
} -X- _ O
, -X- _ O
the -X- _ O
forward -X- _ O
process -X- _ O
q -X- _ O
gradually -X- _ O
adds -X- _ O
Gaussian -X- _ O
noise -X- _ O
with -X- _ O
variance -X- _ O
β -X- _ O
t -X- _ O
∈ -X- _ O
( -X- _ O
0 -X- _ O
, -X- _ O
1 -X- _ O
) -X- _ O
at -X- _ O
timestep -X- _ O
t -X- _ O
to -X- _ O
produce -X- _ O
latent -X- _ O
variables -X- _ O
x -X- _ O
1 -X- _ O
, -X- _ O
x -X- _ O
2 -X- _ O
, -X- _ O
. -X- _ O
. -X- _ O
. -X- _ O
, -X- _ O
x -X- _ O
T -X- _ O
as -X- _ O
follows -X- _ O
: -X- _ O

q -X- _ O
( -X- _ O
x -X- _ O
1 -X- _ O
, -X- _ O
. -X- _ O
. -X- _ O
. -X- _ O
, -X- _ O
x -X- _ O
T -X- _ O
| -X- _ O
x -X- _ O
0 -X- _ O
) -X- _ O
= -X- _ O
T -X- _ O
t=1 -X- _ O
q -X- _ O
( -X- _ O
x -X- _ O
t -X- _ O
| -X- _ O
x -X- _ O
t−1 -X- _ O
) -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
q -X- _ O
( -X- _ O
x -X- _ O
t -X- _ O
| -X- _ O
x -X- _ O
t−1 -X- _ O
) -X- _ O
= -X- _ O
N -X- _ O
x -X- _ O
t -X- _ O
; -X- _ O
1 -X- _ O
− -X- _ O
β -X- _ O
t -X- _ O
x -X- _ O
t−1 -X- _ O
, -X- _ O
β -X- _ O
t -X- _ O
I -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O

An -X- _ O
important -X- _ O
property -X- _ O
of -X- _ O
the -X- _ O
forward -X- _ O
process -X- _ O
is -X- _ O
that -X- _ O
we -X- _ O
can -X- _ O
sample -X- _ O
the -X- _ O
noisy -X- _ O
latents -X- _ O
at -X- _ O
an -X- _ O
arbitrary -X- _ O
timestep -X- _ O
conditioned -X- _ O
on -X- _ O
the -X- _ O
data -X- _ O
x -X- _ O
0 -X- _ O
. -X- _ O
With -X- _ O
the -X- _ O
notation -X- _ O
α -X- _ O
t -X- _ O
: -X- _ O
= -X- _ O
1 -X- _ O
− -X- _ O
β -X- _ O
t -X- _ O
andᾱ -X- _ O
t -X- _ O
: -X- _ O
= -X- _ O
t -X- _ O
s=0 -X- _ O
α -X- _ O
s -X- _ O
, -X- _ O
we -X- _ O
have -X- _ O
: -X- _ O

q -X- _ O
( -X- _ O
x -X- _ O
t -X- _ O
| -X- _ O
x -X- _ O
0 -X- _ O
) -X- _ O
= -X- _ O
N -X- _ O
x -X- _ O
t -X- _ O
; -X- _ O
√ᾱ -X- _ O
t -X- _ O
x -X- _ O
0 -X- _ O
, -X- _ O
( -X- _ O
1 -X- _ O
−ᾱ -X- _ O
t -X- _ O
) -X- _ O
I -X- _ O
( -X- _ O
3 -X- _ O
) -X- _ O

Asᾱ -X- _ O
T -X- _ O
approximates -X- _ O
0 -X- _ O
, -X- _ O
x -X- _ O
T -X- _ O
follows -X- _ O
the -X- _ O
standard -X- _ O
Gaussian -X- _ O
distribution -X- _ O
: -X- _ O
p -X- _ O
( -X- _ O
x -X- _ O
T -X- _ O
) -X- _ O
≈ -X- _ O
N -X- _ O
( -X- _ O
x -X- _ O
T -X- _ O
; -X- _ O
0 -X- _ O
, -X- _ O
I -X- _ O
) -X- _ O
. -X- _ O
Unlike -X- _ O
the -X- _ O
fixed -X- _ O
forward -X- _ O
process -X- _ O
, -X- _ O
the -X- _ O
reverse -X- _ O
process -X- _ O
p -X- _ O
θ -X- _ O
( -X- _ O
x -X- _ O
0 -X- _ O
: -X- _ O
T -X- _ O
) -X- _ O
is -X- _ O
defined -X- _ O
as -X- _ O
a -X- _ O
Markov -X- _ O
chain -X- _ O
with -X- _ O
learnable -X- _ O
Gaussian -X- _ O
transitions -X- _ O
starting -X- _ O
at -X- _ O
a -X- _ O
prior -X- _ O
p -X- _ O
( -X- _ O
x -X- _ O
T -X- _ O
) -X- _ O
= -X- _ O
N -X- _ O
( -X- _ O
x -X- _ O
T -X- _ O
; -X- _ O
0 -X- _ O
, -X- _ O
I -X- _ O
) -X- _ O
: -X- _ O

p -X- _ O
θ -X- _ O
( -X- _ O
x -X- _ O
0 -X- _ O
: -X- _ O
T -X- _ O
) -X- _ O
= -X- _ O
p -X- _ O
( -X- _ O
x -X- _ O
T -X- _ O
) -X- _ O
T -X- _ O
t=1 -X- _ O
p -X- _ O
θ -X- _ O
( -X- _ O
x -X- _ O
t−1 -X- _ O
| -X- _ O
x -X- _ O
t -X- _ O
) -X- _ O
p -X- _ O
θ -X- _ O
( -X- _ O
x -X- _ O
t−1 -X- _ O
| -X- _ O
x -X- _ O
t -X- _ O
) -X- _ O
= -X- _ O
N -X- _ O
( -X- _ O
x -X- _ O
t−1 -X- _ O
; -X- _ O
µ -X- _ O
θ -X- _ O
( -X- _ O
x -X- _ O
t -X- _ O
, -X- _ O
t -X- _ O
) -X- _ O
, -X- _ O
Σ -X- _ O
θ -X- _ O
( -X- _ O
x -X- _ O
t -X- _ O
, -X- _ O
t -X- _ O
) -X- _ O
) -X- _ O

where -X- _ O
θ -X- _ O
denotes -X- _ O
the -X- _ O
parameters -X- _ O
of -X- _ O
the -X- _ O
model -X- _ O
and -X- _ O
µ -X- _ O
θ -X- _ O
and -X- _ O
Σ -X- _ O
θ -X- _ O
are -X- _ O
the -X- _ O
predicted -X- _ O
covariance -X- _ O
and -X- _ O
mean -X- _ O
of -X- _ O
q -X- _ O
( -X- _ O
x -X- _ O
t−1 -X- _ O
| -X- _ O
x -X- _ O
t -X- _ O
) -X- _ O
. -X- _ O
We -X- _ O
set -X- _ O
Σ -X- _ O
θ -X- _ O
( -X- _ O
x -X- _ O
t -X- _ O
, -X- _ O
t -X- _ O
) -X- _ O
= -X- _ O
σ -X- _ O
2 -X- _ O
t -X- _ O
I -X- _ O
and -X- _ O
build -X- _ O
a -X- _ O
neural -X- _ O
network -X- _ O
f -X- _ O
θ -X- _ O
to -X- _ O
predict -X- _ O
the -X- _ O
data -X- _ O
x -X- _ O
0 -X- _ O
, -X- _ O
denoted -X- _ O
asx -X- _ O
0 -X- _ O
= -X- _ O
f -X- _ O
θ -X- _ O
( -X- _ O
x -X- _ O
t -X- _ O
, -X- _ O
t -X- _ O
) -X- _ O
. -X- _ O
Then -X- _ O
we -X- _ O
have -X- _ O
µ -X- _ O
θ -X- _ O
( -X- _ O
x -X- _ O
t -X- _ O
, -X- _ O
t -X- _ O
) -X- _ O
= -X- _ O
µ -X- _ O
t -X- _ O
( -X- _ O
x -X- _ O
t -X- _ O
, -X- _ O
x -X- _ O
0 -X- _ O
) -X- _ O
= -X- _ O
μ -X- _ O
t -X- _ O
( -X- _ O
x -X- _ O
t -X- _ O
, -X- _ O
f -X- _ O
θ -X- _ O
( -X- _ O
x -X- _ O
t -X- _ O
, -X- _ O
t -X- _ O
) -X- _ O
) -X- _ O
, -X- _ O
whereμ -X- _ O
t -X- _ O
denotes -X- _ O
the -X- _ O
mean -X- _ O
of -X- _ O
posterior -X- _ O
q -X- _ O

( -X- _ O
x -X- _ O
t−1 -X- _ O
| -X- _ O
x -X- _ O
t -X- _ O
, -X- _ O
x -X- _ O
0 -X- _ O
) -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
section -X- _ O
, -X- _ O
we -X- _ O
first -X- _ O
present -X- _ O
the -X- _ O
formulation -X- _ O
of -X- _ O
diffusion -X- _ O
model -X- _ O
for -X- _ O
NER -X- _ B-TaskName
( -X- _ O
i.e. -X- _ O
, -X- _ O
the -X- _ O
boundary -X- _ O
denoising -X- _ O
diffusion -X- _ O
process -X- _ O
) -X- _ O
in -X- _ O
§ -X- _ O
4.1 -X- _ O
. -X- _ O
Then -X- _ O
, -X- _ O
we -X- _ O
detail -X- _ O
the -X- _ O
architecture -X- _ O
of -X- _ O
the -X- _ O
denoising -X- _ O
network -X- _ O
for -X- _ O
boundary -X- _ O
reverse -X- _ O
process -X- _ O
in -X- _ O
§ -X- _ O
4.2 -X- _ O
. -X- _ O
Finally -X- _ O
, -X- _ O
we -X- _ O
describe -X- _ O
the -X- _ O
inference -X- _ O
procedure -X- _ O
of -X- _ O
DIFFUSIONNER -X- _ B-MethodName
in -X- _ O
§ -X- _ O
4.3 -X- _ O
. -X- _ O

Given -X- _ O
a -X- _ O
sentence -X- _ O
S -X- _ O
with -X- _ O
length -X- _ O
M -X- _ O
, -X- _ O
the -X- _ O
named -X- _ B-TaskName
entity -X- _ I-TaskName
recognition -X- _ I-TaskName
task -X- _ I-TaskName
is -X- _ O
to -X- _ O
extract -X- _ O
the -X- _ O
entities -X- _ O
E -X- _ O
= -X- _ O
{ -X- _ O
( -X- _ O
l -X- _ O
i -X- _ O
, -X- _ O
r -X- _ O
i -X- _ O
, -X- _ O
t -X- _ O
i -X- _ O
) -X- _ O
} -X- _ O
N -X- _ O
i=0 -X- _ O
contained -X- _ O
in -X- _ O
the -X- _ O
sentence -X- _ O
, -X- _ O
where -X- _ O
N -X- _ O
is -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
entities -X- _ O
and -X- _ O
l -X- _ O
i -X- _ O
, -X- _ O
r -X- _ O
i -X- _ O
, -X- _ O
t -X- _ O
i -X- _ O
denote -X- _ O
the -X- _ O
left -X- _ O
and -X- _ O
right -X- _ O
boundary -X- _ O
indices -X- _ O
and -X- _ O
type -X- _ O
of -X- _ O
the -X- _ O
i -X- _ O
- -X- _ O
th -X- _ O
entity -X- _ O
. -X- _ O
We -X- _ O
formulate -X- _ O
NER -X- _ B-TaskName
as -X- _ O
a -X- _ O
boundary -X- _ O
denoising -X- _ O
diffusion -X- _ O
process -X- _ O
, -X- _ O
as -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
2 -X- _ O
. -X- _ O
We -X- _ O
regard -X- _ O
entity -X- _ O
boundaries -X- _ O
as -X- _ O
data -X- _ O
samples -X- _ O
, -X- _ O
then -X- _ O
the -X- _ O
boundary -X- _ O
forward -X- _ O
diffusion -X- _ O
is -X- _ O
to -X- _ O
add -X- _ O
Gaussian -X- _ O
noise -X- _ O
to -X- _ O
the -X- _ O
entity -X- _ O
boundaries -X- _ O
while -X- _ O
the -X- _ O
reverse -X- _ O
diffusion -X- _ O
process -X- _ O
is -X- _ O
to -X- _ O
progressively -X- _ O
recover -X- _ O
the -X- _ O
original -X- _ O
entity -X- _ O
boundaries -X- _ O
from -X- _ O
the -X- _ O
noisy -X- _ O
spans -X- _ O
. -X- _ O
Boundary -X- _ O
Forward -X- _ O
Diffusion -X- _ O
Boundary -X- _ O
forward -X- _ O
diffusion -X- _ O
is -X- _ O
the -X- _ O
process -X- _ O
of -X- _ O
adding -X- _ O
noise -X- _ O
to -X- _ O
the -X- _ O
entity -X- _ O
boundary -X- _ O
in -X- _ O
a -X- _ O
stepwise -X- _ O
manner -X- _ O
. -X- _ O
In -X- _ O
order -X- _ O
to -X- _ O
align -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
entities -X- _ O
in -X- _ O
different -X- _ O
instances -X- _ O
, -X- _ O
we -X- _ O
first -X- _ O
expand -X- _ O
the -X- _ O
entity -X- _ O
set -X- _ O
to -X- _ O
a -X- _ O
fixed -X- _ O
number -X- _ O
K -X- _ O
( -X- _ O
> -X- _ O
N -X- _ O
) -X- _ O
. -X- _ O
There -X- _ O
are -X- _ O
two -X- _ O
ways -X- _ O
to -X- _ O
expand -X- _ O
the -X- _ O
entities -X- _ O
, -X- _ O
repetition -X- _ O
strategy -X- _ O
and -X- _ O
random -X- _ O
strategy -X- _ O
, -X- _ O
which -X- _ O
add -X- _ O
K -X- _ O
− -X- _ O
N -X- _ O
entities -X- _ O
by -X- _ O
duplicating -X- _ O
entities -X- _ O
or -X- _ O
sampling -X- _ O
random -X- _ O
spans -X- _ O
from -X- _ O
a -X- _ O
Gaussian -X- _ O
distribution -X- _ O
2 -X- _ O
. -X- _ O
For -X- _ O
convenience -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
B -X- _ O
∈ -X- _ O
R -X- _ O
K×2 -X- _ O
to -X- _ O
denote -X- _ O
the -X- _ O
boundaries -X- _ O
of -X- _ O
the -X- _ O
K -X- _ O
expanded -X- _ O
entities -X- _ O
, -X- _ O
with -X- _ O
all -X- _ O
of -X- _ O
them -X- _ O
normalized -X- _ O
by -X- _ O
the -X- _ O
sentence -X- _ O
length -X- _ O
M -X- _ O
and -X- _ O
scaled -X- _ O
to -X- _ O
( -X- _ O
−λ -X- _ O
, -X- _ O
λ -X- _ O
) -X- _ O
interval -X- _ O
. -X- _ O
Formally -X- _ O
, -X- _ O
given -X- _ O
the -X- _ O
entity -X- _ O
boundaries -X- _ O
as -X- _ O
data -X- _ O
samples -X- _ O
x -X- _ O
0 -X- _ O
= -X- _ O
B -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
obtain -X- _ O
the -X- _ O
noisy -X- _ O
spans -X- _ O
at -X- _ O
timestep -X- _ O
t -X- _ O
using -X- _ O
the -X- _ O
forward -X- _ O
diffusion -X- _ O
process -X- _ O
. -X- _ O
According -X- _ O
to -X- _ O
Equation -X- _ O
( -X- _ O
3 -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
have -X- _ O
: -X- _ O

x -X- _ O
t -X- _ O
= -X- _ O
√ᾱ -X- _ O
t -X- _ O
x -X- _ O
0 -X- _ O
+ -X- _ O
√ -X- _ O
1 -X- _ O
−ᾱ -X- _ O
t -X- _ O
ϵ -X- _ O
( -X- _ O
4 -X- _ O
) -X- _ O

where -X- _ O
ϵ -X- _ O
∼ -X- _ O
N -X- _ O
( -X- _ O
0 -X- _ O
, -X- _ O
I -X- _ O
) -X- _ O
is -X- _ O
the -X- _ O
noise -X- _ O
sampled -X- _ O
from -X- _ O
the -X- _ O
standard -X- _ O
Gaussian -X- _ O
. -X- _ O
At -X- _ O
each -X- _ O
timestep -X- _ O
, -X- _ O
the -X- _ O
noisy -X- _ O
spans -X- _ O
have -X- _ O
the -X- _ O
same -X- _ O
shape -X- _ O
as -X- _ O
x -X- _ O
0 -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O

x -X- _ O
1 -X- _ O
, -X- _ O
x -X- _ O
2 -X- _ O
, -X- _ O
. -X- _ O
. -X- _ O
. -X- _ O
, -X- _ O
x -X- _ O
T -X- _ O
∈ -X- _ O
R -X- _ O
K×2 -X- _ O
. -X- _ O

Boundary -X- _ O
Reverse -X- _ O
Diffusion -X- _ O
Starting -X- _ O
from -X- _ O
the -X- _ O
noisy -X- _ O
spans -X- _ O
x -X- _ O
T -X- _ O
sampled -X- _ O
from -X- _ O
the -X- _ O
Gaussian -X- _ O
distribution -X- _ O
, -X- _ O
boundary -X- _ O
reverse -X- _ O
diffusion -X- _ O
adopts -X- _ O
a -X- _ O
non -X- _ O
- -X- _ O
Markovian -X- _ O
denoising -X- _ O
practice -X- _ O
used -X- _ O
in -X- _ O
DDIM -X- _ O
( -X- _ O
Song -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
to -X- _ O
recover -X- _ O
entities -X- _ O
boundaries -X- _ O
. -X- _ O
Assuming -X- _ O
τ -X- _ O
is -X- _ O
an -X- _ O
arithmetic -X- _ O
subsequence -X- _ O
of -X- _ O
the -X- _ O
complete -X- _ O
timestep -X- _ O
sequence -X- _ O
[ -X- _ O
1 -X- _ O
, -X- _ O
. -X- _ O
. -X- _ O
. -X- _ O
, -X- _ O
T -X- _ O
] -X- _ O
of -X- _ O
length -X- _ O
γ -X- _ O
with -X- _ O
τ -X- _ O
γ -X- _ O
= -X- _ O
T -X- _ O
. -X- _ O
Then -X- _ O
we -X- _ O
refine -X- _ O
the -X- _ O
noisy -X- _ O
spans -X- _ O
x -X- _ O
τ -X- _ O
i -X- _ O
to -X- _ O
2 -X- _ O
We -X- _ O
will -X- _ O
discuss -X- _ O
these -X- _ O
two -X- _ O
practices -X- _ O
in -X- _ O
§ -X- _ O
6.3 -X- _ O
. -X- _ O

x -X- _ O
τ -X- _ O
i−1 -X- _ O
as -X- _ O
follows -X- _ O
: -X- _ O

x -X- _ O
0 -X- _ O
= -X- _ O
f -X- _ O
θ -X- _ O
( -X- _ O
x -X- _ O
τ -X- _ O
i -X- _ O
, -X- _ O
S -X- _ O
, -X- _ O
τ -X- _ O
i -X- _ O
) -X- _ O
( -X- _ O
5 -X- _ O
) -X- _ O

ϵ -X- _ O
τ -X- _ O
i -X- _ O
= -X- _ O
x -X- _ O
τ -X- _ O
i -X- _ O
− -X- _ O
√ -X- _ O
α -X- _ O
τ -X- _ O
ix -X- _ O
0 -X- _ O
√ -X- _ O
1 -X- _ O
− -X- _ O
α -X- _ O
τ -X- _ O
i -X- _ O
( -X- _ O
6 -X- _ O
) -X- _ O
x -X- _ O
τ -X- _ O
i−1 -X- _ O
= -X- _ O
√ -X- _ O
α -X- _ O
τ -X- _ O
i−1x -X- _ O
0 -X- _ O
+ -X- _ O
1 -X- _ O
− -X- _ O
α -X- _ O
τ -X- _ O
i−1ε -X- _ O
τ -X- _ O
i -X- _ O
( -X- _ O
7 -X- _ O
) -X- _ O

wherex -X- _ O
0 -X- _ O
andε -X- _ O
τ -X- _ O
i -X- _ O
are -X- _ O
the -X- _ O
predicted -X- _ O
entity -X- _ O
boundary -X- _ O
and -X- _ O
noise -X- _ O
at -X- _ O
timestep -X- _ O
τ -X- _ O
i -X- _ O
. -X- _ O
f -X- _ O
θ -X- _ O
( -X- _ O
x -X- _ O
t -X- _ O
, -X- _ O
S -X- _ O
, -X- _ O
t -X- _ O
) -X- _ O
is -X- _ O
a -X- _ O
learnable -X- _ O
denoising -X- _ O
network -X- _ O
and -X- _ O
we -X- _ O
will -X- _ O
cover -X- _ O
the -X- _ O
network -X- _ O
architecture -X- _ O
in -X- _ O
the -X- _ O
next -X- _ O
section -X- _ O
( -X- _ O
§ -X- _ O
4.2 -X- _ O
) -X- _ O
. -X- _ O
After -X- _ O
γ -X- _ O
iterations -X- _ O
of -X- _ O
DDIM -X- _ O
, -X- _ O
the -X- _ O
noisy -X- _ O
spans -X- _ O
are -X- _ O
progressively -X- _ O
refined -X- _ O
to -X- _ O
the -X- _ O
entity -X- _ O
boundaries -X- _ O
. -X- _ O

Denoising -X- _ O
network -X- _ O
f -X- _ O
θ -X- _ O
( -X- _ O
x -X- _ O
t -X- _ O
, -X- _ O
S -X- _ O
, -X- _ O
t -X- _ O
) -X- _ O
accepts -X- _ O
the -X- _ O
noisy -X- _ O
spans -X- _ O
x -X- _ O
t -X- _ O
and -X- _ O
the -X- _ O
sentence -X- _ O
S -X- _ O
as -X- _ O
inputs -X- _ O
and -X- _ O
predicts -X- _ O
the -X- _ O
corresponding -X- _ O
entity -X- _ O
boundariesx -X- _ O
0 -X- _ O
. -X- _ O
As -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
2 -X- _ O
, -X- _ O
we -X- _ O
parameterize -X- _ O
the -X- _ O
denoising -X- _ O
network -X- _ O
with -X- _ O
a -X- _ O
sentence -X- _ O
encoder -X- _ O
and -X- _ O
an -X- _ O
entity -X- _ O
decoder -X- _ O
. -X- _ O

Sentence -X- _ O
Encoder -X- _ O
consists -X- _ O
of -X- _ O
a -X- _ O
BERT -X- _ O
( -X- _ O
Devlin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
plus -X- _ O
a -X- _ O
stacked -X- _ O
bi -X- _ O
- -X- _ O
directional -X- _ O
LSTM -X- _ O
. -X- _ O

The -X- _ O
whole -X- _ O
span -X- _ O
encoder -X- _ O
takes -X- _ O
the -X- _ O
sentence -X- _ O
S -X- _ O
as -X- _ O
input -X- _ O
and -X- _ O
outputs -X- _ O
the -X- _ O
sentence -X- _ O
encoding -X- _ O
H -X- _ O
S -X- _ O
∈ -X- _ O
R -X- _ O
M -X- _ O
×h -X- _ O
. -X- _ O
The -X- _ O
sentence -X- _ O
encoding -X- _ O
H -X- _ O
S -X- _ O
will -X- _ O
be -X- _ O
calculated -X- _ O
only -X- _ O
once -X- _ O
and -X- _ O
reused -X- _ O
across -X- _ O
all -X- _ O
timesteps -X- _ O
to -X- _ O
save -X- _ O
computations -X- _ O
. -X- _ O

Entity -X- _ O
Decoder -X- _ O
uses -X- _ O
the -X- _ O
sentence -X- _ O
encoding -X- _ O
H -X- _ O
S -X- _ O
to -X- _ O
first -X- _ O
compute -X- _ O
the -X- _ O
representations -X- _ O
of -X- _ O
K -X- _ O
noisy -X- _ O
spans -X- _ O
x -X- _ O
t -X- _ O
and -X- _ O
then -X- _ O
predicts -X- _ O
the -X- _ O
corresponding -X- _ O
entity -X- _ O
boundaries -X- _ O
. -X- _ O
Specifically -X- _ O
, -X- _ O
we -X- _ O
discretize -X- _ O
the -X- _ O
noisy -X- _ O
spans -X- _ O
into -X- _ O
word -X- _ O
indexes -X- _ O
by -X- _ O
rescaling -X- _ O
, -X- _ O
multiplying -X- _ O
and -X- _ O
rounding -X- _ O
3 -X- _ O
, -X- _ O
then -X- _ O
perform -X- _ O
mean -X- _ O
pooling -X- _ O
over -X- _ O
the -X- _ O

x0 -X- _ O
= -X- _ O
B -X- _ O
∈ -X- _ O
R -X- _ O
K×2 -X- _ O
5 -X- _ O
t -X- _ O
∼ -X- _ O
Uniform -X- _ O
( -X- _ O
{ -X- _ O
1 -X- _ O
, -X- _ O
. -X- _ O
. -X- _ O
. -X- _ O
, -X- _ O
T -X- _ O
} -X- _ O
) -X- _ O
6 -X- _ O
ϵ -X- _ O
∼ -X- _ O
N -X- _ O
( -X- _ O
0 -X- _ O
, -X- _ O
I -X- _ O
) -X- _ O
7 -X- _ O
xt -X- _ O
= -X- _ O
√ᾱ -X- _ O
tx0 -X- _ O
+ -X- _ O
√ -X- _ O
1 -X- _ O
−ᾱtϵ -X- _ O
8 -X- _ O

Compute -X- _ O
P -X- _ O
l -X- _ O
, -X- _ O
P -X- _ O
r -X- _ O
and -X- _ O
P -X- _ O
c -X- _ O
by -X- _ O
running -X- _ O
f -X- _ O
θ -X- _ O
( -X- _ O
xt -X- _ O
, -X- _ O
S -X- _ O
, -X- _ O
t -X- _ O
) -X- _ O

− -X- _ O
K -X- _ O
i=1 -X- _ O
log -X- _ O
P -X- _ O
c -X- _ O
i -X- _ O
( -X- _ O
π -X- _ O
c -X- _ O
( -X- _ O
i -X- _ O
) -X- _ O
) -X- _ O
+ -X- _ O
δ∈l -X- _ O
, -X- _ O
r -X- _ O
log -X- _ O
P -X- _ O
δ -X- _ O
i -X- _ O
( -X- _ O
π -X- _ O
δ -X- _ O
( -X- _ O
i -X- _ O
) -X- _ O
) -X- _ O
10 -X- _ O
until -X- _ O
converged -X- _ O
; -X- _ O

inner -X- _ O
- -X- _ O
span -X- _ O
tokens -X- _ O
. -X- _ O
The -X- _ O
extracted -X- _ O
span -X- _ O
representations -X- _ O
can -X- _ O
be -X- _ O
denoted -X- _ O
as -X- _ O
H -X- _ O
X -X- _ O
∈ -X- _ O
R -X- _ O
K×h -X- _ O
. -X- _ O
To -X- _ O
further -X- _ O
encode -X- _ O
the -X- _ O
spans -X- _ O
, -X- _ O
we -X- _ O
design -X- _ O
a -X- _ O
span -X- _ O
encoder -X- _ O
that -X- _ O
consists -X- _ O
of -X- _ O
a -X- _ O
self -X- _ O
- -X- _ O
attention -X- _ O
and -X- _ O
a -X- _ O
cross -X- _ O
- -X- _ O
attention -X- _ O
layer -X- _ O
. -X- _ O
The -X- _ O
former -X- _ O
enhances -X- _ O
the -X- _ O
interaction -X- _ O
between -X- _ O
spans -X- _ O
with -X- _ O
key -X- _ O
, -X- _ O
query -X- _ O
, -X- _ O
and -X- _ O
value -X- _ O
as -X- _ O
H -X- _ O
X -X- _ O
. -X- _ O
And -X- _ O
the -X- _ O
latter -X- _ O
fuses -X- _ O
the -X- _ O
sentence -X- _ O
encoding -X- _ O
to -X- _ O
the -X- _ O
span -X- _ O
representation -X- _ O
with -X- _ O
key -X- _ O
, -X- _ O
value -X- _ O
as -X- _ O
H -X- _ O
S -X- _ O
, -X- _ O
and -X- _ O
query -X- _ O
as -X- _ O
H -X- _ O
X -X- _ O
. -X- _ O
We -X- _ O
further -X- _ O
add -X- _ O
the -X- _ O
sinusoidal -X- _ O
embedding -X- _ O
E -X- _ O
t -X- _ O
( -X- _ O
Vaswani -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
of -X- _ O
timestep -X- _ O
t -X- _ O
to -X- _ O
the -X- _ O
span -X- _ O
representations -X- _ O
. -X- _ O
Thus -X- _ O
the -X- _ O
new -X- _ O
representationsH -X- _ O
X -X- _ O
of -X- _ O
the -X- _ O
noisy -X- _ O
spans -X- _ O
can -X- _ O
be -X- _ O
computed -X- _ O
: -X- _ O

H -X- _ O
X -X- _ O
= -X- _ O
SpanEncoder -X- _ O
( -X- _ O
H -X- _ O
S -X- _ O
, -X- _ O
H -X- _ O
X -X- _ O
) -X- _ O
+ -X- _ O
E -X- _ O
t -X- _ O
, -X- _ O

Then -X- _ O
we -X- _ O
use -X- _ O
two -X- _ O
boundary -X- _ O
pointers -X- _ O
to -X- _ O
predict -X- _ O
the -X- _ O
entity -X- _ O
boundaries -X- _ O
. -X- _ O
For -X- _ O
boundary -X- _ O
δ -X- _ O
∈ -X- _ O
{ -X- _ O
l -X- _ O
, -X- _ O
r -X- _ O
} -X- _ O
, -X- _ O
we -X- _ O
compute -X- _ O
the -X- _ O
fusion -X- _ O
representation -X- _ O
H -X- _ O
δ -X- _ O
SX -X- _ O
∈ -X- _ O
R -X- _ O
K×M -X- _ O
×h -X- _ O
of -X- _ O
the -X- _ O
noisy -X- _ O
spans -X- _ O
and -X- _ O
the -X- _ O
words -X- _ O
, -X- _ O
and -X- _ O
then -X- _ O
the -X- _ O
probability -X- _ O
of -X- _ O
the -X- _ O
word -X- _ O
as -X- _ O
the -X- _ O
left -X- _ O
or -X- _ O
right -X- _ O
boundaries -X- _ O
P -X- _ O
δ -X- _ O
∈ -X- _ O
R -X- _ O
K×M -X- _ O
can -X- _ O
be -X- _ O
computed -X- _ O
as -X- _ O
: -X- _ O

H -X- _ O
δ -X- _ O
SX -X- _ O
= -X- _ O
H -X- _ O
S -X- _ O
W -X- _ O
δ -X- _ O
S -X- _ O
+ -X- _ O
H -X- _ O
X -X- _ O
W -X- _ O
δ -X- _ O
X -X- _ O
P -X- _ O
δ -X- _ O
= -X- _ O
sigmoid -X- _ O
( -X- _ O
MLP -X- _ O
( -X- _ O
H -X- _ O
δ -X- _ O
SX -X- _ O
) -X- _ O

) -X- _ O
where -X- _ O
W -X- _ O
δ -X- _ O
S -X- _ O
, -X- _ O
W -X- _ O
δ -X- _ O
X -X- _ O
∈ -X- _ O
R -X- _ O
h×h -X- _ O
are -X- _ O
two -X- _ O
learnable -X- _ O
matrixes -X- _ O
and -X- _ O
MLP -X- _ O
is -X- _ O
a -X- _ O
two -X- _ O
- -X- _ O
layer -X- _ O
perceptron -X- _ O
. -X- _ O
Based -X- _ O
on -X- _ O
the -X- _ O
boundary -X- _ O
probabilities -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
predict -X- _ O
the -X- _ O
boundary -X- _ O
indices -X- _ O
of -X- _ O
the -X- _ O
K -X- _ O
noisy -X- _ O
spans -X- _ O
. -X- _ O
If -X- _ O
the -X- _ O
current -X- _ O
step -X- _ O
is -X- _ O
not -X- _ O
the -X- _ O
last -X- _ O
denoising -X- _ O
step -X- _ O
, -X- _ O
we -X- _ O
computê -X- _ O
x -X- _ O
0 -X- _ O
by -X- _ O
normalizing -X- _ O
the -X- _ O
indices -X- _ O
with -X- _ O
sentence -X- _ O
length -X- _ O
M -X- _ O
and -X- _ O
scaling -X- _ O
to -X- _ O
( -X- _ O
−λ -X- _ O
, -X- _ O
λ -X- _ O
) -X- _ O
intervals -X- _ O
. -X- _ O
Then -X- _ O
we -X- _ O
conduct -X- _ O
the -X- _ O
next -X- _ O
iteration -X- _ O
of -X- _ O
the -X- _ O
reverse -X- _ O
diffusion -X- _ O
process -X- _ O
according -X- _ O
to -X- _ O
Equations -X- _ O
( -X- _ O
5 -X- _ O
) -X- _ O
to -X- _ O
( -X- _ O
7 -X- _ O
) -X- _ O
. -X- _ O

It -X- _ O
is -X- _ O
worth -X- _ O
noting -X- _ O
that -X- _ O
we -X- _ O
should -X- _ O
not -X- _ O
only -X- _ O
locate -X- _ O
entities -X- _ O
but -X- _ O
also -X- _ O
classify -X- _ O
them -X- _ O
in -X- _ O
named -X- _ O
entity -X- _ O
recognition -X- _ O
. -X- _ O
Therefore -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
an -X- _ O
entity -X- _ O
classifier -X- _ O
to -X- _ O
classify -X- _ O
the -X- _ O
noisy -X- _ O
spans -X- _ O
. -X- _ O
The -X- _ O
classification -X- _ O
probability -X- _ O
P -X- _ O
c -X- _ O
∈ -X- _ O
R -X- _ O
K×C -X- _ O
is -X- _ O
calculated -X- _ O
as -X- _ O
follows -X- _ O
: -X- _ O

P -X- _ O
c -X- _ O
= -X- _ O
Classifier -X- _ O
( -X- _ O
H -X- _ O
X -X- _ O
) -X- _ O
Algorithm -X- _ O
2 -X- _ O
: -X- _ O
Inference -X- _ O
1 -X- _ O
xT -X- _ O
∼ -X- _ O
N -X- _ O
( -X- _ O
0 -X- _ O
, -X- _ O
I -X- _ O
) -X- _ O
∈ -X- _ O
R -X- _ O
K -X- _ O
eval -X- _ O
×2 -X- _ O

2 -X- _ O
τ -X- _ O
is -X- _ O
an -X- _ O
arithmetic -X- _ O
sequence -X- _ O
of -X- _ O
length -X- _ O
γ -X- _ O
with -X- _ O
τγ -X- _ O
= -X- _ O
T -X- _ O
3 -X- _ O
for -X- _ O
i -X- _ O
= -X- _ O
γ -X- _ O
, -X- _ O
. -X- _ O
. -X- _ O
. -X- _ O
, -X- _ O
1 -X- _ O
do -X- _ O
4 -X- _ O
Computex0 -X- _ O
, -X- _ O
P -X- _ O
l -X- _ O
, -X- _ O
P -X- _ O
r -X- _ O
and -X- _ O
P -X- _ O
c -X- _ O
via -X- _ O
f -X- _ O
θ -X- _ O
( -X- _ O
xt -X- _ O
, -X- _ O
S -X- _ O
, -X- _ O
t -X- _ O
) -X- _ O

5 -X- _ O
xτ -X- _ O
i−1 -X- _ O
= -X- _ O
√ -X- _ O
ατ -X- _ O
i−1x -X- _ O
0 -X- _ O
+ -X- _ O
1 -X- _ O
− -X- _ O
ατ -X- _ O
i−1 -X- _ O
• -X- _ O
xτ -X- _ O
i -X- _ O
− -X- _ O
√ -X- _ O
ατ -X- _ O
ix -X- _ O
0 -X- _ O
√ -X- _ O
1−ατ -X- _ O
i -X- _ O

6 -X- _ O
end -X- _ O
7 -X- _ O
Decode -X- _ O
entities -X- _ O
( -X- _ O
li -X- _ O
, -X- _ O
ri -X- _ O
, -X- _ O
ci -X- _ O
) -X- _ O
K -X- _ O
eval -X- _ O
i=0 -X- _ O
, -X- _ O
where -X- _ O
δi -X- _ O
= -X- _ O
argmax -X- _ O
P -X- _ O
δ -X- _ O
i -X- _ O
, -X- _ O
δ -X- _ O
∈ -X- _ O
{ -X- _ O
l -X- _ O
, -X- _ O
r -X- _ O
, -X- _ O
c -X- _ O
} -X- _ O
8 -X- _ O
Perform -X- _ O
post -X- _ O
- -X- _ O
processing -X- _ O
on -X- _ O
( -X- _ O
li -X- _ O
, -X- _ O
ri -X- _ O
, -X- _ O
ci -X- _ O
) -X- _ O
K -X- _ O
eval -X- _ O
i=0 -X- _ O
9 -X- _ O
return -X- _ O
final -X- _ O
entities -X- _ O
where -X- _ O
C -X- _ O
is -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
entity -X- _ O
types -X- _ O
and -X- _ O
Classifier -X- _ O
is -X- _ O
a -X- _ O
two -X- _ O
- -X- _ O
layer -X- _ O
perceptron -X- _ O
with -X- _ O
a -X- _ O
softmax -X- _ O
layer -X- _ O
. -X- _ O

Training -X- _ O
Objective -X- _ O
With -X- _ O
K -X- _ O
entities -X- _ O
predicted -X- _ O
from -X- _ O
the -X- _ O
noisy -X- _ O
spans -X- _ O
and -X- _ O
N -X- _ O
ground -X- _ O
- -X- _ O
truth -X- _ O
entities -X- _ O
, -X- _ O
we -X- _ O
first -X- _ O
use -X- _ O
the -X- _ O
Hungarian -X- _ O
algorithm -X- _ O
( -X- _ O
Kuhn -X- _ O
, -X- _ O
1955 -X- _ O
) -X- _ O
to -X- _ O
solve -X- _ O
the -X- _ O
optimal -X- _ O
matchingπ -X- _ O
between -X- _ O
the -X- _ O
two -X- _ O
sets -X- _ O
4 -X- _ O
as -X- _ O
in -X- _ O
Carion -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
.π -X- _ O
( -X- _ O
i -X- _ O
) -X- _ O
denotes -X- _ O
the -X- _ O
ground -X- _ O
- -X- _ O
truth -X- _ O
entity -X- _ O
corresponding -X- _ O
to -X- _ O
the -X- _ O
i -X- _ O
- -X- _ O
th -X- _ O
noisy -X- _ O
span -X- _ O
. -X- _ O
Then -X- _ O
, -X- _ O
we -X- _ O
train -X- _ O
the -X- _ O
boundary -X- _ O
reverse -X- _ O
process -X- _ O
by -X- _ O
maximizing -X- _ O
the -X- _ O
likelihood -X- _ O
of -X- _ O
the -X- _ O
prediction -X- _ O
: -X- _ O

L -X- _ O
= -X- _ O
− -X- _ O
K -X- _ O
i=1 -X- _ O
δ∈ -X- _ O
{ -X- _ O
l -X- _ O
, -X- _ O
r -X- _ O
, -X- _ O
c -X- _ O
} -X- _ O
log -X- _ O
P -X- _ O
δ -X- _ O
i -X- _ O
π -X- _ O
δ -X- _ O
( -X- _ O
i -X- _ O
) -X- _ O

During -X- _ O
inference -X- _ O
, -X- _ O
DIFFUSIONNER -X- _ B-MethodName
first -X- _ O
samples -X- _ O
K -X- _ O
eval -X- _ O
noisy -X- _ O
spans -X- _ O
from -X- _ O
a -X- _ O
Gaussian -X- _ O
distribution -X- _ O
, -X- _ O
then -X- _ O
performs -X- _ O
iterative -X- _ O
denoising -X- _ O
with -X- _ O
the -X- _ O
learned -X- _ O
boundary -X- _ O
reverse -X- _ O
diffusion -X- _ O
process -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
denoising -X- _ O
timestep -X- _ O
sequence -X- _ O
τ -X- _ O
. -X- _ O
Then -X- _ O
with -X- _ O
the -X- _ O
predicted -X- _ O
probabilities -X- _ O
on -X- _ O
the -X- _ O
boundaries -X- _ O
and -X- _ O
type -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
decode -X- _ O
K -X- _ O
eval -X- _ O
candidate -X- _ O
entities -X- _ O
( -X- _ O
l -X- _ O
i -X- _ O
, -X- _ O
r -X- _ O
i -X- _ O
, -X- _ O
c -X- _ O
i -X- _ O
) -X- _ O
K -X- _ O
eval -X- _ O
i=0 -X- _ O
, -X- _ O
where -X- _ O
δ -X- _ O
i -X- _ O
= -X- _ O
argmax -X- _ O
P -X- _ O
δ -X- _ O
i -X- _ O
, -X- _ O
δ -X- _ O
∈ -X- _ O
{ -X- _ O
l -X- _ O
, -X- _ O
r -X- _ O
, -X- _ O
c -X- _ O
} -X- _ O
. -X- _ O
After -X- _ O
that -X- _ O
, -X- _ O
we -X- _ O
employ -X- _ O
two -X- _ O
simple -X- _ O
post -X- _ O
- -X- _ O
processing -X- _ O
operations -X- _ O
on -X- _ O
these -X- _ O
candidates -X- _ O
: -X- _ O
de -X- _ O
- -X- _ O
duplication -X- _ O
and -X- _ O
filtering -X- _ O
. -X- _ O
For -X- _ O
spans -X- _ O
with -X- _ O
identical -X- _ O
boundaries -X- _ O
, -X- _ O
we -X- _ O
keep -X- _ O
the -X- _ O
one -X- _ O
with -X- _ O
the -X- _ O
maximum -X- _ O
type -X- _ O
probability -X- _ O
. -X- _ O
For -X- _ O
spans -X- _ O
with -X- _ O
the -X- _ O
sum -X- _ O
of -X- _ O
prediction -X- _ O
probabilities -X- _ O
less -X- _ O
than -X- _ O
the -X- _ O
threshold -X- _ O
φ -X- _ O
, -X- _ O
we -X- _ O
discard -X- _ O
them -X- _ O
. -X- _ O
The -X- _ O
inference -X- _ O
procedure -X- _ O
is -X- _ O
shown -X- _ O
in -X- _ O
Algorithm -X- _ O
2 -X- _ O
. -X- _ O
5 -X- _ O
Experimental -X- _ O
Settings -X- _ O

For -X- _ O
nested -X- _ O
NER -X- _ B-TaskName
, -X- _ O
we -X- _ O
choose -X- _ O
three -X- _ O
widely -X- _ O
used -X- _ O
datasets -X- _ O
for -X- _ O
evaluation -X- _ O
: -X- _ O
ACE04 -X- _ B-DatasetName
( -X- _ O
Doddington -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2004 -X- _ O
) -X- _ O
, -X- _ O
ACE05 -X- _ B-DatasetName
( -X- _ O
Walker -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2006 -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
GE -X- _ B-DatasetName
- -X- _ I-DatasetName
NIA -X- _ I-DatasetName
( -X- _ O
Ohta -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2002 -X- _ O
) -X- _ O
. -X- _ O
ACE04 -X- _ B-DatasetName
and -X- _ O
ACE05 -X- _ B-DatasetName
belong -X- _ O
to -X- _ O
the -X- _ O
news -X- _ O
domain -X- _ O
and -X- _ O
GENIA -X- _ B-DatasetName
is -X- _ O
in -X- _ O
the -X- _ O
biological -X- _ O
domain -X- _ O
. -X- _ O
For -X- _ O
flat -X- _ O
NER -X- _ B-TaskName
, -X- _ O
we -X- _ O
use -X- _ O
three -X- _ O
common -X- _ O
datasets -X- _ O
to -X- _ O
validate -X- _ O
: -X- _ O
CoNLL03 -X- _ B-DatasetName
( -X- _ O
Tjong -X- _ O
Kim -X- _ O
Sang -X- _ O
and -X- _ O
De -X- _ O
Meulder -X- _ O
, -X- _ O
2003 -X- _ O
) -X- _ O
, -X- _ O
OntoNotes -X- _ B-DatasetName
( -X- _ O
Pradhan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2013 -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
MSRA -X- _ B-DatasetName
( -X- _ O
Levow -X- _ O
, -X- _ O
2006 -X- _ O
) -X- _ O
. -X- _ O
More -X- _ O
details -X- _ O
about -X- _ O
datasets -X- _ O
can -X- _ O
be -X- _ O
found -X- _ O
in -X- _ O
Appendix -X- _ O
B -X- _ O
. -X- _ O

For -X- _ O
a -X- _ O
fair -X- _ O
comparison -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
bert -X- _ O
- -X- _ O
large -X- _ O
( -X- _ O
Devlin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
2015 -X- _ O
) -X- _ O
as -X- _ O
the -X- _ O
default -X- _ O
optimizer -X- _ O
with -X- _ O
a -X- _ O
linear -X- _ B-HyperparameterValue
warmup -X- _ I-HyperparameterValue
and -X- _ O
linear -X- _ B-HyperparameterValue
decay -X- _ I-HyperparameterValue
learning -X- _ B-HyperparameterName
rate -X- _ I-HyperparameterName
schedule -X- _ I-HyperparameterName
. -X- _ O
The -X- _ O
peak -X- _ O
learning -X- _ B-HyperparameterName
rate -X- _ I-HyperparameterName
is -X- _ O
set -X- _ O
as -X- _ O
2e -X- _ B-HyperparameterValue
− -X- _ I-HyperparameterValue
5 -X- _ I-HyperparameterValue
and -X- _ O
the -X- _ O
batch -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
is -X- _ O
8 -X- _ B-HyperparameterValue
. -X- _ O
For -X- _ O
diffusion -X- _ O
model -X- _ O
, -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
noisy -X- _ B-HyperparameterName
spans -X- _ I-HyperparameterName
K -X- _ B-HyperparameterName
( -X- _ O
K -X- _ O
eval -X- _ O
) -X- _ O
is -X- _ O
set -X- _ O
as -X- _ O
60 -X- _ B-HyperparameterValue
, -X- _ O
the -X- _ O
timestep -X- _ B-HyperparameterName
T -X- _ B-HyperparameterName
is -X- _ O
1000 -X- _ B-HyperparameterValue
, -X- _ O
and -X- _ O
the -X- _ O
sampling -X- _ B-HyperparameterName
timestep -X- _ I-HyperparameterName
γ -X- _ B-HyperparameterName
is -X- _ O
5 -X- _ B-HyperparameterValue
with -X- _ O
a -X- _ O
filtering -X- _ B-HyperparameterName
threshold -X- _ I-HyperparameterName
φ -X- _ B-HyperparameterName
= -X- _ O
2.5 -X- _ B-HyperparameterValue
. -X- _ O
The -X- _ O
scale -X- _ B-HyperparameterName
factor -X- _ I-HyperparameterName
λ -X- _ B-HyperparameterName
for -X- _ O
noisy -X- _ O
spans -X- _ O
is -X- _ O
1.0 -X- _ B-HyperparameterValue
. -X- _ O
Please -X- _ O
see -X- _ O
Appendix -X- _ O
C -X- _ O
for -X- _ O
more -X- _ O
details -X- _ O
. -X- _ O

The -X- _ O
results -X- _ O
also -X- _ O
validate -X- _ O
that -X- _ O
our -X- _ O
DIFFUSIONNER -X- _ B-MethodName
can -X- _ O
recover -X- _ O
entity -X- _ O
boundaries -X- _ O
from -X- _ O
noisy -X- _ O
spans -X- _ O
via -X- _ O
boundary -X- _ O
denoising -X- _ O
diffusion -X- _ O
. -X- _ O

Inference -X- _ O
Efficiency -X- _ O
To -X- _ O
further -X- _ O
validate -X- _ O
whether -X- _ O
our -X- _ O
DIFFUSIONNER -X- _ B-MethodName
requires -X- _ O
more -X- _ O
inference -X- _ O
computations -X- _ O
, -X- _ O
we -X- _ O
also -X- _ O
conduct -X- _ O
experiments -X- _ O
to -X- _ O
compare -X- _ O
the -X- _ O
inference -X- _ O
efficiency -X- _ O
between -X- _ O
DIFFUSIONNER -X- _ B-MethodName
and -X- _ O
other -X- _ O
generation -X- _ O
- -X- _ O
based -X- _ O
models -X- _ O
Yan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021a -X- _ O
) -X- _ O
. -X- _ O
Just -X- _ O
as -X- _ O
shown -X- _ O
in -X- _ O
Table -X- _ O
3 -X- _ O
, -X- _ O
we -X- _ O
find -X- _ O
that -X- _ O
DIFFUSIONNER -X- _ B-MethodName
could -X- _ O
achieve -X- _ O
better -X- _ O
performance -X- _ O
while -X- _ O
maintaining -X- _ O
a -X- _ O
faster -X- _ O
inference -X- _ O
speed -X- _ O
with -X- _ O
minimal -X- _ O
parameter -X- _ O
scale -X- _ O
. -X- _ O
Even -X- _ O
with -X- _ O
a -X- _ O
denoising -X- _ B-HyperparameterName
timestep -X- _ I-HyperparameterName
of -X- _ O
γ -X- _ B-HyperparameterName
= -X- _ O
10 -X- _ B-HyperparameterValue
, -X- _ O
DIFFUSIONNER -X- _ B-MethodName
is -X- _ O
18× -X- _ O
and -X- _ O
3× -X- _ O
faster -X- _ O
than -X- _ O
them -X- _ O
. -X- _ O
This -X- _ O
is -X- _ O
because -X- _ O
DIFFU -X- _ B-MethodName
- -X- _ I-MethodName
SIONNER -X- _ I-MethodName
generates -X- _ O
all -X- _ O
entities -X- _ O
in -X- _ O
parallel -X- _ O
within -X- _ O
several -X- _ O
denoising -X- _ O
timesteps -X- _ O
, -X- _ O
which -X- _ O
avoids -X- _ O
generating -X- _ O
the -X- _ O
linearized -X- _ O
entity -X- _ O
sequence -X- _ O
in -X- _ O
an -X- _ O
autoregressive -X- _ O
manner -X- _ O
. -X- _ O
In -X- _ O
addition -X- _ O
, -X- _ O
DIFFUSIONNER -X- _ B-MethodName
shares -X- _ O
sentence -X- _ O
encoder -X- _ O
across -X- _ O
timesteps -X- _ O
, -X- _ O
which -X- _ O
further -X- _ O
accelerates -X- _ O
inference -X- _ O
speed -X- _ O
. -X- _ O
speed -X- _ O
of -X- _ O
DIFFUSIONNER -X- _ B-MethodName
under -X- _ O
various -X- _ O
numbers -X- _ O
of -X- _ O
noisy -X- _ O
spans -X- _ O
. -X- _ O
Just -X- _ O
as -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
3 -X- _ O
, -X- _ O
we -X- _ O
find -X- _ O
that -X- _ O
, -X- _ O
with -X- _ O
an -X- _ O
increase -X- _ O
of -X- _ O
denoising -X- _ O
steps -X- _ O
, -X- _ O
the -X- _ O
model -X- _ O
obtains -X- _ O
incremental -X- _ O
performance -X- _ O
improvement -X- _ O
while -X- _ O
sacrificing -X- _ O
inference -X- _ O
speed -X- _ O
. -X- _ O
Considering -X- _ O
the -X- _ O
trade -X- _ O
- -X- _ O
off -X- _ O
between -X- _ O
performance -X- _ O
and -X- _ O
efficiency -X- _ O
, -X- _ O
we -X- _ O
set -X- _ O
γ -X- _ B-HyperparameterName
= -X- _ O
5 -X- _ B-HyperparameterValue
as -X- _ O
the -X- _ O
default -X- _ O
setting -X- _ O
. -X- _ O
In -X- _ O
addition -X- _ O
, -X- _ O
when -X- _ O
the -X- _ O
noisy -X- _ O
spans -X- _ O
are -X- _ O
smaller -X- _ O
, -X- _ O
the -X- _ O
improvement -X- _ O
brought -X- _ O
by -X- _ O
increasing -X- _ O
the -X- _ O
denoising -X- _ O
timesteps -X- _ O
is -X- _ O
more -X- _ O
obvious -X- _ O
. -X- _ O
This -X- _ O
study -X- _ O
indicates -X- _ O
that -X- _ O
our -X- _ O
DiffusionNER -X- _ B-MethodName
can -X- _ O
effectively -X- _ O
counterbalance -X- _ O
the -X- _ O
negative -X- _ O
impact -X- _ O
of -X- _ O
undersampling -X- _ O
noise -X- _ O
spans -X- _ O
on -X- _ O
performance -X- _ O
by -X- _ O
utilizing -X- _ O
additional -X- _ O
timesteps -X- _ O
. -X- _ O
4 -X- _ O
. -X- _ O
Overall -X- _ O
, -X- _ O
the -X- _ O
model -X- _ O
performance -X- _ O
becomes -X- _ O
better -X- _ O
as -X- _ O
the -X- _ O
sampling -X- _ O
number -X- _ O
of -X- _ O
noisy -X- _ O
spans -X- _ O
increases -X- _ O
. -X- _ O
Specifically -X- _ O
, -X- _ O
we -X- _ O
find -X- _ O
that -X- _ O
DIFFUSIONNER -X- _ B-MethodName
performs -X- _ O
worse -X- _ O
when -X- _ O
K -X- _ B-HyperparameterName
eval -X- _ O
< -X- _ O
30 -X- _ B-HyperparameterValue
. -X- _ O
We -X- _ O
guess -X- _ O
this -X- _ O
is -X- _ O
because -X- _ O
fewer -X- _ O
noisy -X- _ O
spans -X- _ O
may -X- _ O
not -X- _ O
cover -X- _ O
all -X- _ O
potential -X- _ O
entities -X- _ O
. -X- _ O
When -X- _ O
sampling -X- _ O
number -X- _ O
K -X- _ B-HyperparameterName
eval -X- _ O
> -X- _ O
60 -X- _ B-HyperparameterValue
, -X- _ O
we -X- _ O
find -X- _ O
it -X- _ O
could -X- _ O
also -X- _ O
slightly -X- _ O
improve -X- _ O
model -X- _ O
performance -X- _ O
. -X- _ O
Overall -X- _ O
, -X- _ O
the -X- _ O
dynamic -X- _ O
sampling -X- _ O
of -X- _ O
noisy -X- _ O
spans -X- _ O
in -X- _ O
DIFFUSIONNER -X- _ B-MethodName
has -X- _ O
the -X- _ O
following -X- _ O
advantages -X- _ O
: -X- _ O
1 -X- _ O
) -X- _ O
we -X- _ O
can -X- _ O
improve -X- _ O
model -X- _ O
performance -X- _ O
by -X- _ O
controlling -X- _ O
it -X- _ O
to -X- _ O
sample -X- _ O
more -X- _ O
noisy -X- _ O
spans -X- _ O
; -X- _ O
2 -X- _ O
) -X- _ O
dynamic -X- _ O
sampling -X- _ O
strategy -X- _ O
also -X- _ O
allows -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
predict -X- _ O
an -X- _ O
arbitrary -X- _ O
number -X- _ O
of -X- _ O
entities -X- _ O
in -X- _ O
any -X- _ O
realworld -X- _ O
application -X- _ O
, -X- _ O
avoiding -X- _ O
the -X- _ O
limitations -X- _ O
of -X- _ O
the -X- _ O
sampling -X- _ O
number -X- _ O
at -X- _ O
the -X- _ O
training -X- _ O
stage -X- _ O
. -X- _ O

Network -X- _ O
Architecture -X- _ O
As -X- _ O
shown -X- _ O
in -X- _ O
Table -X- _ O
4 -X- _ O
, -X- _ O
we -X- _ O
conduct -X- _ O
experiments -X- _ O
to -X- _ O
investigate -X- _ O
the -X- _ O
network -X- _ O
architecture -X- _ O
of -X- _ O
the -X- _ O
boundary -X- _ O
reverse -X- _ O
diffusion -X- _ O
process -X- _ O
. -X- _ O
We -X- _ O
found -X- _ O
that -X- _ O
DIFFUSIONNER -X- _ B-MethodName
performs -X- _ O
better -X- _ O
with -X- _ O
a -X- _ O
stronger -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
language -X- _ O
model -X- _ O
( -X- _ O
PLM -X- _ O
) -X- _ O
, -X- _ O
as -X- _ O
evidenced -X- _ O
by -X- _ O
an -X- _ O
improvement -X- _ O
of -X- _ O
+0.53 -X- _ B-MetricValue
% -X- _ I-MetricValue
on -X- _ O
ACE04 -X- _ B-DatasetName
and -X- _ O
+0.11 -X- _ B-MetricValue
% -X- _ I-MetricValue
on -X- _ O
CoNLL03 -X- _ B-DatasetName
when -X- _ O
using -X- _ O
roberta -X- _ O
- -X- _ O
large -X- _ O
. -X- _ O
Additionally -X- _ O
, -X- _ O
for -X- _ O
the -X- _ O
span -X- _ O
encoder -X- _ O
, -X- _ O
we -X- _ O
find -X- _ O
that -X- _ O
directly -X- _ O
removing -X- _ O
self -X- _ O
- -X- _ O
attention -X- _ O
between -X- _ O
noisy -X- _ O
spans -X- _ O
or -X- _ O
cross -X- _ O
- -X- _ O
attention -X- _ O
of -X- _ O
spans -X- _ O
to -X- _ O
the -X- _ O
sentence -X- _ O
can -X- _ O
significantly -X- _ O
impair -X- _ O
performance -X- _ O
. -X- _ O
When -X- _ O
both -X- _ O
are -X- _ O
ablated -X- _ O
, -X- _ O
model -X- _ O
performance -X- _ O
decreases -X- _ O
by -X- _ O
1.37 -X- _ B-MetricValue
% -X- _ I-MetricValue
and -X- _ O
1.15 -X- _ B-MetricValue
% -X- _ I-MetricValue
on -X- _ O
ACE04 -X- _ B-DatasetName
and -X- _ O
CoNLL03 -X- _ B-DatasetName
. -X- _ O
These -X- _ O
results -X- _ O
indicate -X- _ O
that -X- _ O
the -X- _ O
interaction -X- _ O
between -X- _ O
the -X- _ O
spans -X- _ O
or -X- _ O
noisy -X- _ O
spans -X- _ O
and -X- _ O
the -X- _ O
sentence -X- _ O
is -X- _ O
necessary -X- _ O
. -X- _ O
the -X- _ O
added -X- _ O
noise -X- _ O
at -X- _ O
each -X- _ O
timestep -X- _ O
during -X- _ O
boundary -X- _ O
forward -X- _ O
diffusion -X- _ O
process -X- _ O
. -X- _ O
Therefore -X- _ O
, -X- _ O
we -X- _ O
analyze -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
DIFFUSIONNER -X- _ B-MethodName
on -X- _ O
different -X- _ O
variance -X- _ O
schedulers -X- _ O
with -X- _ O
different -X- _ O
noise -X- _ O
timesteps -X- _ O
T -X- _ O
. -X- _ O
The -X- _ O
results -X- _ O
on -X- _ O
ACE04 -X- _ B-DatasetName
and -X- _ O
CoNLL03 -X- _ B-DatasetName
are -X- _ O
shown -X- _ O
in -X- _ O
Table -X- _ O
5 -X- _ O
. -X- _ O
We -X- _ O
find -X- _ O
that -X- _ O
the -X- _ O
cosine -X- _ B-HyperparameterValue
scheduler -X- _ B-HyperparameterName
generally -X- _ O
yields -X- _ O
superior -X- _ O
results -X- _ O
on -X- _ O
the -X- _ O
ACE04 -X- _ B-DatasetName
, -X- _ O
while -X- _ O
the -X- _ O
linear -X- _ B-HyperparameterValue
scheduler -X- _ B-HyperparameterName
proves -X- _ O
to -X- _ O
be -X- _ O
more -X- _ O
effective -X- _ O
on -X- _ O
CoNLL03 -X- _ B-DatasetName
. -X- _ O
In -X- _ O
addition -X- _ O
, -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
DIFFU -X- _ B-MethodName
- -X- _ I-MethodName
SIONNER -X- _ I-MethodName
varies -X- _ O
with -X- _ O
the -X- _ O
choice -X- _ O
of -X- _ O
noise -X- _ O
timestep -X- _ O
, -X- _ O
with -X- _ O
the -X- _ O
best -X- _ O
performance -X- _ O
achieved -X- _ O
at -X- _ O
T -X- _ B-HyperparameterName
= -X- _ O
1000 -X- _ B-HyperparameterValue
for -X- _ O
ACE04 -X- _ B-DatasetName
and -X- _ O
T -X- _ B-HyperparameterName
= -X- _ O
1500 -X- _ B-HyperparameterValue
for -X- _ O
CoNLL03 -X- _ B-DatasetName
. -X- _ O

We -X- _ O
conduct -X- _ O
experiments -X- _ O
to -X- _ O
analyze -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
DIFFUSIONNER -X- _ B-MethodName
for -X- _ O
different -X- _ O
expansion -X- _ O
strategies -X- _ O
with -X- _ O
various -X- _ O
numbers -X- _ O
of -X- _ O
noisy -X- _ O
spans -X- _ O
. -X- _ O
The -X- _ O
experimental -X- _ O
results -X- _ O
are -X- _ O
shown -X- _ O
in -X- _ O
Table -X- _ O
6 -X- _ O
. -X- _ O
Generally -X- _ O
, -X- _ O
we -X- _ O
find -X- _ O
that -X- _ O
the -X- _ O
random -X- _ O
strategy -X- _ O
could -X- _ O
achieve -X- _ O
similar -X- _ O
or -X- _ O
better -X- _ O
performance -X- _ O
than -X- _ O
the -X- _ O
repetitive -X- _ O
strategy -X- _ O
. -X- _ O
In -X- _ O
addition -X- _ O
, -X- _ O
Table -X- _ O
6 -X- _ O
shows -X- _ O
that -X- _ O
DIFFU -X- _ B-MethodName
- -X- _ I-MethodName
SIONNER -X- _ I-MethodName
is -X- _ O
insensitive -X- _ O
to -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
noisy -X- _ O
spans -X- _ O
during -X- _ O
training -X- _ O
. -X- _ O
Considering -X- _ O
that -X- _ O
using -X- _ O
more -X- _ O
noisy -X- _ O
spans -X- _ O
brings -X- _ O
more -X- _ O
computation -X- _ O
and -X- _ O
memory -X- _ O
usage -X- _ O
, -X- _ O
we -X- _ O
set -X- _ O
K -X- _ B-HyperparameterName
= -X- _ O
60 -X- _ B-HyperparameterValue
as -X- _ O
the -X- _ O
default -X- _ O
setting -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
present -X- _ O
DIFFUSIONNER -X- _ B-MethodName
, -X- _ O
a -X- _ O
novel -X- _ O
generative -X- _ O
approach -X- _ O
for -X- _ O
NER -X- _ B-TaskName
that -X- _ O
converts -X- _ O
the -X- _ O
task -X- _ O
into -X- _ O
a -X- _ O
boundary -X- _ O
denoising -X- _ O
diffusion -X- _ O
process -X- _ O
. -X- _ O
Our -X- _ O
evaluations -X- _ O
on -X- _ O
six -X- _ O
nested -X- _ O
and -X- _ O
flat -X- _ O
NER -X- _ B-TaskName
datasets -X- _ O
show -X- _ O
that -X- _ O
DIFFUSIONNER -X- _ B-MethodName
achieves -X- _ O
comparable -X- _ O
or -X- _ O
better -X- _ O
performance -X- _ O
compared -X- _ O
to -X- _ O
previous -X- _ O
stateof -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
models -X- _ O
. -X- _ O
Additionally -X- _ O
, -X- _ O
our -X- _ O
additional -X- _ O
analyses -X- _ O
reveal -X- _ O
the -X- _ O
advantages -X- _ O
of -X- _ O
DIFFUSIONNER -X- _ B-MethodName
in -X- _ O
terms -X- _ O
of -X- _ O
inference -X- _ O
speed -X- _ O
, -X- _ O
progressive -X- _ O
boundary -X- _ O
refinement -X- _ O
, -X- _ O
and -X- _ O
dynamic -X- _ O
entity -X- _ O
sampling -X- _ O
. -X- _ O
Overall -X- _ O
, -X- _ O
this -X- _ O
study -X- _ O
is -X- _ O
a -X- _ O
pioneering -X- _ O
effort -X- _ O
of -X- _ O
diffusion -X- _ O
models -X- _ O
for -X- _ O
extractive -X- _ O
tasks -X- _ O
on -X- _ O
discrete -X- _ O
text -X- _ O
sequences -X- _ O
, -X- _ O
and -X- _ O
we -X- _ O
hope -X- _ O
it -X- _ O
may -X- _ O
serve -X- _ O
as -X- _ O
a -X- _ O
catalyst -X- _ O
for -X- _ O
more -X- _ O
research -X- _ O
about -X- _ O
the -X- _ O
potential -X- _ O
of -X- _ O
diffusion -X- _ O
models -X- _ O
in -X- _ O
natural -X- _ O
language -X- _ O
understanding -X- _ O
tasks -X- _ O
. -X- _ O

We -X- _ O
discuss -X- _ O
here -X- _ O
the -X- _ O
limitations -X- _ O
of -X- _ O
the -X- _ O
proposed -X- _ O
DIF -X- _ B-MethodName
- -X- _ I-MethodName
FUSIONNER -X- _ I-MethodName
. -X- _ O
First -X- _ O
, -X- _ O
as -X- _ O
a -X- _ O
latent -X- _ O
generative -X- _ O
model -X- _ O
, -X- _ O
DIFFUSIONNER -X- _ B-MethodName
relies -X- _ O
on -X- _ O
sampling -X- _ O
from -X- _ O
a -X- _ O
Gaussian -X- _ O
distribution -X- _ O
to -X- _ O
produce -X- _ O
noisy -X- _ O
spans -X- _ O
, -X- _ O
which -X- _ O
leads -X- _ O
to -X- _ O
a -X- _ O
random -X- _ O
characteristic -X- _ O
of -X- _ O
entity -X- _ O
generation -X- _ O
. -X- _ O
Second -X- _ O
, -X- _ O
DIFFUSIONNER -X- _ B-MethodName
converges -X- _ O
slowly -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
denoising -X- _ O
training -X- _ O
and -X- _ O
matching -X- _ O
- -X- _ O
based -X- _ O
loss -X- _ O
over -X- _ O
a -X- _ O
large -X- _ O
noise -X- _ O
timestep -X- _ O
. -X- _ O
Finally -X- _ O
, -X- _ O
since -X- _ O
discontinuous -X- _ O
named -X- _ O
entities -X- _ O
often -X- _ O
contain -X- _ O
multiple -X- _ O
fragments -X- _ O
, -X- _ O
DIFFUSIONNER -X- _ B-MethodName
currently -X- _ O
lacks -X- _ O
the -X- _ O
ability -X- _ O
to -X- _ O
generate -X- _ O
such -X- _ O
entities -X- _ O
. -X- _ O
We -X- _ O
can -X- _ O
design -X- _ O
a -X- _ O
simple -X- _ O
classifier -X- _ O
on -X- _ O
top -X- _ O
of -X- _ O
DIFFUSIONNER -X- _ B-MethodName
, -X- _ O
which -X- _ O
is -X- _ O
used -X- _ O
to -X- _ O
combine -X- _ O
entity -X- _ O
fragments -X- _ O
and -X- _ O
thus -X- _ O
solve -X- _ O
the -X- _ O
problem -X- _ O
of -X- _ O
discontinuous -X- _ O
NER -X- _ B-TaskName
. -X- _ O

Given -X- _ O
a -X- _ O
fixed -X- _ O
- -X- _ O
size -X- _ O
set -X- _ O
of -X- _ O
K -X- _ O
noisy -X- _ O
spans -X- _ O
, -X- _ O
DIFFU -X- _ B-MethodName
- -X- _ I-MethodName
SIONNER -X- _ I-MethodName
infers -X- _ O
K -X- _ O
predictions -X- _ O
, -X- _ O
where -X- _ O
K -X- _ O
is -X- _ O
larger -X- _ O
than -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
N -X- _ O
entities -X- _ O
in -X- _ O
a -X- _ O
sentence -X- _ O
. -X- _ O
One -X- _ O
of -X- _ O
the -X- _ O
main -X- _ O
difficulties -X- _ O
of -X- _ O
training -X- _ O
is -X- _ O
to -X- _ O
assign -X- _ O
the -X- _ O
ground -X- _ O
truth -X- _ O
to -X- _ O
the -X- _ O
prediction -X- _ O
. -X- _ O
Thus -X- _ O
we -X- _ O
first -X- _ O
produce -X- _ O
an -X- _ O
optimal -X- _ O
bipartite -X- _ O
matching -X- _ O
between -X- _ O
predicted -X- _ O
and -X- _ O
ground -X- _ O
truth -X- _ O
entities -X- _ O
and -X- _ O
then -X- _ O
optimize -X- _ O
the -X- _ O
likelihood -X- _ O
- -X- _ O
based -X- _ O
loss -X- _ O
. -X- _ O

Assuming -X- _ O
thatŶ -X- _ O
= -X- _ O
{ -X- _ O
Ŷ -X- _ O
i -X- _ O
} -X- _ O
K -X- _ O
i=1 -X- _ O
are -X- _ O
the -X- _ O
set -X- _ O
of -X- _ O
K -X- _ O
predictions -X- _ O
, -X- _ O
whereŶ -X- _ O
i -X- _ O
= -X- _ O
P -X- _ O
l -X- _ O
i -X- _ O
, -X- _ O
P -X- _ O
r -X- _ O
i -X- _ O
, -X- _ O
P -X- _ O
c -X- _ O
i -X- _ O
. -X- _ O
We -X- _ O
denote -X- _ O
the -X- _ O
ground -X- _ O
truth -X- _ O
set -X- _ O
of -X- _ O
N -X- _ O
entities -X- _ O
as -X- _ O
Y -X- _ O
= -X- _ O
{ -X- _ O
( -X- _ O
l -X- _ O
i -X- _ O
, -X- _ O
r -X- _ O
i -X- _ O
, -X- _ O
c -X- _ O
i -X- _ O
) -X- _ O
} -X- _ O
N -X- _ O
i=1 -X- _ O
, -X- _ O
where -X- _ O
l -X- _ O
i -X- _ O
, -X- _ O
r -X- _ O
i -X- _ O
, -X- _ O
c -X- _ O
i -X- _ O
are -X- _ O
the -X- _ O
boundary -X- _ O
indices -X- _ O
and -X- _ O
type -X- _ O
for -X- _ O
the -X- _ O
i -X- _ O
- -X- _ O
th -X- _ O
entity -X- _ O
. -X- _ O
Since -X- _ O
K -X- _ O
is -X- _ O
larger -X- _ O
than -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
N -X- _ O
entities -X- _ O
, -X- _ O
we -X- _ O
pad -X- _ O
Y -X- _ O
with -X- _ O
∅ -X- _ O
( -X- _ O
no -X- _ O
entity -X- _ O
) -X- _ O
. -X- _ O
To -X- _ O
find -X- _ O
a -X- _ O
bipartite -X- _ O
matching -X- _ O
between -X- _ O
these -X- _ O
two -X- _ O
sets -X- _ O
we -X- _ O
search -X- _ O
for -X- _ O
a -X- _ O
permutation -X- _ O
of -X- _ O
K -X- _ O
elements -X- _ O
π -X- _ O
∈ -X- _ O
S -X- _ O
( -X- _ O
K -X- _ O
) -X- _ O
with -X- _ O
the -X- _ O
lowest -X- _ O
cost -X- _ O
: -X- _ O

π -X- _ O
= -X- _ O
arg -X- _ O
min -X- _ O
π∈S -X- _ O
( -X- _ O
K -X- _ O
) -X- _ O
K -X- _ O
i -X- _ O
L -X- _ O
match -X- _ O
Ŷ -X- _ O
i -X- _ O
, -X- _ O
Y -X- _ O
π -X- _ O
( -X- _ O
i -X- _ O
) -X- _ O

where -X- _ O
L -X- _ O
match -X- _ O
Ŷ -X- _ O
i -X- _ O
, -X- _ O
Y -X- _ O
π -X- _ O
( -X- _ O
i -X- _ O
) -X- _ O
is -X- _ O
a -X- _ O
pair -X- _ O
- -X- _ O
wise -X- _ O
matching -X- _ O
cost -X- _ O
between -X- _ O
the -X- _ O
predictionŶ -X- _ O
i -X- _ O
and -X- _ O
ground -X- _ O
truth -X- _ O
Y -X- _ O
π -X- _ O
( -X- _ O
i -X- _ O
) -X- _ O
with -X- _ O
index -X- _ O
π -X- _ O
( -X- _ O
i -X- _ O
) -X- _ O
. -X- _ O
We -X- _ O
define -X- _ O
it -X- _ O
as -X- _ O
−1 -X- _ O
( -X- _ O
Y -X- _ O
π -X- _ O
( -X- _ O
i -X- _ O
) -X- _ O
̸ -X- _ O
= -X- _ O
∅ -X- _ O
) -X- _ O
σ∈ -X- _ O
{ -X- _ O
l -X- _ O
, -X- _ O
r -X- _ O
, -X- _ O
c -X- _ O
} -X- _ O
P -X- _ O
σ -X- _ O
i -X- _ O
Y -X- _ O
σ -X- _ O
π -X- _ O
( -X- _ O
i -X- _ O
) -X- _ O
, -X- _ O
where -X- _ O
1 -X- _ O
( -X- _ O
• -X- _ O
) -X- _ O
denotes -X- _ O
an -X- _ O
indicator -X- _ O
function -X- _ O
. -X- _ O
Finally -X- _ O
, -X- _ O
the -X- _ O
optimal -X- _ O
assignment -X- _ O
π -X- _ O
can -X- _ O
be -X- _ O
computed -X- _ O
with -X- _ O
the -X- _ O
Hungarian -X- _ O
algorithm -X- _ O
. -X- _ O

We -X- _ O
conduct -X- _ O
experiments -X- _ O
on -X- _ O
six -X- _ O
widely -X- _ O
used -X- _ O
NER -X- _ B-TaskName
datasets -X- _ O
, -X- _ O
including -X- _ O
three -X- _ O
nested -X- _ O
and -X- _ O
three -X- _ O
flat -X- _ O
datasets -X- _ O
. -X- _ O
Table -X- _ O
7 -X- _ O
reports -X- _ O
detailed -X- _ O
statistics -X- _ O
about -X- _ O
the -X- _ O
datasets -X- _ O
. -X- _ O

ACE04 -X- _ B-DatasetName
and -X- _ O
ACE05 -X- _ B-DatasetName
( -X- _ O
Doddington -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2004 -X- _ O
; -X- _ O
Walker -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2006 -X- _ O
) -X- _ O
are -X- _ O
two -X- _ O
nested -X- _ O
NER -X- _ B-TaskName
datasets -X- _ O
and -X- _ O
contain -X- _ O
7 -X- _ O
entity -X- _ O
categories -X- _ O
, -X- _ O
including -X- _ O
PER -X- _ O
, -X- _ O
ORG -X- _ O
, -X- _ O
LOC -X- _ O
, -X- _ O
GPE -X- _ O
, -X- _ O
WEA -X- _ O
, -X- _ O
FAC -X- _ O
and -X- _ O
VEH -X- _ O
categories -X- _ O
. -X- _ O
We -X- _ O
follow -X- _ O
the -X- _ O
same -X- _ O
setup -X- _ O
as -X- _ O
previous -X- _ O
works -X- _ O
Katiyar -X- _ O
and -X- _ O
Cardie -X- _ O
( -X- _ O
2018 -X- _ O
) -X- _ O
; -X- _ O
Lin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2019 -X- _ O
) -X- _ O
. -X- _ O

GENIA -X- _ B-DatasetName
( -X- _ O
Ohta -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2002 -X- _ O
) -X- _ O
is -X- _ O
a -X- _ O
biology -X- _ O
nested -X- _ O
NER -X- _ B-TaskName
dataset -X- _ O
and -X- _ O
contains -X- _ O
5 -X- _ O
entity -X- _ O
types -X- _ O
, -X- _ O
including -X- _ O
DNA -X- _ O
, -X- _ O
RNA -X- _ O
, -X- _ O
protein -X- _ O
, -X- _ O
cell -X- _ O
line -X- _ O
and -X- _ O
cell -X- _ O
type -X- _ O
categories -X- _ O
. -X- _ O
Follow -X- _ O
; -X- _ O
Shen -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2021a -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
train -X- _ O
the -X- _ O
model -X- _ O
on -X- _ O
the -X- _ O
concatenation -X- _ O
of -X- _ O
the -X- _ O
train -X- _ O
and -X- _ O
dev -X- _ O
sets -X- _ O
. -X- _ O

CoNLL03 -X- _ B-DatasetName
( -X- _ O
Tjong -X- _ O
Kim -X- _ O
Sang -X- _ O
and -X- _ O
De -X- _ O
Meulder -X- _ O
, -X- _ O
2003 -X- _ O
) -X- _ O
is -X- _ O
a -X- _ O
flat -X- _ O
dataset -X- _ O
with -X- _ O
4 -X- _ O
types -X- _ O
of -X- _ O
named -X- _ O
entities -X- _ O
: -X- _ O
LOC -X- _ O
, -X- _ O
ORG -X- _ O
, -X- _ O
PER -X- _ O
and -X- _ O
MISC -X- _ O
. -X- _ O
Follow -X- _ O
Yu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
; -X- _ O
Yan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2021c -X- _ O
) -X- _ O
; -X- _ O
Shen -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2021a -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
train -X- _ O
our -X- _ O
model -X- _ O
on -X- _ O
the -X- _ O
combination -X- _ O
of -X- _ O
the -X- _ O
train -X- _ O
and -X- _ O
dev -X- _ O
sets -X- _ O
. -X- _ O

OntoNotes -X- _ B-DatasetName
( -X- _ O
Pradhan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2013 -X- _ O
) -X- _ O
is -X- _ O
a -X- _ O
flat -X- _ O
dataset -X- _ O
with -X- _ O
18 -X- _ O
types -X- _ O
of -X- _ O
named -X- _ O
entities -X- _ O
, -X- _ O
including -X- _ O
11 -X- _ O
entity -X- _ O
types -X- _ O
and -X- _ O
7 -X- _ O
value -X- _ O
types -X- _ O
. -X- _ O
We -X- _ O
use -X- _ O
the -X- _ O
same -X- _ O
train -X- _ O
, -X- _ O
development -X- _ O
, -X- _ O
and -X- _ O
test -X- _ O
splits -X- _ O
as -X- _ O
; -X- _ O
. -X- _ O

MSRA -X- _ B-DatasetName
( -X- _ O
Levow -X- _ O
, -X- _ O
2006 -X- _ O
) -X- _ O
is -X- _ O
a -X- _ O
Chinese -X- _ O
flat -X- _ O
dataset -X- _ O
with -X- _ O
3 -X- _ O
entity -X- _ O
types -X- _ O
, -X- _ O
including -X- _ O
ORG -X- _ O
, -X- _ O
PER -X- _ O
, -X- _ O
LOC -X- _ O
. -X- _ O
We -X- _ O
keep -X- _ O
the -X- _ O
same -X- _ O
dataset -X- _ O
splits -X- _ O
and -X- _ O
pre -X- _ O
- -X- _ O
processing -X- _ O
with -X- _ O
Li -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2022a -X- _ O
) -X- _ O
; -X- _ O
Shen -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2021a -X- _ O
) -X- _ O
. -X- _ O

Entity -X- _ O
boundaries -X- _ O
are -X- _ O
predicted -X- _ O
at -X- _ O
the -X- _ O
word -X- _ O
level -X- _ O
, -X- _ O
and -X- _ O
we -X- _ O
use -X- _ O
max -X- _ O
- -X- _ O
pooling -X- _ O
to -X- _ O
aggregate -X- _ O
subwords -X- _ O
into -X- _ O
word -X- _ O
representations -X- _ O
. -X- _ O
We -X- _ O
use -X- _ O
the -X- _ O
multi -X- _ O
- -X- _ O
headed -X- _ O
attention -X- _ O
with -X- _ O
8 -X- _ O
heads -X- _ O
in -X- _ O
the -X- _ O
span -X- _ O
encoder -X- _ O
, -X- _ O
and -X- _ O
add -X- _ O
a -X- _ O
feedforward -X- _ O
network -X- _ O
layer -X- _ O
after -X- _ O
the -X- _ O
self -X- _ O
- -X- _ O
attention -X- _ O
and -X- _ O
cross -X- _ O
- -X- _ O
attention -X- _ O
layer -X- _ O
. -X- _ O
During -X- _ O
training -X- _ O
, -X- _ O
we -X- _ O
first -X- _ O
fix -X- _ O
the -X- _ O
parameters -X- _ O
of -X- _ O
BERT -X- _ O
and -X- _ O
train -X- _ O
the -X- _ O
model -X- _ O
for -X- _ O
5 -X- _ O
epochs -X- _ O
to -X- _ O
warm -X- _ O
up -X- _ O
the -X- _ O
parameters -X- _ O
of -X- _ O
the -X- _ O
entity -X- _ O
decoder -X- _ O
. -X- _ O
We -X- _ O
tune -X- _ O
the -X- _ O
learning -X- _ B-HyperparameterName
rate -X- _ I-HyperparameterName
from -X- _ O
{ -X- _ O
1e -X- _ B-HyperparameterValue
− -X- _ I-HyperparameterValue
5 -X- _ I-HyperparameterValue
, -X- _ O
2e -X- _ B-HyperparameterValue
− -X- _ I-HyperparameterValue
5 -X- _ I-HyperparameterValue
, -X- _ O
3e -X- _ B-HyperparameterValue
− -X- _ I-HyperparameterValue
5 -X- _ I-HyperparameterValue
} -X- _ O
and -X- _ O
the -X- _ O
threshold -X- _ O
φ -X- _ B-HyperparameterName
from -X- _ O
range -X- _ O
[ -X- _ O
2.5 -X- _ B-HyperparameterValue
, -X- _ O
2.7 -X- _ B-HyperparameterValue
] -X- _ O
with -X- _ O
a -X- _ O
step -X- _ B-HyperparameterName
0.05 -X- _ B-HyperparameterValue
, -X- _ O
and -X- _ O
select -X- _ O
the -X- _ O
best -X- _ O
hyperparameter -X- _ O
setting -X- _ O
according -X- _ O
to -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
the -X- _ O
development -X- _ O
set -X- _ O
. -X- _ O
The -X- _ O
detailed -X- _ O
parameter -X- _ O
settings -X- _ O
are -X- _ O
shown -X- _ O
in -X- _ O
Table -X- _ O
8 -X- _ O
. -X- _ O

• -X- _ O
LinearedCRF -X- _ B-MethodName
( -X- _ O
Straková -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
concatenates -X- _ O
the -X- _ O
nested -X- _ O
entity -X- _ O
multiple -X- _ O
labels -X- _ O
into -X- _ O
one -X- _ O
multilabel -X- _ O
, -X- _ O
and -X- _ O
uses -X- _ O
CRF -X- _ O
- -X- _ O
based -X- _ O
tagger -X- _ O
to -X- _ O
decode -X- _ O
flat -X- _ O
or -X- _ O
nested -X- _ O
entities -X- _ O
. -X- _ O

• -X- _ O
CascadedCRF -X- _ B-MethodName
( -X- _ O
Ju -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
stacks -X- _ O
the -X- _ O
flat -X- _ O
NER -X- _ B-TaskName
layers -X- _ O
and -X- _ O
identifies -X- _ O
nested -X- _ O
entities -X- _ O
in -X- _ O
an -X- _ O
inside -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
outside -X- _ O
way -X- _ O
. -X- _ O

• -X- _ O
Pyramid -X- _ B-MethodName
constructs -X- _ O
the -X- _ O
representations -X- _ O
of -X- _ O
mentions -X- _ O
from -X- _ O
the -X- _ O
bottom -X- _ O
up -X- _ O
by -X- _ O
stacking -X- _ O
flat -X- _ O
NER -X- _ O
layers -X- _ O
in -X- _ O
a -X- _ O
pyramid -X- _ O
, -X- _ O
and -X- _ O
allows -X- _ O
bidirectional -X- _ O
interaction -X- _ O
between -X- _ O
layers -X- _ O
by -X- _ O
an -X- _ O
inverse -X- _ O
pyramid -X- _ O
. -X- _ O

• -X- _ O
Seq2seq -X- _ B-MethodName
( -X- _ O
Straková -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
converts -X- _ O
the -X- _ O
labels -X- _ O
of -X- _ O
nested -X- _ O
entities -X- _ O
into -X- _ O
a -X- _ O
sequence -X- _ O
and -X- _ O
then -X- _ O
uses -X- _ O
a -X- _ O
seq2seq -X- _ O
model -X- _ O
to -X- _ O
decode -X- _ O
entities -X- _ O
. -X- _ O
• -X- _ O
BARTNER -X- _ B-MethodName
( -X- _ O
Yan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021b -X- _ O
) -X- _ O
is -X- _ O
also -X- _ O
a -X- _ O
sequence -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
sequence -X- _ O
framework -X- _ O
that -X- _ O
transforms -X- _ O
entity -X- _ O
labels -X- _ O
into -X- _ O
word -X- _ O
index -X- _ O
sequences -X- _ O
and -X- _ O
decodes -X- _ O
entities -X- _ O
in -X- _ O
a -X- _ O
word -X- _ O
- -X- _ O
pointer -X- _ O
manner -X- _ O
. -X- _ O

• -X- _ O
Seq2Set -X- _ B-MethodName
( -X- _ O
Tan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
treats -X- _ O
NER -X- _ B-TaskName
as -X- _ O
a -X- _ O
sequence -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
set -X- _ O
task -X- _ O
and -X- _ O
constructs -X- _ O
learnable -X- _ O
entity -X- _ O
queries -X- _ O
to -X- _ O
generate -X- _ O
entities -X- _ O
. -X- _ O

• -X- _ O
UIE -X- _ B-MethodName
designs -X- _ O
a -X- _ O
special -X- _ O
schema -X- _ O
for -X- _ O
the -X- _ O
conversion -X- _ O
of -X- _ O
structured -X- _ O
information -X- _ O
to -X- _ O
sequences -X- _ O
, -X- _ O
and -X- _ O
adopts -X- _ O
a -X- _ O
generative -X- _ O
model -X- _ O
to -X- _ O
generate -X- _ O
linearized -X- _ O
sequences -X- _ O
to -X- _ O
unify -X- _ O
various -X- _ O
information -X- _ O
extraction -X- _ O
tasks -X- _ O
. -X- _ O

• -X- _ O
Biaffine -X- _ B-MethodName
( -X- _ O
Yu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
reformulates -X- _ O
NER -X- _ B-TaskName
as -X- _ O
a -X- _ O
structured -X- _ O
prediction -X- _ O
task -X- _ O
and -X- _ O
adopts -X- _ O
a -X- _ O
dependency -X- _ O
parsing -X- _ O
approach -X- _ O
for -X- _ O
NER -X- _ B-TaskName
. -X- _ O

• -X- _ O
MRC -X- _ B-MethodName
reformulates -X- _ O
NER -X- _ B-TaskName
as -X- _ O
a -X- _ O
reading -X- _ O
comprehension -X- _ O
task -X- _ O
and -X- _ O
extracts -X- _ O
entities -X- _ O
to -X- _ O
answer -X- _ O
the -X- _ O
type -X- _ O
- -X- _ O
specific -X- _ O
questions -X- _ O
. -X- _ O

• -X- _ O
Locate -X- _ B-MethodName
& -X- _ I-MethodName
label -X- _ I-MethodName
( -X- _ O
Shen -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021a -X- _ O
) -X- _ O
is -X- _ O
a -X- _ O
twostage -X- _ O
method -X- _ O
that -X- _ O
first -X- _ O
regresses -X- _ O
boundaries -X- _ O
to -X- _ O
locate -X- _ O
entities -X- _ O
and -X- _ O
then -X- _ O
performs -X- _ O
entity -X- _ O
typing -X- _ O
. -X- _ O

• -X- _ O
SpanGraph -X- _ B-MethodName
( -X- _ O
Wan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
utilizes -X- _ O
a -X- _ O
retrieval -X- _ O
- -X- _ O
based -X- _ O
span -X- _ O
- -X- _ O
level -X- _ O
graph -X- _ O
to -X- _ O
improve -X- _ O
the -X- _ O
span -X- _ O
representation -X- _ O
, -X- _ O
which -X- _ O
can -X- _ O
connect -X- _ O
spans -X- _ O
and -X- _ O
entities -X- _ O
in -X- _ O
the -X- _ O
training -X- _ O
data -X- _ O
. -X- _ O

• -X- _ O
LLCP -X- _ B-MethodName
( -X- _ O
Lou -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
treat -X- _ O
NER -X- _ B-TaskName
as -X- _ O
latent -X- _ O
lexicalized -X- _ O
constituency -X- _ O
parsing -X- _ O
and -X- _ O
resort -X- _ O
to -X- _ O
constituency -X- _ O
trees -X- _ O
to -X- _ O
model -X- _ O
nested -X- _ O
entities -X- _ O
. -X- _ O

• -X- _ O
BoundarySmooth -X- _ B-MethodName
( -X- _ O
Zhu -X- _ O
and -X- _ O
Li -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
, -X- _ O
inspired -X- _ O
by -X- _ O
label -X- _ O
smoothing -X- _ O
, -X- _ O
proposes -X- _ O
boundary -X- _ O
smoothing -X- _ O
for -X- _ O
span -X- _ O
- -X- _ O
based -X- _ O
NER -X- _ B-TaskName
methods -X- _ O
. -X- _ O

• -X- _ O
Triffine -X- _ B-MethodName
( -X- _ O
Yuan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022b -X- _ O
) -X- _ O
proposes -X- _ O
a -X- _ O
triaffine -X- _ O
mechanism -X- _ O
to -X- _ O
integrate -X- _ O
heterogeneous -X- _ O
factors -X- _ O
to -X- _ O
enhance -X- _ O
the -X- _ O
span -X- _ O
representation -X- _ O
, -X- _ O
including -X- _ O
inside -X- _ O
tokens -X- _ O
, -X- _ O
boundaries -X- _ O
, -X- _ O
labels -X- _ O
, -X- _ O
and -X- _ O
related -X- _ O
spans -X- _ O
. -X- _ O

• -X- _ O
Word2Word -X- _ B-MethodName
( -X- _ O
Li -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022a -X- _ O
) -X- _ O
treats -X- _ O
NER -X- _ B-TaskName
as -X- _ O
word -X- _ O
- -X- _ O
word -X- _ O
relation -X- _ O
classification -X- _ O
and -X- _ O
uses -X- _ O
multi -X- _ O
- -X- _ O
granularity -X- _ O
2D -X- _ O
convolutions -X- _ O
to -X- _ O
construct -X- _ O
the -X- _ O
2D -X- _ O
word -X- _ O
- -X- _ O
word -X- _ O
grid -X- _ O
representations -X- _ O
. -X- _ O
C1 -X- _ O
. -X- _ O
Did -X- _ O
you -X- _ O
report -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
parameters -X- _ O
in -X- _ O
the -X- _ O
models -X- _ O
used -X- _ O
, -X- _ O
the -X- _ O
total -X- _ O
computational -X- _ O
budget -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
GPU -X- _ O
hours -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
computing -X- _ O
infrastructure -X- _ O
used -X- _ O
? -X- _ O
No -X- _ O
response -X- _ O
. -X- _ O

TRIPS -X- _ B-MethodName
: -X- _ O
Efficient -X- _ O
Vision -X- _ O
- -X- _ O
and -X- _ O
- -X- _ O
Language -X- _ O
Pre -X- _ O
- -X- _ O
training -X- _ O
with -X- _ O
Text -X- _ O
- -X- _ O
Relevant -X- _ O
Image -X- _ O
Patch -X- _ O
Selection -X- _ O

Vision -X- _ O
Transformers -X- _ O
( -X- _ O
ViTs -X- _ O
) -X- _ O
have -X- _ O
been -X- _ O
widely -X- _ O
used -X- _ O
in -X- _ O
large -X- _ O
- -X- _ O
scale -X- _ O
Vision -X- _ O
and -X- _ O
Language -X- _ O
Pretraining -X- _ O
( -X- _ O
VLP -X- _ O
) -X- _ O
models -X- _ O
. -X- _ O
Though -X- _ O
previous -X- _ O
VLP -X- _ O
works -X- _ O
have -X- _ O
proved -X- _ O
the -X- _ O
effectiveness -X- _ O
of -X- _ O
ViTs -X- _ O
, -X- _ O
they -X- _ O
still -X- _ O
suffer -X- _ O
from -X- _ O
computational -X- _ O
efficiency -X- _ O
brought -X- _ O
by -X- _ O
the -X- _ O
long -X- _ O
visual -X- _ O
sequence -X- _ O
. -X- _ O
To -X- _ O
tackle -X- _ O
this -X- _ O
problem -X- _ O
, -X- _ O
in -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
an -X- _ O
efficient -X- _ O
vision -X- _ O
- -X- _ O
and -X- _ O
- -X- _ O
language -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
model -X- _ O
with -X- _ O
Text -X- _ B-MethodName
- -X- _ I-MethodName
Relevant -X- _ I-MethodName
Image -X- _ I-MethodName
Patch -X- _ I-MethodName
Selection -X- _ I-MethodName
, -X- _ O
namely -X- _ O
TRIPS -X- _ B-MethodName
, -X- _ O
which -X- _ O
reduces -X- _ O
the -X- _ O
visual -X- _ O
sequence -X- _ O
progressively -X- _ O
with -X- _ O
a -X- _ O
text -X- _ O
- -X- _ O
guided -X- _ O
patchselection -X- _ O
layer -X- _ O
in -X- _ O
the -X- _ O
visual -X- _ O
backbone -X- _ O
for -X- _ O
efficient -X- _ O
training -X- _ O
and -X- _ O
inference -X- _ O
. -X- _ O
The -X- _ O
patchselection -X- _ O
layer -X- _ O
can -X- _ O
dynamically -X- _ O
compute -X- _ O
textdependent -X- _ O
visual -X- _ O
attention -X- _ O
to -X- _ O
identify -X- _ O
the -X- _ O
attentive -X- _ O
image -X- _ O
tokens -X- _ O
with -X- _ O
text -X- _ O
guidance -X- _ O
and -X- _ O
fuse -X- _ O
inattentive -X- _ O
ones -X- _ O
in -X- _ O
an -X- _ O
end -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
end -X- _ O
manner -X- _ O
. -X- _ O
Meanwhile -X- _ O
, -X- _ O
TRIPS -X- _ B-MethodName
does -X- _ O
not -X- _ O
introduce -X- _ O
extra -X- _ O
parameters -X- _ O
to -X- _ O
ViTs -X- _ O
. -X- _ O
Experimental -X- _ O
results -X- _ O
on -X- _ O
a -X- _ O
variety -X- _ O
of -X- _ O
popular -X- _ O
benchmark -X- _ O
datasets -X- _ O
demonstrate -X- _ O
that -X- _ O
TRIPS -X- _ O
gain -X- _ O
a -X- _ O
speedup -X- _ O
of -X- _ O
40 -X- _ O
% -X- _ O
over -X- _ O
previous -X- _ O
similar -X- _ O
VLP -X- _ O
models -X- _ O
, -X- _ O
yet -X- _ O
with -X- _ O
competitive -X- _ O
or -X- _ O
better -X- _ O
downstream -X- _ O
task -X- _ O
performance -X- _ O
. -X- _ O

Introduction -X- _ O

In -X- _ O
recent -X- _ O
years -X- _ O
, -X- _ O
Vision -X- _ O
- -X- _ O
Language -X- _ O
Pre -X- _ O
- -X- _ O
training -X- _ O
( -X- _ O
VLP -X- _ O
) -X- _ O
( -X- _ O
Tan -X- _ O
and -X- _ O
Bansal -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Chen -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Lu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Huang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
; -X- _ O
Su -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
; -X- _ O
Chen -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
; -X- _ O
Zhou -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
; -X- _ O
Li -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
has -X- _ O
developed -X- _ O
at -X- _ O
an -X- _ O
astonishing -X- _ O
rate -X- _ O
and -X- _ O
become -X- _ O
a -X- _ O
prevalent -X- _ O
paradigm -X- _ O
to -X- _ O
tackle -X- _ O
VL -X- _ O
tasks -X- _ O
. -X- _ O
Traditional -X- _ O
VLP -X- _ O
models -X- _ O
( -X- _ O
Tan -X- _ O
and -X- _ O
Bansal -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Chen -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Lu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
utilize -X- _ O
pretrained -X- _ O
object -X- _ O
detectors -X- _ O
( -X- _ O
Ren -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
; -X- _ O
Redmon -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
; -X- _ O
He -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
to -X- _ O
extract -X- _ O
region -X- _ O
- -X- _ O
based -X- _ O
image -X- _ O
features -X- _ O
but -X- _ O
suffer -X- _ O
from -X- _ O
extensive -X- _ O
annotation -X- _ O
and -X- _ O
expensive -X- _ O
computation -X- _ O
of -X- _ O
object -X- _ O
detector -X- _ O
training -X- _ O
. -X- _ O
Inspired -X- _ O
by -X- _ O
the -X- _ O
success -X- _ O
of -X- _ O
the -X- _ O
Vision -X- _ O
Transformer -X- _ O
( -X- _ O
ViT -X- _ O
) -X- _ O
( -X- _ O
Dosovitskiy -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
and -X- _ O
its -X- _ O
variants -X- _ O
; -X- _ O
Wang -X- _ O
* -X- _ O
corresponding -X- _ O
author -X- _ O
. -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021a -X- _ O
) -X- _ O
in -X- _ O
computer -X- _ O
vision -X- _ O
field -X- _ O
, -X- _ O
more -X- _ O
recent -X- _ O
VLP -X- _ O
models -X- _ O
Radford -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
; -X- _ O
Kim -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
; -X- _ O
Wang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021b -X- _ O
; -X- _ O
Singh -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
have -X- _ O
adopted -X- _ O
ViT -X- _ O
as -X- _ O
the -X- _ O
visual -X- _ O
encoder -X- _ O
or -X- _ O
cross -X- _ O
- -X- _ O
modal -X- _ O
fusion -X- _ O
encoder -X- _ O
without -X- _ O
using -X- _ O
region -X- _ O
features -X- _ O
from -X- _ O
the -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
object -X- _ O
detector -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
these -X- _ O
ViT -X- _ O
- -X- _ O
based -X- _ O
VLP -X- _ O
methods -X- _ O
are -X- _ O
required -X- _ O
to -X- _ O
model -X- _ O
long -X- _ O
visual -X- _ O
sequences -X- _ O
from -X- _ O
highresolution -X- _ O
images -X- _ O
for -X- _ O
good -X- _ O
vision -X- _ O
understanding -X- _ O
, -X- _ O
with -X- _ O
quadratic -X- _ O
computational -X- _ O
complexity -X- _ O
to -X- _ O
the -X- _ O
length -X- _ O
of -X- _ O
the -X- _ O
visual -X- _ O
sequence -X- _ O
. -X- _ O
Moreover -X- _ O
, -X- _ O
recent -X- _ O
efforts -X- _ O
( -X- _ O
Dosovitskiy -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
; -X- _ O
Touvron -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
have -X- _ O
also -X- _ O
begun -X- _ O
to -X- _ O
explore -X- _ O
vision -X- _ O
- -X- _ O
language -X- _ O
foundation -X- _ O
models -X- _ O
, -X- _ O
which -X- _ O
scale -X- _ O
up -X- _ O
the -X- _ O
model -X- _ O
and -X- _ O
data -X- _ O
size -X- _ O
. -X- _ O
This -X- _ O
raises -X- _ O
the -X- _ O
necessity -X- _ O
to -X- _ O
decrease -X- _ O
the -X- _ O
high -X- _ O
computational -X- _ O
cost -X- _ O
of -X- _ O
ViT -X- _ O
- -X- _ O
based -X- _ O
VLP -X- _ O
models -X- _ O
. -X- _ O
As -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
1 -X- _ O
( -X- _ O
a -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
find -X- _ O
that -X- _ O
removing -X- _ O
the -X- _ O
inattentive -X- _ O
patch -X- _ O
tokens -X- _ O
of -X- _ O
the -X- _ O
image -X- _ O
[ -X- _ O
CLS -X- _ O
] -X- _ O
token -X- _ O
will -X- _ O
generally -X- _ O
not -X- _ O
affect -X- _ O
the -X- _ O
result -X- _ O
of -X- _ O
Visual -X- _ O
Question -X- _ O
Answering -X- _ O
( -X- _ O
VQA -X- _ O
) -X- _ O
( -X- _ O
Agrawal -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
prediction -X- _ O
. -X- _ O
Based -X- _ O
on -X- _ O
the -X- _ O
observation -X- _ O
, -X- _ O
we -X- _ O
conjecture -X- _ O
that -X- _ O
not -X- _ O
all -X- _ O
image -X- _ O
tokens -X- _ O
in -X- _ O
the -X- _ O
visual -X- _ O
encoder -X- _ O
contribute -X- _ O
positively -X- _ O
to -X- _ O
the -X- _ O
final -X- _ O
prediction -X- _ O
result -X- _ O
of -X- _ O
VLP -X- _ O
models -X- _ O
, -X- _ O
and -X- _ O
large -X- _ O
numbers -X- _ O
of -X- _ O
redundant -X- _ O
image -X- _ O
tokens -X- _ O
exist -X- _ O
. -X- _ O

There -X- _ O
have -X- _ O
been -X- _ O
some -X- _ O
recent -X- _ O
studies -X- _ O
( -X- _ O
Rao -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
; -X- _ O
Xu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021b -X- _ O
; -X- _ O
Zong -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
; -X- _ O
Liang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
focusing -X- _ O
on -X- _ O
ViT -X- _ O
model -X- _ O
accelerations -X- _ O
by -X- _ O
reducing -X- _ O
unrelated -X- _ O
image -X- _ O
tokens -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
these -X- _ O
methods -X- _ O
are -X- _ O
specially -X- _ O
designed -X- _ O
for -X- _ O
computer -X- _ O
vision -X- _ O
tasks -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
image -X- _ O
recognition -X- _ O
) -X- _ O
and -X- _ O
remove -X- _ O
the -X- _ O
redundant -X- _ O
tokens -X- _ O
based -X- _ O
on -X- _ O
visual -X- _ O
semantics -X- _ O
, -X- _ O
ignoring -X- _ O
the -X- _ O
aligned -X- _ O
knowledge -X- _ O
in -X- _ O
text -X- _ O
modality -X- _ O
, -X- _ O
and -X- _ O
thus -X- _ O
are -X- _ O
not -X- _ O
suitable -X- _ O
for -X- _ O
VL -X- _ O
tasks -X- _ O
. -X- _ O
As -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
1 -X- _ O
( -X- _ O
Lu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Li -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Tan -X- _ O
and -X- _ O
Bansal -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Chen -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
mainly -X- _ O
take -X- _ O
a -X- _ O
two -X- _ O
- -X- _ O
step -X- _ O
training -X- _ O
pipeline -X- _ O
approach -X- _ O
, -X- _ O
which -X- _ O
first -X- _ O
extracts -X- _ O
visual -X- _ O
features -X- _ O
by -X- _ O
a -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
object -X- _ O
detector -X- _ O
and -X- _ O
then -X- _ O
trains -X- _ O
the -X- _ O
cross -X- _ O
- -X- _ O
modal -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
model -X- _ O
to -X- _ O
align -X- _ O
text -X- _ O
and -X- _ O
visual -X- _ O
features -X- _ O
. -X- _ O
Some -X- _ O
region -X- _ O
- -X- _ O
based -X- _ O
methods -X- _ O
reduce -X- _ O
the -X- _ O
computation -X- _ O
cost -X- _ O
with -X- _ O
the -X- _ O
lightweight -X- _ O
model -X- _ O
architecture -X- _ O
( -X- _ O
Wang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020a -X- _ O
; -X- _ O
and -X- _ O
knowledge -X- _ O
distillation -X- _ O
( -X- _ O
Fang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
. -X- _ O
The -X- _ O
main -X- _ O
challenge -X- _ O
for -X- _ O
these -X- _ O
methods -X- _ O
is -X- _ O
to -X- _ O
balance -X- _ O
effectiveness -X- _ O
and -X- _ O
efficiency -X- _ O
. -X- _ O
More -X- _ O
recent -X- _ O
CNN -X- _ O
- -X- _ O
based -X- _ O
( -X- _ O
Huang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
; -X- _ O
Xu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021a -X- _ O
) -X- _ O
and -X- _ O
ViTsbased -X- _ O
Kim -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
; -X- _ O
Radford -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
; -X- _ O
Wang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021b -X- _ O
) -X- _ O

ViTs -X- _ O
Acceleration -X- _ O

To -X- _ O
accelerate -X- _ O
the -X- _ O
computation -X- _ O
of -X- _ O
the -X- _ O
transformer -X- _ O
( -X- _ O
Vaswani -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
based -X- _ O
model -X- _ O
, -X- _ O
many -X- _ O
studies -X- _ O
focus -X- _ O
on -X- _ O
proposing -X- _ O
more -X- _ O
efficient -X- _ O
attention -X- _ O
mechanisms -X- _ O
( -X- _ O
Wang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020b -X- _ O
; -X- _ O
Kitaev -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2020 -X- _ O
; -X- _ O
Choromanski -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
or -X- _ O
compress -X- _ O
Transformer -X- _ O
structures -X- _ O
Heo -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
; -X- _ O
Wang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021a -X- _ O
) -X- _ O
. -X- _ O
Recently -X- _ O
, -X- _ O
some -X- _ O
approaches -X- _ O
have -X- _ O
focused -X- _ O
on -X- _ O
accelerating -X- _ O
ViTs -X- _ O
by -X- _ O
reducing -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
tokens -X- _ O
involved -X- _ O
in -X- _ O
the -X- _ O
inference -X- _ O
of -X- _ O
ViTs -X- _ O
. -X- _ O
For -X- _ O
example -X- _ O
, -X- _ O
to -X- _ O
expedite -X- _ O
ViTs -X- _ O
, -X- _ O
Ryoo -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
proposed -X- _ O
TokenLearner -X- _ O
, -X- _ O
in -X- _ O
which -X- _ O
a -X- _ O
relatively -X- _ O
small -X- _ O
amount -X- _ O
of -X- _ O
tokens -X- _ O
are -X- _ O
learned -X- _ O
by -X- _ O
aggregating -X- _ O
the -X- _ O
entire -X- _ O
feature -X- _ O
map -X- _ O
weighted -X- _ O
by -X- _ O
dynamic -X- _ O
attention -X- _ O
. -X- _ O
Rao -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2021 -X- _ O
) -X- _ O
introduces -X- _ O
a -X- _ O
method -X- _ O
to -X- _ O
reduce -X- _ O
tokens -X- _ O
for -X- _ O
a -X- _ O
fully -X- _ O
trained -X- _ O
ViT -X- _ O
, -X- _ O
where -X- _ O
an -X- _ O
extra -X- _ O
learnable -X- _ O
neural -X- _ O
network -X- _ O
is -X- _ O
added -X- _ O
to -X- _ O
ViT -X- _ O
to -X- _ O
select -X- _ O
a -X- _ O
subset -X- _ O
of -X- _ O
tokens -X- _ O
. -X- _ O
Liang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2022 -X- _ O
) -X- _ O
propose -X- _ O
to -X- _ O
reduce -X- _ O
the -X- _ O
computational -X- _ O
overhead -X- _ O
of -X- _ O
inference -X- _ O
by -X- _ O
proposing -X- _ O
a -X- _ O
token -X- _ O
reorganization -X- _ O
method -X- _ O
to -X- _ O
reduce -X- _ O
and -X- _ O
reorganize -X- _ O
image -X- _ O
tokens -X- _ O
progressively -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
those -X- _ O
methods -X- _ O
are -X- _ O
unsuitable -X- _ O
for -X- _ O
VLP -X- _ O
as -X- _ O
they -X- _ O
reduce -X- _ O
the -X- _ O
image -X- _ O
tokens -X- _ O
without -X- _ O
considering -X- _ O
the -X- _ O
text -X- _ O
context -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
section -X- _ O
, -X- _ O
we -X- _ O
will -X- _ O
first -X- _ O
introduce -X- _ O
TRIPS -X- _ B-MethodName
with -X- _ O
the -X- _ O
acceleration -X- _ O
module -X- _ O
of -X- _ O
the -X- _ O
text -X- _ O
- -X- _ O
relevant -X- _ O
patchselection -X- _ O
layer -X- _ O
, -X- _ O
and -X- _ O
then -X- _ O
give -X- _ O
the -X- _ O
details -X- _ O
of -X- _ O
the -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
objectives -X- _ O
. -X- _ O

As -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
2 -X- _ O
( -X- _ O
a -X- _ O
) -X- _ O
, -X- _ O
TRIPS -X- _ B-MethodName
contains -X- _ O
a -X- _ O
visual -X- _ O
encoder -X- _ O
with -X- _ O
text -X- _ O
- -X- _ O
relevant -X- _ O
patch -X- _ O
- -X- _ O
selection -X- _ O
layer -X- _ O
, -X- _ O
a -X- _ O
text -X- _ O
encoder -X- _ O
, -X- _ O
and -X- _ O
a -X- _ O
multimodal -X- _ O
fusion -X- _ O
encoder -X- _ O
. -X- _ O
The -X- _ O
visual -X- _ O
encoder -X- _ O
takes -X- _ O
a -X- _ O
Vision -X- _ O
Transformer -X- _ O
( -X- _ O
ViT -X- _ O
) -X- _ O
, -X- _ O
where -X- _ O
text -X- _ O
- -X- _ O
relevant -X- _ O
patch -X- _ O
- -X- _ O
selection -X- _ O
layers -X- _ O
are -X- _ O
used -X- _ O
to -X- _ O
progressively -X- _ O
reduce -X- _ O
and -X- _ O
reorganize -X- _ O
image -X- _ O
tokens -X- _ O
, -X- _ O
namely -X- _ O
ViT -X- _ O
- -X- _ O
TRIPS -X- _ O
. -X- _ O
The -X- _ O
text -X- _ O
encoder -X- _ O
adopts -X- _ O
BERT -X- _ O
base -X- _ O
transformer -X- _ O
( -X- _ O
Devlin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
. -X- _ O
Similar -X- _ O
with -X- _ O
, -X- _ O
the -X- _ O
multimodal -X- _ O
fusion -X- _ O
encoder -X- _ O
is -X- _ O
a -X- _ O
transformer -X- _ O
encoder -X- _ O
that -X- _ O
performs -X- _ O
the -X- _ O
cross -X- _ O
- -X- _ O
modal -X- _ O
interaction -X- _ O
and -X- _ O
fusion -X- _ O
through -X- _ O
a -X- _ O
crossattention -X- _ O
mechanism -X- _ O
. -X- _ O

Formally -X- _ O
, -X- _ O
given -X- _ O
an -X- _ O
input -X- _ O
image -X- _ O
- -X- _ O
text -X- _ O
pair -X- _ O
, -X- _ O
we -X- _ O
first -X- _ O
feed -X- _ O
the -X- _ O
input -X- _ O
text -X- _ O
to -X- _ O
the -X- _ O
text -X- _ O
encoder -X- _ O
and -X- _ O
represent -X- _ O
it -X- _ O
as -X- _ O
a -X- _ O
sequence -X- _ O
of -X- _ O
embeddings -X- _ O

T -X- _ O
= -X- _ O
{ -X- _ O
t -X- _ O
cls -X- _ O
, -X- _ O
t -X- _ O
1 -X- _ O
, -X- _ O
t -X- _ O
2 -X- _ O
, -X- _ O
• -X- _ O
• -X- _ O
• -X- _ O
, -X- _ O
t -X- _ O
m -X- _ O
} -X- _ O
, -X- _ O

V -X- _ O
= -X- _ O
{ -X- _ O
v -X- _ O
cls -X- _ O
, -X- _ O
v -X- _ O
1 -X- _ O
, -X- _ O
v -X- _ O
2 -X- _ O
, -X- _ O
• -X- _ O
• -X- _ O
• -X- _ O
, -X- _ O
v -X- _ O
e -X- _ O
} -X- _ O
. -X- _ O

Note -X- _ O
that -X- _ O
e -X- _ O
< -X- _ O
u -X- _ O
, -X- _ O
since -X- _ O
we -X- _ O
apply -X- _ O
the -X- _ O
text -X- _ O
- -X- _ O
relevant -X- _ O
patchselection -X- _ O
layer -X- _ O
to -X- _ O
select -X- _ O
the -X- _ O
text -X- _ O
- -X- _ O
aware -X- _ O
image -X- _ O
tokens -X- _ O
and -X- _ O
fuse -X- _ O
the -X- _ O
redundant -X- _ O
tokens -X- _ O
, -X- _ O
allowing -X- _ O
us -X- _ O
to -X- _ O
reduce -X- _ O
total -X- _ O
visual -X- _ O
sequence -X- _ O
length -X- _ O
for -X- _ O
efficiency -X- _ O
. -X- _ O
Finally -X- _ O
, -X- _ O
the -X- _ O
text -X- _ O
features -X- _ O
{ -X- _ O
t -X- _ O
cls -X- _ O
, -X- _ O
t -X- _ O
1 -X- _ O
, -X- _ O
t -X- _ O
2 -X- _ O
, -X- _ O
• -X- _ O
• -X- _ O
• -X- _ O
, -X- _ O
t -X- _ O
m -X- _ O
} -X- _ O
and -X- _ O
the -X- _ O
image -X- _ O
features -X- _ O

{ -X- _ O
v -X- _ O
cls -X- _ O
, -X- _ O
v -X- _ O
1 -X- _ O
, -X- _ O
v -X- _ O
2 -X- _ O
, -X- _ O
• -X- _ O
• -X- _ O
• -X- _ O
, -X- _ O
v -X- _ O
e -X- _ O
} -X- _ O

encoded -X- _ O
by -X- _ O
the -X- _ O
image -X- _ O
encoder -X- _ O
are -X- _ O
fused -X- _ O
by -X- _ O
cross -X- _ O
attention -X- _ O
at -X- _ O
each -X- _ O
layer -X- _ O
of -X- _ O
the -X- _ O
multimodal -X- _ O
encoder -X- _ O
as -X- _ O
in -X- _ O
AL -X- _ O
- -X- _ O
BEF -X- _ O
. -X- _ O
The -X- _ O
output -X- _ O
of -X- _ O
the -X- _ O
multimodal -X- _ O
encoder -X- _ O
is -X- _ O
used -X- _ O
to -X- _ O
pre -X- _ O
- -X- _ O
train -X- _ O
and -X- _ O
finetune -X- _ O
downstream -X- _ O
tasks -X- _ O
. -X- _ O

Text -X- _ B-MethodName
- -X- _ I-MethodName
Relevant -X- _ I-MethodName
Image -X- _ I-MethodName
Patch -X- _ I-MethodName
Selection -X- _ I-MethodName

Existing -X- _ O
works -X- _ O
in -X- _ O
computer -X- _ O
vision -X- _ O
( -X- _ O
Rao -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
; -X- _ O
Liang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2022 -X- _ O
) -X- _ O
ViT -X- _ O
- -X- _ O
TRIPS -X- _ O
ance -X- _ O
of -X- _ O
aligned -X- _ O
textual -X- _ O
content -X- _ O
can -X- _ O
help -X- _ O
the -X- _ O
VLP -X- _ O
model -X- _ O
focus -X- _ O
on -X- _ O
the -X- _ O
key -X- _ O
parts -X- _ O
of -X- _ O
the -X- _ O
image -X- _ O
for -X- _ O
more -X- _ O
effective -X- _ O
and -X- _ O
efficient -X- _ O
cross -X- _ O
- -X- _ O
modal -X- _ O
fusion -X- _ O
. -X- _ O
Here -X- _ O
, -X- _ O
we -X- _ O
present -X- _ O
a -X- _ O
text -X- _ O
- -X- _ O
relevant -X- _ O
patch -X- _ O
- -X- _ O
selection -X- _ O
layer -X- _ O
that -X- _ O
can -X- _ O
dynamically -X- _ O
select -X- _ O
the -X- _ O
image -X- _ O
patches -X- _ O
with -X- _ O
the -X- _ O
guidance -X- _ O
of -X- _ O
textual -X- _ O
input -X- _ O
, -X- _ O
yet -X- _ O
with -X- _ O
no -X- _ O
additional -X- _ O
parameters -X- _ O
introduced -X- _ O
. -X- _ O
As -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
2 -X- _ O
( -X- _ O
b -X- _ O
) -X- _ O
, -X- _ O
for -X- _ O
a -X- _ O
ViT -X- _ O
with -X- _ O
L -X- _ O
standard -X- _ O
Transformer -X- _ O
layers -X- _ O
and -X- _ O
t -X- _ O
patch -X- _ O
- -X- _ O
selection -X- _ O
layers -X- _ O
in -X- _ O
total -X- _ O
, -X- _ O
the -X- _ O
interval -X- _ O
length -X- _ O
is -X- _ O
obtained -X- _ O
as -X- _ O
s -X- _ O
= -X- _ O
L -X- _ O
/ -X- _ O
( -X- _ O
t -X- _ O
+ -X- _ O
1 -X- _ O
) -X- _ O
. -X- _ O
Then -X- _ O
, -X- _ O
we -X- _ O
choose -X- _ O
the -X- _ O
layer -X- _ O
index -X- _ O
j -X- _ O
= -X- _ O
i -X- _ O
* -X- _ O
s -X- _ O
+ -X- _ O
1 -X- _ O
as -X- _ O
the -X- _ O
i -X- _ O
th -X- _ O
patch -X- _ O
- -X- _ O
selection -X- _ O
layer -X- _ O
, -X- _ O
so -X- _ O
that -X- _ O
patch -X- _ O
- -X- _ O
selection -X- _ O
layers -X- _ O
are -X- _ O
uniformly -X- _ O
inserted -X- _ O
into -X- _ O
the -X- _ O
ViT -X- _ O
- -X- _ O
TRIPS -X- _ O
backbone -X- _ O
. -X- _ O
In -X- _ O
each -X- _ O
patch -X- _ O
- -X- _ O
selection -X- _ O
layer -X- _ O
, -X- _ O
as -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
2 -X- _ O
( -X- _ O
c -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
adopt -X- _ O
standard -X- _ O
self -X- _ O
- -X- _ O
attention -X- _ O
( -X- _ O
SA -X- _ O
) -X- _ O
, -X- _ O
Text -X- _ O
- -X- _ O
aware -X- _ O
Dynamic -X- _ O
Attention -X- _ O
( -X- _ O
TD -X- _ O
- -X- _ O
ATT -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
Inattentive -X- _ O
Token -X- _ O
Fusion -X- _ O
( -X- _ O
ITF -X- _ O
) -X- _ O
modules -X- _ O
to -X- _ O
progressively -X- _ O
reduce -X- _ O
image -X- _ O
tokens -X- _ O
. -X- _ O

v -X- _ O
j−1 -X- _ O
= -X- _ O
{ -X- _ O
v -X- _ O
j−1 -X- _ O
cls -X- _ O
, -X- _ O
v -X- _ O
j−1 -X- _ O
1 -X- _ O
, -X- _ O
• -X- _ O
• -X- _ O
• -X- _ O
, -X- _ O
v -X- _ O
j−1 -X- _ O

v -X- _ O
j -X- _ O
= -X- _ O
LN -X- _ O
( -X- _ O
SA -X- _ O
( -X- _ O
v -X- _ O
j−1 -X- _ O
) -X- _ O
+ -X- _ O
v -X- _ O
j−1 -X- _ O
) -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O

Next -X- _ O
, -X- _ O
we -X- _ O
will -X- _ O
illustrate -X- _ O
how -X- _ O
to -X- _ O
use -X- _ O
the -X- _ O
Textaware -X- _ O
Dynamic -X- _ O
Attention -X- _ O
mechanism -X- _ O
( -X- _ O
TD -X- _ O
- -X- _ O
ATT -X- _ O
) -X- _ O
to -X- _ O
select -X- _ O
the -X- _ O
text -X- _ O
- -X- _ O
aware -X- _ O
image -X- _ O
patch -X- _ O
tokens -X- _ O
. -X- _ O
The -X- _ O
text -X- _ O
[ -X- _ O
CLS -X- _ O
] -X- _ O
embedding -X- _ O
t -X- _ O
cls -X- _ O
is -X- _ O
linearly -X- _ O
projected -X- _ O
to -X- _ O
the -X- _ O
query -X- _ O
vector -X- _ O
denoted -X- _ O
as -X- _ O
q -X- _ O
text -X- _ O
by -X- _ O
the -X- _ O
shared -X- _ O
query -X- _ O
linear -X- _ O
layer -X- _ O
of -X- _ O
the -X- _ O
j -X- _ O
th -X- _ O
visual -X- _ O
self -X- _ O
- -X- _ O
attention -X- _ O
layer -X- _ O
. -X- _ O
We -X- _ O
compute -X- _ O
the -X- _ O
text -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
image -X- _ O
attention -X- _ O
feature -X- _ O
map -X- _ O
excluding -X- _ O
the -X- _ O
image -X- _ O
[ -X- _ O
CLS -X- _ O
] -X- _ O
token -X- _ O
as -X- _ O
follows -X- _ O
: -X- _ O

a -X- _ O
cls -X- _ O
= -X- _ O
sof -X- _ O
tmax -X- _ O
( -X- _ O
q -X- _ O
text -X- _ O
• -X- _ O
v -X- _ O
j -X- _ O
[ -X- _ O
1 -X- _ O
: -X- _ O
] -X- _ O
T -X- _ O
√ -X- _ O
d -X- _ O
) -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O

We -X- _ O
identify -X- _ O
and -X- _ O
preserve -X- _ O
the -X- _ O
attentive -X- _ O
image -X- _ O
tokens -X- _ O
corresponding -X- _ O
to -X- _ O
the -X- _ O
k -X- _ O
largest -X- _ O
elements -X- _ O
in -X- _ O
the -X- _ O
attention -X- _ O
map -X- _ O
a -X- _ O
cls -X- _ O
= -X- _ O
{ -X- _ O
a -X- _ O
1 -X- _ O
, -X- _ O
.. -X- _ O
a -X- _ O
n -X- _ O
} -X- _ O
, -X- _ O
where -X- _ O
k -X- _ O
= -X- _ O
n -X- _ O
× -X- _ O
r -X- _ O
, -X- _ O
and -X- _ O
r -X- _ O
is -X- _ O
the -X- _ O
keep -X- _ O
rate -X- _ O
of -X- _ O
this -X- _ O
layer -X- _ O
. -X- _ O
The -X- _ O
selected -X- _ O
image -X- _ O
tokens -X- _ O
are -X- _ O
kept -X- _ O
and -X- _ O
the -X- _ O
un -X- _ O
- -X- _ O
selected -X- _ O
image -X- _ O
tokens -X- _ O
are -X- _ O
further -X- _ O
fused -X- _ O
by -X- _ O
an -X- _ O
inattentive -X- _ O
token -X- _ O
fusion -X- _ O
operation -X- _ O
ITF -X- _ O
. -X- _ O

{ -X- _ O
v -X- _ O
z -X- _ O
1 -X- _ O
, -X- _ O
v -X- _ O
z -X- _ O
2 -X- _ O
, -X- _ O
• -X- _ O
• -X- _ O
• -X- _ O
, -X- _ O
v -X- _ O
z -X- _ O
n−k -X- _ O
} -X- _ O
are -X- _ O
treated -X- _ O
as -X- _ O
text -X- _ O
- -X- _ O
irrelevant -X- _ O
tokens -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
the -X- _ O
fixed -X- _ O
keep -X- _ O
rate -X- _ O
may -X- _ O
remove -X- _ O
some -X- _ O
useful -X- _ O
tokens -X- _ O
, -X- _ O
so -X- _ O
we -X- _ O
fuse -X- _ O
inattentive -X- _ O
tokens -X- _ O
to -X- _ O
one -X- _ O
token -X- _ O
v -X- _ O
f -X- _ O
by -X- _ O
a -X- _ O
weighted -X- _ O
sum -X- _ O
operation -X- _ O
to -X- _ O
supplement -X- _ O
attentive -X- _ O
ones -X- _ O
as -X- _ O
follow -X- _ O
: -X- _ O

v -X- _ O
f -X- _ O
= -X- _ O
n−k -X- _ O
i=1 -X- _ O
a -X- _ O
cls -X- _ O
, -X- _ O
z -X- _ O
i -X- _ O
•v -X- _ O
z -X- _ O
i -X- _ O
( -X- _ O
3 -X- _ O
) -X- _ O

After -X- _ O
fusing -X- _ O
the -X- _ O
inattentive -X- _ O
patch -X- _ O
tokens -X- _ O
, -X- _ O
we -X- _ O
reconstruct -X- _ O
the -X- _ O
j -X- _ O
th -X- _ O
visual -X- _ O
sequence -X- _ O
as -X- _ O

v -X- _ O
j -X- _ O
= -X- _ O
v -X- _ O
j -X- _ O
cls -X- _ O
, -X- _ O
v -X- _ O
j -X- _ O
1 -X- _ O
, -X- _ O
• -X- _ O
• -X- _ O
• -X- _ O
, -X- _ O
v -X- _ O
j -X- _ O
k -X- _ O
, -X- _ O
v -X- _ O
j -X- _ O
f -X- _ O

, -X- _ O
which -X- _ O
consists -X- _ O
of -X- _ O
the -X- _ O
image -X- _ O
[ -X- _ O
CLS -X- _ O
] -X- _ O
token -X- _ O
embedding -X- _ O
, -X- _ O
the -X- _ O
selected -X- _ O
textaware -X- _ O
image -X- _ O
patch -X- _ O
embedding -X- _ O
, -X- _ O
and -X- _ O
fused -X- _ O
inattentive -X- _ O
patch -X- _ O
embedding -X- _ O
. -X- _ O
Then -X- _ O
the -X- _ O
new -X- _ O
visual -X- _ O
sequence -X- _ O
is -X- _ O
fed -X- _ O
to -X- _ O
the -X- _ O
feed -X- _ O
- -X- _ O
forward -X- _ O
network -X- _ O
( -X- _ O
FFN -X- _ O
) -X- _ O
. -X- _ O

Extension -X- _ O
to -X- _ O
Single -X- _ O
- -X- _ O
stream -X- _ O
Model -X- _ O
. -X- _ O
The -X- _ O
proposed -X- _ O
Text -X- _ B-MethodName
- -X- _ I-MethodName
Relevant -X- _ I-MethodName
Image -X- _ I-MethodName
Patch -X- _ I-MethodName
Selection -X- _ I-MethodName
layer -X- _ O
can -X- _ O
be -X- _ O
also -X- _ O
extended -X- _ O
to -X- _ O
the -X- _ O
single -X- _ O
- -X- _ O
stream -X- _ O
model -X- _ O
( -X- _ O
TRIPS -X- _ O
- -X- _ O
S -X- _ O
) -X- _ O
, -X- _ O
which -X- _ O
employs -X- _ O
the -X- _ O
[ -X- _ O
CLS -X- _ O
] -X- _ O
token -X- _ O
of -X- _ O
the -X- _ O
multimodal -X- _ O
encoder -X- _ O
to -X- _ O
preserve -X- _ O
the -X- _ O
attentive -X- _ O
image -X- _ O
tokens -X- _ O
and -X- _ O
fuses -X- _ O
the -X- _ O
inattentive -X- _ O
tokens -X- _ O
to -X- _ O
speed -X- _ O
up -X- _ O
the -X- _ O
training -X- _ O
and -X- _ O
inference -X- _ O
. -X- _ O
While -X- _ O
parameterefficient -X- _ O
, -X- _ O
it -X- _ O
may -X- _ O
be -X- _ O
difficult -X- _ O
to -X- _ O
learn -X- _ O
uni -X- _ O
- -X- _ O
modal -X- _ O
and -X- _ O
multi -X- _ O
- -X- _ O
modal -X- _ O
interactions -X- _ O
concurrently -X- _ O
. -X- _ O
Therefore -X- _ O
, -X- _ O
its -X- _ O
performance -X- _ O
lags -X- _ O
behind -X- _ O
two -X- _ O
- -X- _ O
stream -X- _ O
performance -X- _ O
on -X- _ O
downstream -X- _ O
VL -X- _ O
tasks -X- _ O
. -X- _ O

We -X- _ O
pre -X- _ O
- -X- _ O
train -X- _ O
our -X- _ O
model -X- _ O
with -X- _ O
three -X- _ O
standard -X- _ O
objectives -X- _ O
: -X- _ O
We -X- _ O
take -X- _ O
the -X- _ O
text -X- _ O
[ -X- _ O
CLS -X- _ O
] -X- _ O
embedding -X- _ O
of -X- _ O
the -X- _ O
multimodal -X- _ O
encoder -X- _ O
's -X- _ O
output -X- _ O
as -X- _ O
the -X- _ O
joint -X- _ O
representation -X- _ O
, -X- _ O
followed -X- _ O
by -X- _ O
a -X- _ O
Multi -X- _ O
- -X- _ O
Layer -X- _ O
Perceptron -X- _ O
( -X- _ O
MLP -X- _ O
) -X- _ O
layer -X- _ O
for -X- _ O
prediction -X- _ O
. -X- _ O
Masked -X- _ O
Language -X- _ O
Modeling -X- _ O
( -X- _ O
MLM -X- _ O
) -X- _ O
The -X- _ O
task -X- _ O
setup -X- _ O
is -X- _ O
basically -X- _ O
the -X- _ O
same -X- _ O
as -X- _ O
in -X- _ O
BERT -X- _ O
( -X- _ O
Devlin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
where -X- _ O
we -X- _ O
randomly -X- _ O
mask -X- _ O
15 -X- _ O
% -X- _ O
of -X- _ O
tokens -X- _ O
in -X- _ O
text -X- _ O
and -X- _ O
the -X- _ O
model -X- _ O
is -X- _ O
asked -X- _ O
to -X- _ O
predict -X- _ O
these -X- _ O
masked -X- _ O
words -X- _ O
with -X- _ O
the -X- _ O
cross -X- _ O
- -X- _ O
modal -X- _ O
representations -X- _ O
. -X- _ O

We -X- _ O
pre -X- _ O
- -X- _ O
train -X- _ O
the -X- _ O
TRIPS -X- _ B-MethodName
for -X- _ O
30 -X- _ O
epochs -X- _ O
with -X- _ O
a -X- _ O
total -X- _ O
batch -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
of -X- _ O
512 -X- _ B-HyperparameterValue
on -X- _ O
8 -X- _ O
NVIDIA -X- _ O
V100 -X- _ O
GPUs -X- _ O
. -X- _ O
We -X- _ O
initialize -X- _ O
the -X- _ O
visual -X- _ O
encoder -X- _ O
by -X- _ O
CLIP -X- _ O
( -X- _ O
ViT -X- _ O
- -X- _ O
B -X- _ O
/ -X- _ O
16 -X- _ O
) -X- _ O
( -X- _ O
Radford -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
on -X- _ O
400 -X- _ O
M -X- _ O
noisy -X- _ O
image -X- _ O
- -X- _ O
text -X- _ O
pairs -X- _ O
and -X- _ O
we -X- _ O
use -X- _ O
the -X- _ O
AdamW -X- _ B-HyperparameterValue
( -X- _ O
Loshchilov -X- _ O
and -X- _ O
Hutter -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
optimizer -X- _ B-HyperparameterName
with -X- _ O
a -X- _ O
weight -X- _ B-HyperparameterName
decay -X- _ I-HyperparameterName
of -X- _ O
1e-2 -X- _ B-HyperparameterValue
. -X- _ O
The -X- _ O
learning -X- _ B-HyperparameterName
rate -X- _ I-HyperparameterName
is -X- _ O
warmed -X- _ B-HyperparameterValue
- -X- _ I-HyperparameterValue
up -X- _ I-HyperparameterValue
to -X- _ O
1e-5 -X- _ B-HyperparameterValue
( -X- _ O
ViT -X- _ O
- -X- _ O
B -X- _ O
/ -X- _ O
16 -X- _ O
) -X- _ O
and -X- _ O
1e-4 -X- _ B-HyperparameterValue
( -X- _ O
BERT -X- _ O
base -X- _ O
) -X- _ O
in -X- _ O
the -X- _ O
first -X- _ O
1000 -X- _ O
iterations -X- _ O
. -X- _ O
During -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
, -X- _ O
we -X- _ O
take -X- _ O
the -X- _ O
image -X- _ O
with -X- _ O
the -X- _ O
resolution -X- _ O
of -X- _ O
256 -X- _ O
× -X- _ O
256 -X- _ O
as -X- _ O
input -X- _ O
and -X- _ O
increase -X- _ O
the -X- _ O
image -X- _ O
resolution -X- _ O
during -X- _ O
finetuning -X- _ O
. -X- _ O
We -X- _ O
use -X- _ O
a -X- _ O
6 -X- _ O
- -X- _ O
layer -X- _ O
Transformer -X- _ O
for -X- _ O
both -X- _ O
the -X- _ O
text -X- _ O
encoder -X- _ O
and -X- _ O
the -X- _ O
cross -X- _ O
- -X- _ O
modal -X- _ O
fusion -X- _ O
network -X- _ O
. -X- _ O
As -X- _ O
, -X- _ O
the -X- _ O
text -X- _ O
encoder -X- _ O
is -X- _ O
initialized -X- _ O
using -X- _ O
the -X- _ O
first -X- _ O
6 -X- _ O
layers -X- _ O
of -X- _ O
the -X- _ O
BERT -X- _ O
base -X- _ O
( -X- _ O
Devlin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
model -X- _ O
and -X- _ O
the -X- _ O
cross -X- _ O
- -X- _ O
modal -X- _ O
network -X- _ O
is -X- _ O
initialized -X- _ O
using -X- _ O
the -X- _ O
last -X- _ O
6 -X- _ O
layers -X- _ O
of -X- _ O
the -X- _ O
BERT -X- _ O
base -X- _ O
. -X- _ O
For -X- _ O
the -X- _ O
image -X- _ O
- -X- _ O
text -X- _ O
contrastive -X- _ O
learning -X- _ O
, -X- _ O
the -X- _ O
queue -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
is -X- _ O
set -X- _ O
as -X- _ O
65,536 -X- _ B-HyperparameterValue
and -X- _ O
the -X- _ O
momentum -X- _ B-HyperparameterName
coefficient -X- _ I-HyperparameterName
is -X- _ O
set -X- _ O
as -X- _ O
0.995 -X- _ B-HyperparameterValue
. -X- _ O

We -X- _ O
construct -X- _ O
our -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
data -X- _ O
using -X- _ O
two -X- _ O
web -X- _ O
datasets -X- _ O
( -X- _ O
Conceptual -X- _ B-DatasetName
Captions -X- _ I-DatasetName
( -X- _ O
Sharma -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
, -X- _ O
SBU -X- _ B-DatasetName
Captions -X- _ I-DatasetName
( -X- _ O
Ordonez -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2011 -X- _ O
) -X- _ O
) -X- _ O
and -X- _ O
two -X- _ O
in -X- _ O
- -X- _ O
domain -X- _ O
datasets -X- _ O
( -X- _ O
MSCOCO -X- _ B-DatasetName
( -X- _ O
Lin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2014 -X- _ O
) -X- _ O
and -X- _ O
Visual -X- _ B-DatasetName
Genome -X- _ I-DatasetName
( -X- _ O
Krishna -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
) -X- _ O
. -X- _ O
The -X- _ O
total -X- _ O
number -X- _ O
of -X- _ O
unique -X- _ O
images -X- _ O
is -X- _ O
4.0 -X- _ O
M -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
image -X- _ O
- -X- _ O
text -X- _ O
pairs -X- _ O
is -X- _ O
5.1M -X- _ O
. -X- _ O

We -X- _ O
evaluate -X- _ O
our -X- _ O
model -X- _ O
TRIPS -X- _ B-MethodName
on -X- _ O
three -X- _ O
widely -X- _ O
explored -X- _ O
vision -X- _ O
- -X- _ O
language -X- _ O
downstream -X- _ O
tasks -X- _ O
: -X- _ O
Visual -X- _ B-TaskName
Question -X- _ I-TaskName
Answering -X- _ I-TaskName
( -X- _ O
VQA -X- _ B-TaskName
) -X- _ O
, -X- _ O
Cross -X- _ B-TaskName
- -X- _ I-TaskName
modal -X- _ I-TaskName
Retrieval -X- _ I-TaskName
, -X- _ O
and -X- _ O
Natural -X- _ B-TaskName
Language -X- _ I-TaskName
for -X- _ I-TaskName
Visual -X- _ I-TaskName
Reasoning -X- _ I-TaskName
( -X- _ O
NLVR -X- _ B-TaskName
) -X- _ O
. -X- _ O
For -X- _ O
the -X- _ O
proposed -X- _ O
model -X- _ O
TRIPS -X- _ B-MethodName
, -X- _ O
we -X- _ O
take -X- _ O
the -X- _ O
5 -X- _ O
- -X- _ O
th -X- _ O
and -X- _ O
10 -X- _ O
- -X- _ O
th -X- _ O
as -X- _ O
the -X- _ O
patch -X- _ O
- -X- _ O
selection -X- _ O
layer -X- _ O
in -X- _ O
ViT -X- _ O
encoder -X- _ O
and -X- _ O
set -X- _ O
the -X- _ O
keep -X- _ O
rate -X- _ O
of -X- _ O
each -X- _ O
layer -X- _ O
to -X- _ O
70 -X- _ O
% -X- _ O
, -X- _ O
achieving -X- _ O
the -X- _ O
desired -X- _ O
trade -X- _ O
- -X- _ O
off -X- _ O
between -X- _ O
the -X- _ O
downstream -X- _ O
task -X- _ O
performance -X- _ O
and -X- _ O
the -X- _ O
model -X- _ O
inference -X- _ O
speed -X- _ O
. -X- _ O
Details -X- _ O
of -X- _ O
the -X- _ O
datasets -X- _ O
and -X- _ O
fine -X- _ O
- -X- _ O
tuning -X- _ O
hyperparameters -X- _ O
are -X- _ O
in -X- _ O
Appendix -X- _ O
B. -X- _ O
Details -X- _ O
of -X- _ O
the -X- _ O
comparison -X- _ O
methods -X- _ O
are -X- _ O
in -X- _ O
Appendix -X- _ O
A -X- _ O
. -X- _ O

The -X- _ O
VQA -X- _ B-TaskName
task -X- _ O
( -X- _ O
Agrawal -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
requires -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
answer -X- _ O
natural -X- _ O
language -X- _ O
questions -X- _ O
given -X- _ O
an -X- _ O
image -X- _ O
. -X- _ O
We -X- _ O
follow -X- _ O
and -X- _ O
consider -X- _ O
VQA -X- _ B-TaskName
as -X- _ O
an -X- _ O
answer -X- _ O
generation -X- _ O
problem -X- _ O
. -X- _ O
We -X- _ O
report -X- _ O
test -X- _ O
- -X- _ O
dev -X- _ O
and -X- _ O
test -X- _ O
- -X- _ O
std -X- _ O
scores -X- _ O
by -X- _ O
submitting -X- _ O
our -X- _ O
results -X- _ O
to -X- _ O
the -X- _ O
evaluation -X- _ O
server -X- _ O
1 -X- _ O
in -X- _ O
Table -X- _ O
2 -X- _ O
. -X- _ O
Compared -X- _ O
with -X- _ O
the -X- _ O
VLP -X- _ O
baselines -X- _ O
, -X- _ O
TRIPS -X- _ B-MethodName
can -X- _ O
improve -X- _ O
the -X- _ O
performance -X- _ O
on -X- _ O
the -X- _ O
VQA -X- _ B-TaskName
task -X- _ O
. -X- _ O
The -X- _ O
results -X- _ O
demonstrate -X- _ O
the -X- _ O
effectiveness -X- _ O
of -X- _ O
TRIPS -X- _ B-MethodName
. -X- _ O

The -X- _ O
NLVR2 -X- _ B-TaskName
( -X- _ O
Suhr -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
task -X- _ O
requires -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
predict -X- _ O
whether -X- _ O
a -X- _ O
sentence -X- _ O
describes -X- _ O
a -X- _ O
pair -X- _ O
of -X- _ O
images -X- _ O
which -X- _ O
is -X- _ O
a -X- _ O
binary -X- _ O
classification -X- _ O
task -X- _ O
. -X- _ O
We -X- _ O
follow -X- _ O
) -X- _ O
and -X- _ O
use -X- _ O
two -X- _ O
crossattention -X- _ O
layers -X- _ O
to -X- _ O
process -X- _ O
the -X- _ O
two -X- _ O
input -X- _ O
images -X- _ O
, -X- _ O
and -X- _ O
their -X- _ O
outputs -X- _ O
are -X- _ O
merged -X- _ O
and -X- _ O
fed -X- _ O
to -X- _ O
a -X- _ O
Feed -X- _ O
Forward -X- _ O
Network -X- _ O
( -X- _ O
FFN -X- _ O
) -X- _ O
. -X- _ O
An -X- _ O
MLP -X- _ O
classifier -X- _ O
is -X- _ O
then -X- _ O
applied -X- _ O
to -X- _ O
the -X- _ O
output -X- _ O
embedding -X- _ O
of -X- _ O
the -X- _ O
text -X- _ O
[ -X- _ O
CLS -X- _ O
] -X- _ O
token -X- _ O
. -X- _ O
As -X- _ O
shown -X- _ O
in -X- _ O
Table -X- _ O
2 -X- _ O
, -X- _ O
TRIPS -X- _ B-MethodName
has -X- _ O
a -X- _ O
better -X- _ O
performance -X- _ O
than -X- _ O
existing -X- _ O
VLP -X- _ O
methods -X- _ O
. -X- _ O

We -X- _ O
conduct -X- _ O
experiments -X- _ O
for -X- _ O
both -X- _ O
image -X- _ B-TaskName
- -X- _ I-TaskName
to -X- _ I-TaskName
- -X- _ I-TaskName
text -X- _ I-TaskName
retrieval -X- _ I-TaskName
( -X- _ O
TR -X- _ B-TaskName
) -X- _ O
and -X- _ O
text -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
image -X- _ O
retrieval -X- _ O
( -X- _ O
IR -X- _ B-TaskName
) -X- _ O
on -X- _ O
MSCOCO -X- _ B-DatasetName
( -X- _ O
Lin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2014 -X- _ O
) -X- _ O
( -X- _ O
Kim -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
. -X- _ O
Since -X- _ O
FLOPs -X- _ O
are -X- _ O
proportional -X- _ O
to -X- _ O
input -X- _ O
size -X- _ O
, -X- _ O
for -X- _ O
a -X- _ O
fair -X- _ O
comparison -X- _ O
, -X- _ O
we -X- _ O
keep -X- _ O
same -X- _ O
the -X- _ O
input -X- _ O
size -X- _ O
with -X- _ O
( -X- _ O
Kim -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
which -X- _ O
is -X- _ O
197 -X- _ O
for -X- _ O
image -X- _ O
patches -X- _ O
length -X- _ O
and -X- _ O
40 -X- _ O
for -X- _ O
text -X- _ O
tokens -X- _ O
length -X- _ O
. -X- _ O
We -X- _ O
keep -X- _ O
the -X- _ O
same -X- _ O
setting -X- _ O
when -X- _ O
calculating -X- _ O
throughput -X- _ O
and -X- _ O
latency -X- _ O
. -X- _ O
mer -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
datasets -X- _ O
. -X- _ O
We -X- _ O
jointly -X- _ O
optimize -X- _ O
the -X- _ O
ITC -X- _ O
loss -X- _ O
and -X- _ O
the -X- _ O
ITM -X- _ O
loss -X- _ O
during -X- _ O
fine -X- _ O
- -X- _ O
tuning -X- _ O
. -X- _ O
The -X- _ O
results -X- _ O
are -X- _ O
reported -X- _ O
in -X- _ O
Table -X- _ O
1 -X- _ O
. -X- _ O
As -X- _ O
shown -X- _ O
in -X- _ O
Table -X- _ O
1 -X- _ O
, -X- _ O
the -X- _ O
experimental -X- _ O
results -X- _ O
show -X- _ O
that -X- _ O
our -X- _ O
model -X- _ O
gets -X- _ O
comparable -X- _ O
performance -X- _ O
with -X- _ O
other -X- _ O
VLP -X- _ O
baselines -X- _ O
. -X- _ O
lowest -X- _ O
computational -X- _ O
complexity -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
20.89 -X- _ O
of -X- _ O
FLOPs -X- _ O
) -X- _ O
but -X- _ O
also -X- _ O
the -X- _ O
fastest -X- _ O
computational -X- _ O
speed -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
343.05 -X- _ O
of -X- _ O
Throughput -X- _ O
and -X- _ O
11ms -X- _ O
of -X- _ O
Latency -X- _ O
) -X- _ O
. -X- _ O

To -X- _ O
validate -X- _ O
the -X- _ O
impact -X- _ O
of -X- _ O
patch -X- _ O
- -X- _ O
selection -X- _ O
layer -X- _ O
location -X- _ O
and -X- _ O
keep -X- _ O
rate -X- _ O
on -X- _ O
the -X- _ O
efficiency -X- _ O
and -X- _ O
effectiveness -X- _ O
of -X- _ O
the -X- _ O
model -X- _ O
, -X- _ O
we -X- _ O
train -X- _ O
TRIPS -X- _ B-MethodName
with -X- _ O
different -X- _ O
patch -X- _ O
- -X- _ O
selection -X- _ O
locations -X- _ O
and -X- _ O
numbers -X- _ O
of -X- _ O
selected -X- _ O
tokens -X- _ O
. -X- _ O
The -X- _ O
results -X- _ O
in -X- _ O
Table -X- _ O
4 -X- _ O
demonstrate -X- _ O
two -X- _ O
findings -X- _ O
. -X- _ O
First -X- _ O
, -X- _ O
moving -X- _ O
the -X- _ O
patch -X- _ O
- -X- _ O
selection -X- _ O
layer -X- _ O
into -X- _ O
shallower -X- _ O
layers -X- _ O
reduces -X- _ O
computational -X- _ O
complexity -X- _ O
but -X- _ O
deteriorates -X- _ O
the -X- _ O
accuracy -X- _ O
. -X- _ O
For -X- _ O
example -X- _ O
, -X- _ O
the -X- _ O
accuracy -X- _ B-MetricName
drops -X- _ O
considerably -X- _ O
with -X- _ O
the -X- _ O
remarkable -X- _ O
throughput -X- _ O
increasing -X- _ O
when -X- _ O
the -X- _ O
patchselection -X- _ O
layer -X- _ O
is -X- _ O
placed -X- _ O
before -X- _ O
the -X- _ O
third -X- _ O
layer -X- _ O
( -X- _ O
i.e. -X- _ O
, -X- _ O
at -X- _ O
the -X- _ O
second -X- _ O
layer -X- _ O
) -X- _ O
. -X- _ O
A -X- _ O
possible -X- _ O
explanation -X- _ O
is -X- _ O
that -X- _ O
the -X- _ O
attention -X- _ O
maps -X- _ O
between -X- _ O
the -X- _ O
text -X- _ O
[ -X- _ O
CLS -X- _ O
] -X- _ O
embedding -X- _ O
and -X- _ O
the -X- _ O
input -X- _ O
tokens -X- _ O
can -X- _ O
be -X- _ O
unreliable -X- _ O
during -X- _ O
the -X- _ O
early -X- _ O
processing -X- _ O
of -X- _ O
input -X- _ O
tokens -X- _ O
in -X- _ O
shallow -X- _ O
layers -X- _ O
. -X- _ O
Second -X- _ O
, -X- _ O
too -X- _ O
many -X- _ O
image -X- _ O
tokens -X- _ O
fused -X- _ O
in -X- _ O
the -X- _ O
patch -X- _ O
- -X- _ O
selection -X- _ O
layer -X- _ O
will -X- _ O
considerably -X- _ O
drop -X- _ O
the -X- _ O
downstream -X- _ O
task -X- _ O
performance -X- _ O
. -X- _ O
For -X- _ O
example -X- _ O
, -X- _ O
if -X- _ O
we -X- _ O
take -X- _ O
the -X- _ O
2 -X- _ O
- -X- _ O
nd -X- _ O
and -X- _ O
4 -X- _ O
- -X- _ O
th -X- _ O
layers -X- _ O
in -X- _ O
ViT -X- _ O
as -X- _ O
the -X- _ O
patchselection -X- _ O
layer -X- _ O
and -X- _ O
set -X- _ O
the -X- _ O
keep -X- _ O
rate -X- _ O
to -X- _ O
50 -X- _ O
% -X- _ O
, -X- _ O
the -X- _ O
performance -X- _ O
will -X- _ O
decrease -X- _ O
to -X- _ O
74.21 -X- _ B-MetricValue
on -X- _ O
the -X- _ O
VQA -X- _ B-TaskName
task -X- _ O
, -X- _ O
compared -X- _ O
to -X- _ O
76.12 -X- _ B-MetricValue
of -X- _ O
the -X- _ O
model -X- _ O
without -X- _ O
the -X- _ O
patch -X- _ O
- -X- _ O
selection -X- _ O
layer -X- _ O
. -X- _ O

We -X- _ O
can -X- _ O
control -X- _ O
the -X- _ O
computational -X- _ O
cost -X- _ O
by -X- _ O
fusing -X- _ O
different -X- _ O
numbers -X- _ O
of -X- _ O
inattentive -X- _ O
tokens -X- _ O
. -X- _ O
Therefore -X- _ O
, -X- _ O
we -X- _ O
finetune -X- _ O
TRIPS -X- _ B-MethodName
on -X- _ O
the -X- _ O
VQA -X- _ B-TaskName
and -X- _ O
NLVR -X- _ B-TaskName
tasks -X- _ O
, -X- _ O
which -X- _ O
take -X- _ O
images -X- _ O
with -X- _ O
varying -X- _ O
resolutions -X- _ O
as -X- _ O
input -X- _ O
. -X- _ O

We -X- _ O
report -X- _ O
the -X- _ O
results -X- _ O
in -X- _ O
Table -X- _ O
5 -X- _ O
. -X- _ O
The -X- _ O
experimental -X- _ O
results -X- _ O
show -X- _ O
that -X- _ O
by -X- _ O
increasing -X- _ O
the -X- _ O
input -X- _ O
image -X- _ O
resolution -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
facilitate -X- _ O
the -X- _ O
model -X- _ O
by -X- _ O
taking -X- _ O
more -X- _ O
image -X- _ O
tokens -X- _ O
to -X- _ O
gain -X- _ O
better -X- _ O
performance -X- _ O
. -X- _ O
For -X- _ O
example -X- _ O
, -X- _ O
by -X- _ O
finetuning -X- _ O
TRIPS -X- _ B-MethodName
with -X- _ O
the -X- _ O
images -X- _ O
of -X- _ O
456×456 -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
achieve -X- _ O
the -X- _ O
score -X- _ O
of -X- _ O
76.54 -X- _ B-MetricValue
on -X- _ O
VQA -X- _ B-TaskName
, -X- _ O
outperforming -X- _ O
the -X- _ O
baseline -X- _ O
finetuned -X- _ O
with -X- _ O
images -X- _ O
of -X- _ O
384×384 -X- _ O
yet -X- _ O
keeping -X- _ O
similar -X- _ O
computational -X- _ B-MetricName
complexity -X- _ I-MetricName
. -X- _ O

To -X- _ O
verify -X- _ O
the -X- _ O
effectiveness -X- _ O
of -X- _ O
Text -X- _ B-MethodName
- -X- _ I-MethodName
Relevant -X- _ I-MethodName
Image -X- _ I-MethodName
Patch -X- _ I-MethodName
Selection -X- _ I-MethodName
, -X- _ O
we -X- _ O
first -X- _ O
implement -X- _ O
the -X- _ O
singlestream -X- _ O
model -X- _ O
TRIPS -X- _ O
- -X- _ O
S -X- _ O
as -X- _ O
we -X- _ O
present -X- _ O
in -X- _ O
subsection -X- _ O
3.2 -X- _ O
. -X- _ O
Then -X- _ O
, -X- _ O
we -X- _ O
examine -X- _ O
the -X- _ O
downstream -X- _ O
task -X- _ O
performance -X- _ O
, -X- _ O
computational -X- _ O
complexity -X- _ O
, -X- _ O
and -X- _ O
inference -X- _ O
speed -X- _ O
of -X- _ O
TRIPS -X- _ B-MethodName
and -X- _ O
TRIPS -X- _ O
- -X- _ O
S -X- _ O
( -X- _ O
both -X- _ O
with -X- _ O
and -X- _ O
without -X- _ O
Text -X- _ B-MethodName
- -X- _ I-MethodName
Relevant -X- _ I-MethodName
Image -X- _ I-MethodName
Patch -X- _ I-MethodName
Selection -X- _ I-MethodName
) -X- _ O
. -X- _ O
The -X- _ O
results -X- _ O
are -X- _ O
shown -X- _ O
in -X- _ O
Table -X- _ O
6 -X- _ O
, -X- _ O
and -X- _ O
we -X- _ O
find -X- _ O
that -X- _ O
for -X- _ O
both -X- _ O
TRIPS -X- _ B-MethodName
and -X- _ O
TRIPS -X- _ O
- -X- _ O
S -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
see -X- _ O
a -X- _ O
consistent -X- _ O
improvement -X- _ O
in -X- _ O
the -X- _ O
inference -X- _ B-MetricName
speed -X- _ I-MetricName
and -X- _ O
downstream -X- _ O
task -X- _ O
performance -X- _ O
by -X- _ O
incorporating -X- _ O
the -X- _ O
textrelevant -X- _ O
image -X- _ O
patch -X- _ O
selection -X- _ O
mechanism -X- _ O
. -X- _ O
These -X- _ O
results -X- _ O
suggest -X- _ O
that -X- _ O
the -X- _ O
proposed -X- _ O
image -X- _ O
patch -X- _ O
se- -X- _ O
We -X- _ O
also -X- _ O
perform -X- _ O
ablation -X- _ O
studies -X- _ O
to -X- _ O
investigate -X- _ O
the -X- _ O
effects -X- _ O
of -X- _ O
inattentive -X- _ O
image -X- _ O
token -X- _ O
fusing -X- _ O
and -X- _ O
Text -X- _ O
- -X- _ O
aware -X- _ O
Dynamic -X- _ O
Attention -X- _ O
. -X- _ O
In -X- _ O
Table -X- _ O
7 -X- _ O
, -X- _ O
w -X- _ O
/ -X- _ O
o -X- _ O
ITF -X- _ O
indicates -X- _ O
the -X- _ O
inattentive -X- _ O
tokens -X- _ O
are -X- _ O
directly -X- _ O
discarded -X- _ O
without -X- _ O
fusing -X- _ O
. -X- _ O
As -X- _ O
shown -X- _ O
in -X- _ O
Table -X- _ O
7 -X- _ O
, -X- _ O
fusing -X- _ O
inattentive -X- _ O
tokens -X- _ O
outperforms -X- _ O
the -X- _ O
model -X- _ O
without -X- _ O
inattentive -X- _ O
tokens -X- _ O
. -X- _ O
Although -X- _ O
the -X- _ O
improvement -X- _ O
is -X- _ O
small -X- _ O
, -X- _ O
there -X- _ O
is -X- _ O
no -X- _ O
additional -X- _ O
computational -X- _ O
over -X- _ O
- -X- _ O
head -X- _ O
introduced -X- _ O
. -X- _ O
We -X- _ O
also -X- _ O
verify -X- _ O
the -X- _ O
impacts -X- _ O
of -X- _ O
Text -X- _ O
- -X- _ O
aware -X- _ O
Dynamic -X- _ O
Attention -X- _ O
( -X- _ O
TD -X- _ O
- -X- _ O
ATT -X- _ O
) -X- _ O
. -X- _ O
Specifically -X- _ O
, -X- _ O
w -X- _ O
/ -X- _ O
o -X- _ O
TD -X- _ O
- -X- _ O
ATT -X- _ O
indicates -X- _ O
that -X- _ O
we -X- _ O
remove -X- _ O
the -X- _ O
TD -X- _ O
- -X- _ O
ATT -X- _ O
in -X- _ O
the -X- _ O
patch -X- _ O
- -X- _ O
selection -X- _ O
layer -X- _ O
and -X- _ O
select -X- _ O
the -X- _ O
image -X- _ O
tokens -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
image -X- _ O
[ -X- _ O
CLS -X- _ O
] -X- _ O
token -X- _ O
. -X- _ O
As -X- _ O
shown -X- _ O
in -X- _ O
Table -X- _ O
7 -X- _ O
, -X- _ O
selecting -X- _ O
the -X- _ O
image -X- _ O
patch -X- _ O
tokens -X- _ O
with -X- _ O
the -X- _ O
image -X- _ O
[ -X- _ O
CLS -X- _ O
] -X- _ O
token -X- _ O
without -X- _ O
considering -X- _ O
the -X- _ O
linguistic -X- _ O
context -X- _ O
will -X- _ O
degrade -X- _ O
the -X- _ O
model -X- _ O
's -X- _ O
performance -X- _ O
. -X- _ O
This -X- _ O
result -X- _ O
supports -X- _ O
our -X- _ O
motivation -X- _ O
that -X- _ O
directly -X- _ O
removing -X- _ O
patch -X- _ O
tokens -X- _ O
based -X- _ O
on -X- _ O
image -X- _ O
[ -X- _ O
CLS -X- _ O
] -X- _ O
without -X- _ O
incorporating -X- _ O
the -X- _ O
text -X- _ O
knowledge -X- _ O
is -X- _ O
unsuitable -X- _ O
for -X- _ O
VLP -X- _ O
models -X- _ O
. -X- _ O

The -X- _ O
proposed -X- _ O
TRIPS -X- _ B-MethodName
accelerates -X- _ O
VLP -X- _ B-TaskName
by -X- _ O
a -X- _ O
novel -X- _ O
patch -X- _ O
selection -X- _ O
module -X- _ O
that -X- _ O
selects -X- _ O
the -X- _ O
textconsistent -X- _ O
image -X- _ O
tokens -X- _ O
in -X- _ O
the -X- _ O
vision -X- _ O
backbone -X- _ O
and -X- _ O
preserves -X- _ O
the -X- _ O
attentive -X- _ O
image -X- _ O
tokens -X- _ O
. -X- _ O
To -X- _ O
further -X- _ O
investigate -X- _ O
the -X- _ O
interpretability -X- _ O
of -X- _ O
our -X- _ O
model -X- _ O
, -X- _ O
we -X- _ O
visualize -X- _ O
the -X- _ O
procedure -X- _ O
of -X- _ O
text -X- _ B-MethodName
- -X- _ I-MethodName
relevant -X- _ I-MethodName
image -X- _ I-MethodName
path -X- _ I-MethodName
selection -X- _ I-MethodName
in -X- _ O
Figure -X- _ O
3 -X- _ O
. -X- _ O
It -X- _ O
can -X- _ O
be -X- _ O
seen -X- _ O
that -X- _ O
as -X- _ O
the -X- _ O
network -X- _ O
deepens -X- _ O
, -X- _ O
the -X- _ O
inattentive -X- _ O
tokens -X- _ O
are -X- _ O
gradually -X- _ O
removed -X- _ O
or -X- _ O
fused -X- _ O
, -X- _ O
while -X- _ O
the -X- _ O
text -X- _ O
- -X- _ O
relevant -X- _ O
tokens -X- _ O
are -X- _ O
selected -X- _ O
and -X- _ O
preserved -X- _ O
. -X- _ O
Besides -X- _ O
, -X- _ O
we -X- _ O
present -X- _ O
more -X- _ O
visualization -X- _ O
results -X- _ O
in -X- _ O
Figure -X- _ O
4 -X- _ O
to -X- _ O
show -X- _ O
the -X- _ O
effectiveness -X- _ O
of -X- _ O
the -X- _ O
text -X- _ O
- -X- _ O
relevant -X- _ O
image -X- _ O
patch -X- _ O
selection -X- _ O
module -X- _ O
. -X- _ O
We -X- _ O
take -X- _ O
different -X- _ O
text -X- _ O
words -X- _ O
as -X- _ O
the -X- _ O
input -X- _ O
and -X- _ O
visualize -X- _ O
the -X- _ O
text -X- _ O
- -X- _ O
aware -X- _ O
image -X- _ O
patches -X- _ O
selected -X- _ O
by -X- _ O
the -X- _ O
text -X- _ O
- -X- _ O
relevant -X- _ O
image -X- _ O
patch -X- _ O
selection -X- _ O
module -X- _ O
. -X- _ O
As -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
4 -X- _ O
, -X- _ O
the -X- _ O
selected -X- _ O
image -X- _ O
patches -X- _ O
are -X- _ O
highly -X- _ O
relevant -X- _ O
to -X- _ O
the -X- _ O
query -X- _ O
texts -X- _ O
and -X- _ O
thus -X- _ O
enable -X- _ O
our -X- _ O
model -X- _ O
to -X- _ O
make -X- _ O
a -X- _ O
correct -X- _ O
prediction -X- _ O
. -X- _ O

We -X- _ O
have -X- _ O
presented -X- _ O
TRIPS -X- _ B-MethodName
, -X- _ O
an -X- _ O
efficient -X- _ O
VLP -X- _ O
model -X- _ O
with -X- _ O
Text -X- _ B-MethodName
- -X- _ I-MethodName
Relevant -X- _ I-MethodName
Image -X- _ I-MethodName
Patch -X- _ I-MethodName
Selection -X- _ I-MethodName
to -X- _ O
progressively -X- _ O
reduce -X- _ O
the -X- _ O
redundant -X- _ O
image -X- _ O
tokens -X- _ O
with -X- _ O
text -X- _ O
guidance -X- _ O
. -X- _ O
TRIPS -X- _ B-MethodName
introduces -X- _ O
a -X- _ O
novel -X- _ O
patch -X- _ O
selection -X- _ O
module -X- _ O
to -X- _ O
select -X- _ O
the -X- _ O
text -X- _ O
- -X- _ O
consistent -X- _ O
image -X- _ O
tokens -X- _ O
in -X- _ O
the -X- _ O
vision -X- _ O
backbone -X- _ O
, -X- _ O
which -X- _ O
preserve -X- _ O
the -X- _ O
attentive -X- _ O
image -X- _ O
tokens -X- _ O
with -X- _ O
text -X- _ O
guidance -X- _ O
and -X- _ O
fuses -X- _ O
the -X- _ O
inattentive -X- _ O
tokens -X- _ O
into -X- _ O
one -X- _ O
token -X- _ O
by -X- _ O
dynamically -X- _ O
computing -X- _ O
text -X- _ O
- -X- _ O
dependent -X- _ O
visual -X- _ O
attention -X- _ O
in -X- _ O
an -X- _ O
end -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
end -X- _ O
way -X- _ O
. -X- _ O
The -X- _ O
experiment -X- _ O
shows -X- _ O
our -X- _ O
method -X- _ O
not -X- _ O
only -X- _ O
decreases -X- _ O
the -X- _ O
computation -X- _ B-MetricName
cost -X- _ I-MetricName
of -X- _ O
VLP -X- _ O
but -X- _ O
also -X- _ O
improves -X- _ O
the -X- _ O
efficiency -X- _ O
of -X- _ O
cross -X- _ O
- -X- _ O
modal -X- _ O
fusion -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
reduction -X- _ O
of -X- _ O
visual -X- _ O
sequences -X- _ O
, -X- _ O
while -X- _ O
keeping -X- _ O
or -X- _ O
even -X- _ O
improving -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
downstream -X- _ O
image -X- _ O
- -X- _ O
text -X- _ O
tasks -X- _ O
. -X- _ O

Despite -X- _ O
the -X- _ O
effectiveness -X- _ O
and -X- _ O
efficiency -X- _ O
of -X- _ O
TRIPS -X- _ B-MethodName
across -X- _ O
a -X- _ O
wide -X- _ O
range -X- _ O
of -X- _ O
downstream -X- _ O
image -X- _ O
- -X- _ O
text -X- _ O
tasks -X- _ O
, -X- _ O
our -X- _ O
model -X- _ O
still -X- _ O
has -X- _ O
several -X- _ O
limitations -X- _ O
. -X- _ O
First -X- _ O
, -X- _ O
in -X- _ O
our -X- _ O
current -X- _ O
settings -X- _ O
, -X- _ O
we -X- _ O
pre -X- _ O
- -X- _ O
train -X- _ O
TRIPS -X- _ B-MethodName
with -X- _ O
only -X- _ O
4 -X- _ O
M -X- _ O
image -X- _ O
- -X- _ O
text -X- _ O
pairs -X- _ O
. -X- _ O
It -X- _ O
is -X- _ O
unclear -X- _ O
how -X- _ O
well -X- _ O
the -X- _ O
performance -X- _ O
will -X- _ O
be -X- _ O
if -X- _ O
we -X- _ O
pre -X- _ O
- -X- _ O
train -X- _ O
TRIPS -X- _ B-MethodName
on -X- _ O
a -X- _ O
larger -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
dataset -X- _ O
with -X- _ O
other -X- _ O
available -X- _ O
data -X- _ O
types -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
text -X- _ O
- -X- _ O
only -X- _ O
, -X- _ O
image -X- _ O
- -X- _ O
only -X- _ O
data -X- _ O
, -X- _ O
and -X- _ O
some -X- _ O
labeled -X- _ O
data -X- _ O
. -X- _ O
Second -X- _ O
, -X- _ O
we -X- _ O
provide -X- _ O
a -X- _ O
novel -X- _ O
perspective -X- _ O
for -X- _ O
the -X- _ O
efficient -X- _ O
training -X- _ O
and -X- _ O
inference -X- _ O
of -X- _ O
VLP -X- _ O
models -X- _ O
, -X- _ O
which -X- _ O
reduces -X- _ O
the -X- _ O
visual -X- _ O
sequence -X- _ O
progressively -X- _ O
with -X- _ O
a -X- _ O
text -X- _ O
- -X- _ O
guided -X- _ O
patch -X- _ O
- -X- _ O
selection -X- _ O
layer -X- _ O
in -X- _ O
the -X- _ O
visual -X- _ O
backbone -X- _ O
. -X- _ O
In -X- _ O
our -X- _ O
method -X- _ O
, -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
selected -X- _ O
text -X- _ O
- -X- _ O
aware -X- _ O
image -X- _ O
patches -X- _ O
in -X- _ O
each -X- _ O
patchselection -X- _ O
layer -X- _ O
is -X- _ O
fixed -X- _ O
, -X- _ O
and -X- _ O
there -X- _ O
can -X- _ O
be -X- _ O
a -X- _ O
more -X- _ O
ingenious -X- _ O
technical -X- _ O
design -X- _ O
that -X- _ O
can -X- _ O
dynamically -X- _ O
select -X- _ O
different -X- _ O
numbers -X- _ O
of -X- _ O
image -X- _ O
patches -X- _ O
. -X- _ O

LXMERT -X- _ O
( -X- _ O
Tan -X- _ O
and -X- _ O
Bansal -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
: -X- _ O
is -X- _ O
the -X- _ O
first -X- _ O
twostream -X- _ O
region -X- _ O
- -X- _ O
based -X- _ O
VLP -X- _ O
model -X- _ O
, -X- _ O
which -X- _ O
consists -X- _ O
of -X- _ O
an -X- _ O
object -X- _ O
relationship -X- _ O
encoder -X- _ O
, -X- _ O
a -X- _ O
language -X- _ O
encoder -X- _ O
and -X- _ O
a -X- _ O
cross -X- _ O
- -X- _ O
modality -X- _ O
encoder -X- _ O
. -X- _ O

E2E -X- _ B-MethodName
- -X- _ I-MethodName
VLP -X- _ I-MethodName
( -X- _ O
Xu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021a -X- _ O
) -X- _ O
: -X- _ O
proposes -X- _ O
the -X- _ O
first -X- _ O
endto -X- _ O
- -X- _ O
end -X- _ O
VLP -X- _ O
method -X- _ O
for -X- _ O
both -X- _ O
V+L -X- _ O
understanding -X- _ O
and -X- _ O
generation -X- _ O
, -X- _ O
with -X- _ O
a -X- _ O
unified -X- _ O
Transformer -X- _ O
encoderdecoder -X- _ O
architecture -X- _ O
. -X- _ O

VILT -X- _ B-MethodName
( -X- _ O
Kim -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2021 -X- _ O
) -X- _ O
: -X- _ O
adopts -X- _ O
linear -X- _ O
projection -X- _ O
and -X- _ O
word -X- _ O
embedding -X- _ O
as -X- _ O
the -X- _ O
visual -X- _ O
and -X- _ O
textual -X- _ O
encoders -X- _ O
, -X- _ O
and -X- _ O
uses -X- _ O
the -X- _ O
visual -X- _ O
transformer -X- _ O
as -X- _ O
the -X- _ O
crossmodal -X- _ O
encoder -X- _ O
to -X- _ O
align -X- _ O
and -X- _ O
fuse -X- _ O
the -X- _ O
features -X- _ O
of -X- _ O
both -X- _ O
modalities -X- _ O
in -X- _ O
an -X- _ O
end -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
end -X- _ O
manner -X- _ O
. -X- _ O

OSCAR -X- _ B-MethodName
: -X- _ O
proposes -X- _ O
to -X- _ O
use -X- _ O
object -X- _ O
tags -X- _ O
detected -X- _ O
in -X- _ O
images -X- _ O
as -X- _ O
anchor -X- _ O
points -X- _ O
to -X- _ O
the -X- _ O
learning -X- _ O
of -X- _ O
cross -X- _ O
- -X- _ O
modal -X- _ O
alignments -X- _ O
. -X- _ O

VinVL -X- _ B-MethodName
: -X- _ O
pre -X- _ O
- -X- _ O
trains -X- _ O
a -X- _ O
largescale -X- _ O
object -X- _ O
- -X- _ O
attribute -X- _ O
detection -X- _ O
model -X- _ O
with -X- _ O
much -X- _ O
larger -X- _ O
amounts -X- _ O
of -X- _ O
supervised -X- _ O
data -X- _ O
to -X- _ O
extract -X- _ O
better -X- _ O
region -X- _ O
- -X- _ O
based -X- _ O
visual -X- _ O
features -X- _ O
. -X- _ O

ALBEF -X- _ B-MethodName
: -X- _ O
adopts -X- _ O
a -X- _ O
contrastive -X- _ O
loss -X- _ O
to -X- _ O
align -X- _ O
the -X- _ O
image -X- _ O
and -X- _ O
text -X- _ O
representations -X- _ O
, -X- _ O
then -X- _ O
fuses -X- _ O
them -X- _ O
through -X- _ O
cross -X- _ O
- -X- _ O
modal -X- _ O
attention -X- _ O
in -X- _ O
an -X- _ O
end -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
end -X- _ O
manner -X- _ O
. -X- _ O

UNITER -X- _ B-MethodName
( -X- _ O
Chen -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
: -X- _ O
proposes -X- _ O
a -X- _ O
new -X- _ O
wordregion -X- _ O
alignment -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
task -X- _ O
via -X- _ O
the -X- _ O
use -X- _ O
of -X- _ O
optimal -X- _ O
transport -X- _ O
to -X- _ O
help -X- _ O
fine -X- _ O
- -X- _ O
grained -X- _ O
alignment -X- _ O
between -X- _ O
words -X- _ O
and -X- _ O
image -X- _ O
regions -X- _ O
. -X- _ O

ViLBERT -X- _ O
( -X- _ O
Lu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
: -X- _ O
proposes -X- _ O
one -X- _ O
of -X- _ O
the -X- _ O
first -X- _ O
work -X- _ O
that -X- _ O
extend -X- _ O
the -X- _ O
BERT -X- _ O
architecture -X- _ O
to -X- _ O
a -X- _ O
multi -X- _ O
- -X- _ O
modal -X- _ O
two -X- _ O
- -X- _ O
stream -X- _ O
region -X- _ O
- -X- _ O
based -X- _ O
VLP -X- _ O
model -X- _ O
. -X- _ O

We -X- _ O
evaluate -X- _ O
TRIPS -X- _ B-MethodName
on -X- _ O
the -X- _ O
three -X- _ O
downstream -X- _ O
visionlanguage -X- _ O
tasks -X- _ O
. -X- _ O
The -X- _ O
hyperparameters -X- _ O
that -X- _ O
we -X- _ O
use -X- _ O
for -X- _ O
finetuning -X- _ O
on -X- _ O
the -X- _ O
downstream -X- _ O
tasks -X- _ O
are -X- _ O
listed -X- _ O
in -X- _ O
Table -X- _ O
8 -X- _ O
. -X- _ O
Following -X- _ O
VQA -X- _ B-TaskName
. -X- _ O
The -X- _ O
VQA -X- _ B-TaskName
task -X- _ O
( -X- _ O
Agrawal -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
requires -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
answer -X- _ O
natural -X- _ O
language -X- _ O
questions -X- _ O
given -X- _ O
an -X- _ O
image -X- _ O
. -X- _ O
We -X- _ O
conduct -X- _ O
experiment -X- _ O
on -X- _ O
the -X- _ O
VQA2.0 -X- _ B-DatasetName
dataset -X- _ O
( -X- _ O
Agrawal -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
, -X- _ O
which -X- _ O
contains -X- _ O
83k -X- _ O
/ -X- _ O
41k -X- _ O
/ -X- _ O
81k -X- _ O
images -X- _ O
for -X- _ O
training -X- _ O
/ -X- _ O
validation -X- _ O
/ -X- _ O
test -X- _ O
. -X- _ O
Following -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
both -X- _ O
training -X- _ O
and -X- _ O
validation -X- _ O
splits -X- _ O
for -X- _ O
training -X- _ O
, -X- _ O
and -X- _ O
incorporate -X- _ O
additional -X- _ O
training -X- _ O
data -X- _ O
from -X- _ O
Visual -X- _ B-DatasetName
Genome -X- _ I-DatasetName
( -X- _ O
Suhr -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
. -X- _ O

Image -X- _ B-TaskName
- -X- _ I-TaskName
Text -X- _ I-TaskName
Retrieval -X- _ I-TaskName
. -X- _ O
We -X- _ O
conduct -X- _ O
experiments -X- _ O
for -X- _ O
both -X- _ O
image -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
text -X- _ O
retrieval -X- _ O
( -X- _ O
TR -X- _ O
) -X- _ O
and -X- _ O
textto -X- _ O
- -X- _ O
image -X- _ O
retrieval -X- _ O
( -X- _ O
IR -X- _ O
) -X- _ O
on -X- _ O
COCO -X- _ B-DatasetName
( -X- _ O
Lin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2014 -X- _ O
) -X- _ O
andFlickr30 -X- _ B-DatasetName
K -X- _ I-DatasetName
( -X- _ O
Plummer -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
datasets -X- _ O
. -X- _ O
We -X- _ O
take -X- _ O
the -X- _ O
widely -X- _ O
- -X- _ O
used -X- _ O
Karpathy -X- _ O
split -X- _ O
( -X- _ O
Karpathy -X- _ O
and -X- _ O
Fei -X- _ O
- -X- _ O
Fei -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
for -X- _ O
both -X- _ O
COCO -X- _ B-DatasetName
and -X- _ O
Flickr30K. -X- _ B-DatasetName
COCO -X- _ B-DatasetName
contains -X- _ O
113k -X- _ O
/ -X- _ O
5k -X- _ O
/ -X- _ O
5k -X- _ O
images -X- _ O
for -X- _ O
train -X- _ O
/ -X- _ O
validation -X- _ O
/ -X- _ O
test -X- _ O
, -X- _ O
and -X- _ O
Flickr30 -X- _ B-DatasetName
K -X- _ I-DatasetName
contains -X- _ O
29k -X- _ O
/ -X- _ O
1k -X- _ O
/ -X- _ O
1k -X- _ O
images -X- _ O
for -X- _ O
train -X- _ O
/ -X- _ O
validation -X- _ O
/ -X- _ O
test -X- _ O
. -X- _ O

NLVR2 -X- _ B-TaskName
. -X- _ O
The -X- _ O
NLVR2 -X- _ B-TaskName
( -X- _ O
Suhr -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
task -X- _ O
requires -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
predict -X- _ O
whether -X- _ O
a -X- _ O
sentence -X- _ O
. -X- _ O