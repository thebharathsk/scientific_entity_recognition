{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bharathsk/anaconda3/envs/11711/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp_trf = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "nlp_bg = spacy.load(\"en_core_web_lg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pass text through model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I love paris. There are 10245565 parisians each of whom weigh 61.08348 kgs on average. The average radius is 143.8957349182391 cm.'\n",
    " \n",
    "doc_trf = nlp_trf(text)\n",
    "doc_bg = nlp_bg(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "love\n",
      "paris\n",
      ".\n",
      "There\n",
      "are\n",
      "10245565\n",
      "parisians\n",
      "each\n",
      "of\n",
      "whom\n",
      "weigh\n",
      "61.08348\n",
      "kgs\n",
      "on\n",
      "average\n",
      ".\n",
      "The\n",
      "average\n",
      "radius\n",
      "is\n",
      "143.8957349182391\n",
      "cm\n",
      ".\n",
      "#############\n",
      "I\n",
      "love\n",
      "paris\n",
      ".\n",
      "There\n",
      "are\n",
      "10245565\n",
      "parisians\n",
      "each\n",
      "of\n",
      "whom\n",
      "weigh\n",
      "61.08348\n",
      "kgs\n",
      "on\n",
      "average\n",
      ".\n",
      "The\n",
      "average\n",
      "radius\n",
      "is\n",
      "143.8957349182391\n",
      "cm\n",
      ".\n",
      "24 24\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "list_trf = []\n",
    "list_bg = []\n",
    "\n",
    "#print tokens\n",
    "for token in doc_trf:\n",
    "    print(token.text)\n",
    "    list_trf.append(token.text)\n",
    "\n",
    "print('#############')\n",
    "\n",
    "#print tokens\n",
    "for token in doc_bg:\n",
    "    print(token.text)\n",
    "    list_bg.append(token.text)\n",
    "    \n",
    "print(len(list_trf), len(list_bg))\n",
    "\n",
    "#check if the two lists are same\n",
    "print(list_trf == list_bg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print entities detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paris 7 12 NORP\n",
      "10245565 24 32 CARDINAL\n",
      "parisians 33 42 NORP\n",
      "61.08348 kgs 62 74 QUANTITY\n",
      "143.8957349182391 cm 109 129 QUANTITY\n"
     ]
    }
   ],
   "source": [
    "for ent in doc_trf.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-large-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-large-NER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pass example input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = \"My name is Wolfgang Schaffer and I live in Berlin\"\n",
    "\n",
    "ner_results = nlp(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'B-PER', 'score': 0.99859005, 'index': 4, 'word': 'Wolfgang', 'start': 11, 'end': 19}\n",
      "Wolfgang B-PER\n",
      "{'entity': 'I-PER', 'score': 0.99945515, 'index': 5, 'word': 'Sc', 'start': 20, 'end': 22}\n",
      "Sc I-PER\n",
      "{'entity': 'I-PER', 'score': 0.9952127, 'index': 6, 'word': '##ha', 'start': 22, 'end': 24}\n",
      "##ha I-PER\n",
      "{'entity': 'I-PER', 'score': 0.9401913, 'index': 7, 'word': '##ffer', 'start': 24, 'end': 28}\n",
      "##ffer I-PER\n",
      "{'entity': 'B-LOC', 'score': 0.99870574, 'index': 12, 'word': 'Berlin', 'start': 43, 'end': 49}\n",
      "Berlin B-LOC\n"
     ]
    }
   ],
   "source": [
    "#print tokens\n",
    "for token in ner_results:\n",
    "    print(token)\n",
    "    print(token['word'], token['entity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-large-NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.modeling_bert.BertForTokenClassification'>\n"
     ]
    }
   ],
   "source": [
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.linear.Linear'>\n"
     ]
    }
   ],
   "source": [
    "print(type(model.classifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=1024, out_features=9, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "model.classifier = nn.Linear(model.classifier.in_features, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=1024, out_features=25, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(model.config.num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "11711",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
