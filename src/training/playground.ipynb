{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_trf\",  disable=[\"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n",
    "\n",
    "# Importing as module.\n",
    "import en_core_web_trf\n",
    "nlp = en_core_web_trf.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pass text through model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"In this paper , we improve the fine - tuning based approaches by proposing BERT : Bidirectional Encoder Representations from Transformers . BERT alleviates the previously mentioned unidirectionality constraint by using a “ masked language model ” ( MLM ) pre - training objective , inspired by the Cloze task ( Taylor , 1953 ) .\"\n",
    "\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In\n",
      "this\n",
      "paper\n",
      ",\n",
      "we\n",
      "improve\n",
      "the\n",
      "fine\n",
      "-\n",
      "tuning\n",
      "based\n",
      "approaches\n",
      "by\n",
      "proposing\n",
      "BERT\n",
      ":\n",
      "Bidirectional\n",
      "Encoder\n",
      "Representations\n",
      "from\n",
      "Transformers\n",
      ".\n",
      "BERT\n",
      "alleviates\n",
      "the\n",
      "previously\n",
      "mentioned\n",
      "unidirectionality\n",
      "constraint\n",
      "by\n",
      "using\n",
      "a\n",
      "“\n",
      "masked\n",
      "language\n",
      "model\n",
      "”\n",
      "(\n",
      "MLM\n",
      ")\n",
      "pre\n",
      "-\n",
      "training\n",
      "objective\n",
      ",\n",
      "inspired\n",
      "by\n",
      "the\n",
      "Cloze\n",
      "task\n",
      "(\n",
      "Taylor\n",
      ",\n",
      "1953\n",
      ")\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "#print tokens\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print entities detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloze 298 303 PERSON\n",
      "Taylor 311 317 PERSON\n",
      "1953 320 324 DATE\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "11711",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
