Jordan was in charge of taking the food on the camping trip and left all the food at home . Jordan felt horrible that he let his friends down on the camping trip .

We first introduce our view to unify RSE problems and then discuss how TAGPRIME approaches this problem under a unified framework of sequence tagging model with priming . 1 , r c 2 , ... , r c l ] towards the condition c , where r c i ∈ A and A is the set of all possible relationships or attributes . Many NLP tasks can be formulated as an RSE task . We showcase how this formulation can be applied to event extraction , entity relation extraction , and taskoriented semantic parsing below .

In this section , we re - introduce premises to our analysis to evaluate a set of hypotheses regarding latent , class - specific annotator heuristics . If annotators do employ class - specific heuristics , we should expect the semantic contents , ϕ , of a given hypothesis , h ∈ H , to be influenced not only by the semantic contents of its associated premise , p ∈ P , but also by the target class , c ∈ C.

Using separate scalar mixes for source and target tokens allows us to explore the cross - formalism encoding of role semantics by mBERT in detail . For both English and German role labeling , the probe 's layer utilization drastically differs for predicate and 4 Echoing the recent findings on mBERT 's multilingual capacity ( Pires et al . , 2019;Kondratyuk and Straka , 2019 [ 6.16 ] xnli [ 6.28 ] en Layer [ 4.61 ] [ 5.2 ] [ 5.09 ] [ 5.75 ] [ 6.01 ] [ 5.99 ] [ 5.18 ] [ 5.24 ] [ 5.13 ] [ 6.12 ] [ 6.06 ] [ 5.75 ] [ 6.15 ] de argument tokens . While the argument representation role * tgt mostly focuses on the same layers as the dependency parsing probe , the layer utilization of the predicates role * src is affected by the chosen formalism . In English , PropBank predicate token mixing weights emphasize the same layers as dependency parsing -in line with the previously published results . However , the probes for VerbNet and FrameNet predicates ( role.vn src and role.fn src ) utilize the layers associated with ttype and lex.unit that contain lexical information . Coupled with the fact that both VerbNet and FrameNet assign semantic roles based on lexical - semantic predicate groupings ( frames in FrameNet and verb classes in VerbNet ) , this suggests that the lower layers of mBERT implicitly encode predicate sense information ; moreover , sense encoding for VerbNet utilizes deeper layers of the model associated with syntax , in line with Verb - Net 's predicate classification strategy . This finding confirms that the formalism can indeed have linguistically meaningful effects on probing results .

We show part of the analysis results below due to the limited space , and more analysis can be found in Appendix § A and § D .

where θ q and θ k are model parameters of f q and f k , respectively . m is the momentum coefficient .

Sequential vs. parallel . In the main experiments , we used a ' sequential ' ( seq ) structure where our tiny - attention modules are placed between the pretrained attention and feed - forward net . Another option is to put the tiny - attention module in ' parallel ' ( para ) to the original attention layer as in He et al . ( 2021 ) .

To evaluate how well our generation model achieves target difficulties , we take 15 unseen students and generate 30 questions for each of 9 input difficulties ( 0.1 - 0.9 ) . We then use LM - KT ( a wellcalibrated proxy for true difficulty ) to measure the difficulty of these generated questions for each student . Figure 3 shows that we are able to achieve fine - grained control over target difficulty for both Spanish and French students , with an average Root - Mean Squared Error ( RMSE ) of .052 across all students and target difficulties . Adding a sampling penalty ( Keskar et al . , 2019 ) increases the variance in difficulty ( RMSE .062 ) in exchange for more novel and diverse questions , as discussed next .

Our primary experiments are cross - KB and focus on English datasets . We train models to link to one KB during training ( viz . Wikidata ) , and evaluate them for their ability to link to an unseen KB ( viz . the TAC - KBP Knowledge Base ) . These experiments reveal that our model with attributeseparators and the two generalization schemes are 12 - 14 % more accurate than the baseline zero - shot models . Ablation studies reveal that all components individually contribute to this improvement , but combining all of them yields the most accurate models .

To detect realis events in our stories , we train a tagger ( using BERT - base ; Devlin et al . , 2019 ) on the annotated corpus by Sims et al . ( 2019 ) . This corpus contains 8k realis events annotated by experts in sentences drawn from 100 English books . With development and test F 1 scores of 83.7 % and 75.8 % , respectively , our event tagger slightly outperforms the best performing model in Sims et al . ( 2019 ) , which reached 73.9 % F 1 . In our analyses , we use our tagger to detect the number of realis event mentions .

Besides the aforementioned three major directions , Artetxe and Schwenk ( 2019 ) train a multilingual sentence encoder on 93 languages . Their stacked BiLSTM encoder is trained by first generating embedding of a source sentence and then decoding the embedding into the target sentence in other languages .

OK . Here is a description of the cartoon followed by the five choices .

So far , we have analyzed errors in predictions for all the answerable questions . Next , we focus our attention on questions with multi - span and multispeaker answers . Within the multi - span split , we calculate the fraction of incorrect predictions ( as per exact match ) that are multi - span , denoted by multi - span preds ( % ) . Similarly , for multi - speaker split , we calculate the fraction of incorrect predictions ( as per exact match ) that are multi - speaker in nature , denoted by multi - speaker preds ( % ) . Further , we compare the list of speakers in the reference and predicted spans using Jaccard similarity ( IoU ) denoted as speaker IoU. We compute and report these metrics for all the aforementioned models in Table 10 .

XNLI Table 2 shows results on XNLI measured by accuracy . Devlin et al . ( 2019 ) only provide results on a few languages 6 , so we use the mBERT results from as our baseline for zeroshot cross - lingual transfer , and Wu and Dredze ( 2019 ) for translate - train . Our best model , trained with 2 M parallel sentences per language improves over mBERT baseline by 4.7 % for zero - shot transfer , and 3.2 % for translate - train .

Step We show the process of Step 1 and Step 2 in Figure 3 ( a ) and Figure 3 ( b ) , respectively . Overall , the collected code - code corpus contains 23.4 million pairs . We provide a more detailed description on building code - code corpus , involved hyperparameters and detailed cross - language statistics of code - code pairs in Appendix B , C and D .

In order to determine which questions are appropriate for a given sentence , we examine the dependency structure of the original sentence and check if it contains the required part to be removed before parameterizing the realization . The generated questions are then filtered to remove any question for which the answer is composed of a single stopword . Table 1 shows the number of questions generated for each dataset . An example of a synthetic question is shown in Figure 3 .

Concretely , these queries are encountered during scientific literature review ( Voorhees et al . , 2021 ; Dasigi et al . , 2021 ) , when looking for news and background information ( Soboroff et al . , 2018 ) , during argument retrieval , ( Bondarenko et al . , 2021 ) or in the legal context for case law retrieval ( Locke and Zuccon , 2018 ) .

, and d k is set to 300 . The model is trained using the InfoNCE loss :

Decoder . The goal of the decoder is to autoregressively generate the translated column header Y = hy 1 , . . . , y m i. Specifically , taking X 0 and the representation of previously output token as input , the decoder predicts the translation token by token until an ending signal hendi is generated . Similar to the encoder , a special token htgti which indicates the target language is added at the front to guide the prediction of the target language .

2 . Replace the k random positions with negative samples :

Anchor tasks . We employ two analytical tools from the original layer probing setup . Mixing weight plotting compares layer utilization among tasks by visually aligning the respective learned weight distributions transformed via a softmax function . Layer center - of - gravity is used as a summary statistic for a task 's layer utilization . While the distribution of mixing weights along the layers allows us to estimate the order in which information is processed during encoding , it does n't allow to directly assess the similarity between the layer utilization of the probing tasks . Tenney et al . ( 2019a ) have demonstrated that the order in which linguistic information is stored in BERT mirrors the traditional NLP pipeline . A prominent property of the NLP pipelines is their use of low - level features to predict downstream phenomena . In the context of layer probing , probing tasks can be seen as end - to - end feature extractors . Following this intuition , we define two groups of probing tasks : target tasks -the main tasks under investigation , and anchor tasks -a set of related tasks that serve as a basis for qualitative comparison between the targets . The softmax transformation of the scalar mixing weights allows to treat them as probability distributions : the higher the mixing weight of a layer , the more likely the probe is to utilize information from this layer during prediction . We use Kullback - Leibler divergence to compare target tasks ( e.g. role labeling in different formalisms ) in terms of their similarity to lowerlevel anchor tasks ( e.g. dependency relation and lemma ) . Note that the notion of anchor task is contextual : the same task can serve as a target and as an anchor , depending on the focus of the study . jections in the background , but do not investigate the differences between the learned projections .

tional words . Then , we would like to evaluate the accuracy of different losses on different types of syntactic multi - modality . However , in real - world corpora , the different types are usually entangled , making it difficult to control and analyse one aspect without changing the other . Thus , we construct synthesized datasets based on phrase structure rules ( Chomsky , 1959 ) to manually control the degree of syntactic multi - modality in different aspects , and evaluate the performance of different existing techniques .

The first iteration for the foundation of the MAS - SIVE dataset was the NLU Evaluation Benchmarking Dataset , with 25k utterances across 18 domains ( Liu et al . , 2019a ) . The authors updated the dataset and added audio and ASR transcriptions in the release of the Spoken Language Understanding Resource Package ( SLURP ) ( Bastianelli et al . , 2020 ) , allowing for full end - to - end Spoken Language Understanding ( SLU ) evaluation similar to the Fluent Speech Commands dataset ( Lugosch et al . , 2019 ) and Chinese Audio - Textual Spoken Language Understanding ( CATSLU ) ( Zhu et al . , 2019 ) . An overview of selected existing NLU datasets can be seen in Table 1 .

• STS : Semantic Textual Similarity ( Cer et al . , 2017 ) . The tasks is to predict how semantically similar two sentences are on a 1 - 5 scale . The dataset contains 5.8k train examples drawn from new headlines , video and image captions , and natural language inference data .

• CoLA : Corpus of Linguistic Acceptability ( Warstadt et al . , 2018 ) . The task is to determine whether a given sentence is grammatical or not . The dataset contains 8.5k train examples from books and journal articles on linguistic theory .

We would like to thank Nontawat Charoenphakdee and anonymous reviewers for helpful comments . Also , the first author wishes to thank the support from Anandamahidol Foundation , Thailand .

Case study In order to understand the performance gain of our model over sub - word based BERT on cross - domain tasks , we look into the cases where BERT makes incorrect predictions . We found that many of these cases contain excessively fragmented words . Table 5 shows two examples from the NCBI - disease NER task . The word fragility in case 1 is segmented into f , # # rag , # # ility , and the word rupture in case 2 is segmented into r , # # up , # # ture . We think these tokenization results

Based on this , we model a gating variable g as :

Our work is a first step toward showing that sequence - based models combined with domain knowledge , such as pre - trained LMs , can be leveraged for adaptive learning tasks . We show how to use modern LMs to generate novel reversetranslation questions that achieve a target difficulty , allowing adaptive education methods to expand beyond limited question pools . Limitations of our approach include the compute constraints of large LMs and training data availability . More detailed student data will be crucial to future model development . For instance , while most publicly available education datasets do not include the full student responses ( e.g. full translation response in Duolingo ) , such information could significantly improve the performance of our LM - KT model . Other future directions include exploring non - language domains , such as math or logic exercises , and controlling for auxiliary objectives such as question topic .

In this feasibility study , we assessed the effectiveness of word clouds as visual explanations to reveal the behavior of CNN features . We trained CNN models using small training datasets and evaluated the quality of CNN features based on responses from MTurk workers to the feature word clouds .

• ALBEF mscoco is the only model to predict ISA ( 99.4 % ) on the caption with coherentbut mostly textual -indicators . It fails on foil prediction , still relying on the same textual indicators , and on the visual side focuses on counter - evidence regions , erroneously taking them as positive support for ISA .

Compared to the unsupervised approaches , MB - RPG is also superior especially in terms of the iBLEU and BLEU metrics . The main reason might be that the word editing or sampling attempts proposed in the unsupervised baselines yield less desirable target paraphrases and thus makes the performance of the model trained under the unsupervised data fall far below our method and various supervised baselines . The inferior performance of the unsupervised methods has also been empirically evaluated and discussed by Niu et al . ( 2020 ) .

Paraphraser We finetune the paraphraser using a batch size of 1 , 024 tokens for 5 , 000 iterations ( 500 for warm - up ) , with a learning rate of 3e − 5 using ADAM . We apply label smoothing with a probability of 0.1 . Semantic Parser Our semantic parser is a neural sequence - to - sequence model with dot - product attention ( Luong et al . , 2015 ) , using a BERT Base encoder and an LSTM decoder , augmented with copying mechanism . The size of the LSTM hidden state is 256 . We decode programs using beam search with a beam size of 5 . Following , we remove hypotheses from the beam that leads to error executions .

set to 4 , given our minimum requirements for fusion labels . The final evaluated score reported was an average score over 20 different trained models . This is due to BART being highly sensitive to the ordering of the input sentences . Both baseline models were trained using the train / test splits that were reported in Thadani and McKeown ( 2013 )

We propose a simple method to align multilingual contextual embeddings as a postpretraining step for improved cross - lingual transferability of the pretrained language models . Using parallel data , our method aligns embeddings on the word level through the recently proposed Translation Language Modeling objective as well as on the sentence level via contrastive learning and random input shuffling . We also perform sentence - level code - switching with English when finetuning on downstream tasks . On XNLI , our best model ( initialized from mBERT ) improves over mBERT by 4.7 % in the zero - shot setting and achieves comparable result to XLM for translate - train while using less than 18 % of the same parallel data and 31 % fewer model parameters . On MLQA , our model outperforms XLM - R Base , which has 57 % more parameters than ours .

Q : who was last person to be executed in us A : Ruben Cardenas Ramirez Billy Bailey : He became only the third person to be hanged in the United States since 1965 ( the previous two were Charles Rodman Campbell and Westley Allan Dodd , both in Washington ) and the first person hanged in Delaware in 50 years . As of 2018 , he remains the last person to be executed by hanging in the United States .

Dataset Statistics Table 5 shows the detailed statistics of HyperRED , such as the number of unique facts and entities , as well as the average number of words in each sentence . dataset , we use the Wikidata which has 594,088 hyper - relational facts and introductions from English Wikipedia which has 4,650,000 articles .

Subsequently , the new parameters are evaluated on their ability to adapt to the second task T 2 by computing the loss of the predictions made by the model g θ ′ . By backpropagating through this entire process ( i.e. computing the gradients w.r.t . to θ ) , the original parameters of the model are optimized :

Finally , designing appropriate user studies to evaluate our method is a complex yet critical next step to determine its suitability in a real - world education setting . Our techniques allows control for individual student difficulty , but it leaves open the question of optimal curriculum design using difficulty - directed question generation .

diverse beam search is a diversity - promoting variant of beam search ( Vijayakumar et al . , 2018 ) . We experiment with different numbers of beam groups for diverse beam search : 5 for DBS and 10 for DBS+ . Sample is represented by two widelyadopted strong stochastic sampling methods , nucleus sampling ( NCLS ) ( Holtzman et al . , 2020 ) and typical sampling ( TYP ) ( Meister et al . , 2022a ) .

i=1 are initial embeddings of our input sequence , and the relational matrix R is induced from the input graph , where r ij is a learned embedding according to the type of edge that x i and x j hold in the directed input graph . The following section will describe the set of relations our model uses to encode a target header concatenated with its context .

Physician - annotators were asked to write a definitely true , maybe true , and definitely false set of hypotheses for each premise , corresponding to entailment , neutral and contradiction labels , respectively . The resulting dataset has cardinality : n train = 11232 ; n dev = 1395 ; n test = 1422 .

In this work , we first collect and annotate an E2E stance dataset , SEESAW 2 ( Stance between Entity and Entity Supplemented with Article - level vieWpoint ) , based on 609 news articles crawled from AllSides . 3 SEESAW contains 10,619 stance triplets annotated at the sentence level , drawn from 203 political news stories , with each " story " consisting of 3 articles by media of different ideological leanings , as collected , coded , and aligned by AllSides . Our entities cover people , organizations , events , topics , and other objects .

2 . Probable Cause Heuristic : a premisehypothesis pair satisfies this heuristic if : ( 1 ) the premise contains one or more MeSH entities belonging to high - level categories C ( diseases ) , D ( chemicals and drugs ) , E ( analytical , diagnostic and therapeutic techniques , and equipment ) or F ( psychiatry and psychology ) ; and ( 2 ) the hypothesis contains one or more MeSH entities that can be interpreted as providing a plausible causal or behavioral explanation for the condition , finding , or event described in the premise ( e.g. , smoking , substance - related disorders , mental disorders , alcoholism , homelessness , obesity ) .

Most probing studies use linguistics as a theoretical scaffolding and operate on a task level . However , there often exist multiple ways to represent the same linguistic phenomenon : for example , English dependency syntax can be encoded using a variety of formalisms , incl . Universal ( Schuster and Manning , 2016 ) , Stanford ( de Marneffe and Manning , 2008 ) and CoNLL-2009 dependencies ( Hajič et al . , 2009 ) , all using different label sets and syntactic head attachment rules . Any probing study inevitably commits to the specific theoretical framework used to produce the underlying data . The differences between linguistic formalisms , however , can be substantial .

Similar to the masked language model ( MLM ) objective adopted in BERT , our model is trained by predicting randomly masked entities . Specifically , we randomly replace some percentage of the entities with special [ MASK ] entity tokens and then trains the model to predict masked entities .

To minimize the loss , the expectations could be approximated by sampling as shown in Algorithm 1 . Taking the gradient of this estimated loss produces Algorithm 1 Naive NCE loss estimation Given : Input sequence x , number of negative samples k , noise distribution q , modelp θ .

• ( Q1 ) How could different adaptation configurations ( hyperparameters , tuning methods , and training data ) affect PLM 's mode connectivity ?

We now turn to the probing results for decompositional semantic proto - role labeling tasks . Unlike ( Tenney et al . , 2019b ) who used a multi - label classification probe , we treat SPR properties as separate regression tasks . The results in Table 6 show that the performance varies by property , with some of the properties attaining reasonably high R 2 scores despite the simplicity of the probe architecture and the small dataset size . We observe that properties associated with Proto - Agent tend to perform better . The untrained mBERT baseline performs poorly which we attribute to the lack of data and the finegrained semantic nature of the task . Our fine - grained , property - level task design allows for more detailed insights into the layer utilization by the SPR probes ( Fig . 4 ) . The results indicate that while the layer utilization on the predicate side ( src ) shows no clear preference for particular layers ( similar to the results obtained by Tenney et al . ( 2019a ) ) , some of the proto - role features follow the pattern seen in the categorical role labeling and dependency parsing tasks for the argument tokens tgt . With few exceptions , we observe that the properties displaying that behavior are Proto - Agent properties ; moreover , a close examination of the results on syntactic preference by Reisinger et al . ( 2015 , p. 483 ) reveals that these properties are also the ones with strong preference for the subject position , including the outlier case of stationary which in their data behaves like a Proto - Agent property . The correspondence is not strict , and we leave an in - depth investigation of the reasons behind these discrepancies for follow - up work .

To make predictions on personas , the arg max function is used for the estimated distribution of persona predictors . However , the internal distribution conveys crucial information about how the persona predictors estimate f ( u ) . We follow the setup of imbalanced data split of 8 clusters in Section 5.5 to examine persona predictors of attacker A and fake attacker A p .

Our model is based on BERT and takes words and entities ( Wikipedia entities or the [ MASK ] entity ) .

Building models for entity linking against unseen KBs requires that such models do not overfit to the training data by memorizing characteristics of the training KB . This is done by using two regularization schemes that we apply on top of the candidate string generation techniques discussed in the previous section .

-DOCSTART- Question Generation for Adaptive Education

Our main probing results mirror the findings of Tenney et al . ( 2019a ) about the sequential processing order in BERT . We observe that the layer utilization among tasks ( Fig . 2 ) generally aligns for English and German 4 , although we note that in terms of center - of - gravity mBERT tends to utilize deeper layers for German probes . Basic word - level tasks are indeed processed early by the model , and XNLI probes focus on deeper levels , suggesting that the representation of higher - level semantic phenomena follows the encoding of syntax and predicate semantics .

Slot filling . As described in Section 4 , we approach the slot filler extraction task as fine - grained entity - typing - in - context , assuming that each sentence represents a single experiment frame . We use the same sequence tagging architectures as above for tagging the tokens of each experimentdescribing sentence with the set of slot types ( see Table 11 ) . Future work may contrast this sequence tagging baseline with graph - induction based frame extraction .

A training dataset D = { ( x 1 , y 1 ) , . . . , ( x N , y N ) } is given , where x i is the i - th document containing a sequence of L words , [ x i1 , x i2 , ... , x iL ] , and y i ∈ C is the class label of

. m is again the number of perturbation samples . We will use " SVS - m " and " KS - m " in the rest of the paper to indicate the sample size for SVS and KS . In practice , the specific perturbation samples depend on the random seed of the sampler , and we will show that the explanation scores are highly sensitive to the random seed under a small sample size .

The training data publicly available to build wordlevel QE models is limited to very few language pairs , which makes it difficult to build QE models for many languages . From an application perspective , even for the languages with resources , it is difficult to maintain separate QE models for each language since the state - of - the - art neural QE models are large in size ( Ranasinghe et al . , 2020b ) .

There are two benefits of using the label name to augment utterances . First , we make use of different appended label name to distinguish the same utterance with multiple labels , thus obtaining more discriminative representations for each class . As illustrated in Figure 3 , we can obtain different prototypes for class 1 and class 2 , eliminating the problem that the multi - label utterances share the same representation essentially without any complex architecture or parameters . Second , this enables us to utilize the label name information as well as increase the number of support samples to eliminate the ambiguity caused by the scarcity of utterances . In Section 4.4 , we analyze the experimental results to verify the improvements of augmentation .

When it comes to the polysemy headers , with the help of context like " Height " , " Width " and " Depth " , H2H+CXT and CAST can disambiguate polysemy header " Area " from region or zone to acreage . For header " Volume " , However , H2H+CXT copies the source language column , which is not a valid translation , because the translator is disturbed by the context . On the other hand , with the help of the relational - aware transformer encoder , CAST generates a proper translation for " Volume " as the capacity of the engine . Affected by the context , H2H+CXT only translates part of the information from header ' Film.1 ' and ' Rank of the year ' , while M2M-100 , H2H , and CAST give an appropriate translation .

The balancing factor λ is set to be 1 1 .

B6 . Did you report relevant statistics like the number of examples , details of train / test / dev splits , etc . for the data that you used / created ? Even for commonly - used benchmark datasets , include the number of examples in train / validation / test splits , as these provide necessary context for a reader to understand experimental results . For example , small differences in accuracy on large test sets may be significant , while on small test sets they may not be . In Appendix C .

Tables 1 , 2 , and 4 report the experimental results for 1 - shot and 5 - shot multi - label intent detection tasks on TourSG and StanfordLU . We use both Electra - small ( + E ) and Bert - base ( + B ) as feature encoders . The baseline results are taken from ( Hou et al . , 2021 ) and the top 1 results are highlighted in bold .

Finally , while the focus of this work is only on English entity linking , challenges associated with this work naturally occur in multilingual settings as well . Just as we can not expect labeled data for every target KB of interest , we also can not expect labeled data for different KBs in different languages . In future work , we aim to investigate how we can port the solutions introduced here to multilingual settings as well develop novel solutions for scenarios where the documents and the KB are in languages other than English ( Sil et al . , 2018;Upadhyay et al . , 2018;Botha et al . , 2020 ) .

Previous studies ( Jagarlamudi et al . , 2012 ; Meng et al . , 2020a ) have tried some other datasets ( e.g. , RCV1 , 20 Newsgroups , NYT , and Yelp ) . However , the category names they use in these datasets are 4 https : / / github.com / allenai / scidocs 5 http : / / jmcauley.ucsd.edu / data / amazon / index.html 6 https : / / github.com / franticnerd / geoburst 7 https : / / github.com / shangjingbo1226 / AutoPhrase all picked from in - vocabulary terms . Therefore , we do not consider these datasets for evaluation in our task settings . Following ( Sia et al . , 2020 ) , we adopt a 60 - 40 train - test split for all three datasets . The training set is used as the input corpus D , and the testing set is used for calculating topic coherence metrics ( see evaluation metrics for details ) .

Figure 2 presents the result of Williams significance test for BiRNN model variants . It is a correlation matrix that can be read as follows : the value in cell ( i , j ) is the p - value of Williams test for the change in performance of the model at row i compared to the model at column j ( Graham , 2015 ) .

The Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022 , with the addition of a question on AI writing assistance .

We evaluate the effectiveness of the proposed NOVEL on TO - MAR dataset , followed by a discus- sion of NOVEL 's property with controlled studies .

First , we categorize all newly correct mentions , i.e. mentions that are correctly linked by the top model but incorrectly linked by the [ SEP]-separation baseline by the entity type of the gold entity . This type is one of person ( PER ) , organization ( ORG ) , geo - political entity ( GPE ) , and a catchall unknown 10 The 0 % results are the same as those in Table 3 . category ( UKN ) . 11 This categorization reveals that the newly correct mentions represent about 15 % of the total mentions of the ORG , GPE , and UKN categories and as much as 25 % of the total mentions of the PER category . This distributed improvement highlights that the relatively higher accuracy of our model is due to a holistic improvement in modeling unseen KBs across all entity types .

The pizza problem , . . . all simple correlations between input features and output labels are spurious " ( emphasis in the original ) . The property that individual input features should be independent of labels -which I will call marginally uninformative input features ( UIF ) 1 -is treated as an assumption about the nature of language processing and also as a desideratum that datasets should satisfy : if the label can be predicted from input features alone , then the dataset is in some sense too easy . 2

B Search Query and Topic Relevance Synonyms

In order to generalize the framework beyond CNNs , there are two questions to consider . First , what is an effective way to understand each feature ? We exemplified this with two word clouds representing each BiLSTM feature in Appendix C , and we plan to experiment with advanced visualizations such as LSTMVis ( Strobelt et al . , 2018 ) in the future . Second , can we make the model features more interpretable ? For example , using ReLU as activation functions in LSTM cells ( instead of tanh ) renders the features non - negative . So , they can be summarized using one word cloud which is more practical for debugging .

The S & P 500 gauges the performance of the stocks of the 500 largest , most stable companies in the Stock Exchange . It is often considered the most accurate measure of the stock market as a whole . The current average annual return from 1926 , the year of the S & Ps inception , through 2011 is 11.69 % . That 's a long look back , and most people are n't interested in what happened in the market 80 years ago .

We did not use any AI writing assistants .

For our framework , we use the huggingface implementation of GPT2 - base ( Wolf et al . , 2020 ) as the PLM and the prefix - tuning implementation is derived from OpenPrompt . All results are averaged over 5 different seeds . The prefix length has an essential impact on the results , so we search it from { 10 , 50 , 100 , 200 , 300 , 400 , 500 } . For PTO + Label , the total prefix length 300 is equally allocated to each label . For PTO + OOD , the OOD prefix length is also set to 300 . The hyper - parameters of PTO + Label + OOD are consistent with PTO + OOD and PTO + Label .

Prompt learning has been demonstrated to be a successful remedy for challenges associated with pre - training and fine - tuning paradigm , especially in zero / few - shot scenarios ( Gao et al . , 2021 ; Schick and Schütze , 2021a , b ; Tam et al . , 2021 ; Lu et al . , 2022a ) .

Surprisal Theory ( Hale , 2001 ; Levy , 2008 ) posits that comprehenders construct probabilistic interpretations of sentences based on previously encountered structures . Mathematically , the surprisal of the k th word , w k , is defined as the negative log probability of w k given the preceding context :

Natural 242 Translated 219 UNMT : style gap and content data . We divide the test sets into two portions : the natural input portion with source sentences originally written in the source language and the translated input portion with source sentences translated from the target language . Due to the limited space , we conduct the experiments with pre - trained XLM initialization and perform analysis with different kinds of inputs ( i.e. , natural and translated inputs ) on De⇒En newstest2013 - 2018 unless otherwise stated .

Summary - level Likert scale annotations are the most commonly used setup for collecting coherence in single - document news summarization research ( Fabbri et al . , 2021 ) . Here , we run an analogous study for our longer narrative summaries . We ask 3 Mechanical Turk workers with prior experience in annotation for NLP tasks , specifically discourse analysis and text simplification , to rate the overall coherence of 100 generated summaries on a 5 - point scale . Table 1 reports the observed agreement , measured by Kripendorff 's α . Compared to newswire summaries collected under a similar setup ( Fabbri et al . , 2021 ) , annotations for longer narratives have a much lower agreement . This shows the difficulty in obtaining a consensus on coherence for a 500 + word summary through a single value on a 5 - point scale .

Dependency relation ( deprel ) predicts the dependency relation between the parent src and dependent tgt tokens ; Semantic role ( role.[frm ] ) predicts the semantic role given a predicate src and an argument tgt token in one of the three role labeling formalisms : PropBank pb , VerbNet vn and FrameNet fn . Note that we only probe for the role label , and the model has no access to the verb sense information from the data . Semantic proto - role ( spr . [ prop ] ) is a set of eleven regression tasks predicting the values of the proto - role properties as defined in ( Reisinger et al . , 2015 ) , given a predicate src and an argument tgt . XNLI is a sentence - level NLI task directly sourced from the corresponding dataset . Given two sentences , the goal is to determine whether an entailment or a contradiction relationship holds between them . We use NLI to investigate the layer utilization of mBERT for high - level semantic tasks . We extract the sentence pair representation via the [ CLS ] token and treat it as a unary probing task .

We also evaluated how the QE models behave with a limited number of training instances . For each language pair , we initiated the weights of the bilingual model with those of the relevant All-1 QE and trained it on 100 , 200 , 300 and up to 1000 training instances . We compared the results with those obtained having trained the QE model from scratch for that language pair . The results in Figure 2 show

In simultaneous machine translation ( SNMT ) models , the probability of predicting the target token y i ∈ y depends on the partial source and target sequences ( x ≤j ∈ x , y < i ∈ y ) . In sequence - tosequence based SNMT model , each target token y i is generated as follows :

where I ( X 1 : X 2 ) is the mutual information of random variables X 1 and X 2 , H ( X ) represents the entropy of the random variable X , and ξ represents the Lagrange multiplier .

Test dataset : Yelp Negative F1

We evaluate seven types of models on GQNLI , fine - tuned with different combinations of NLI datasets . As data creation only relied on RoBERTa and MNLI , nothing prevents that models with different architectures and training data will perform well . They do not , however . The results are shown in Table 8 .

In this paper , we study the following question : How to shepherd a PLM to recall a series of stored knowledge ( e.g. , C1 and C2 ) that is necessary for multi - step inference ( e.g. , answering Q ) , analogous to how humans develop a " chain of thought " for complex decision making ?

Although our model has successfully tackled the two missing patterns , it may still fail in more complicated cases . For example , if missing happens randomly in terms of frames ( some timestamps within a unimodal clip ) instead of instances ( the entire unimodal clip ) , then our proposed approach could not be directly used to deal with the problem , since we need at least several instances of complete parallel data to learn how to map from one modality sequences to the other . However , we believe these types of problems can still be properly solved by adding some mathematical tools like interpolation , etc . We will consider this idea as the direction of our future work .

in the schema . The translation of H i is denoted as Y i = hy 1 , . . . , y m i , where y j is the jth token of the header in the target language . Taking a header H and its corresponding context C as input , the model outputs the header Y in the target language .

The above two definitions of f use the values v i , but not the attributes k i , which also contain meaningful information . For example , if an entity seen during inference has a capital attribute with the value " New Delhi " , seeing the capital attribute allows us to infer that the target entity is likely to be a place , rather than a person , especially if we have seen the capital attribute during training . We capture this information using attribute separators , which are reserved tokens ( in the vein of [ SEP ] tokens ) corresponding to attributes . In this case ,

Everything Is Fine Heuristic This heuristic applies when the premise contains condition(s ) or finding(s ) , the target class is contradiction , and the generated hypothesis negates the premise or asserts unremarkable finding(s ) . This can take two forms : repetition of premise content plus negation , or inclusion of modifiers that convey good health .

It is challenging to make exact claims about what can cause tokens to be unargmaxable because the models we tested varied in so many ways . However , we outline some general trends below .

Since obtaining a perfect training dataset ( i.e. , a dataset which is considerably large , unbiased , and well - representative of unseen cases ) is hardly possible , many real - world text classifiers are trained on the available , yet imperfect , datasets . These classifiers are thus likely to have undesirable properties . For instance , they may have biases against some sub - populations or may not work effectively in the wild due to overfitting . In this paper , we propose FINDa framework which enables humans to debug deep learning text classifiers by disabling irrelevant hidden features . Experiments show that by using FIND , humans can improve CNN text classifiers which were trained under different types of imperfect datasets ( including datasets with biases and datasets with dissimilar traintest distributions ) .

Overall , our method enhances the lexical - level information captured by pretrained MLMs , as shown empirically . This is consistent with our intuition that cross - lingual embeddings capture a bilingual signal that can benefit MLM representations . 1 - gram precision scores . To examine whether the improved translation performance is a result of the lexical - level information provided by static embeddings , we present 1 - gram precision scores in Ta- ble 3 , as they can be directly attributed to lexical alignment . The biggest performance gains ( up to +10.4 ) are obtained when the proposed approach is applied to XLM . This correlates with the BLEU scores of Table 1 . Moreover , the En - Mk language pair benefits more than En - Sq from the lexicallevel alignment both in terms of 1 - gram precision and BLEU . These results show that the improved BLEU scores can be attributed to the enhanced lexical representations . How should static embeddings be integrated in the MLM training ? We explore different ways of incorporating the lexical knowledge of pretrained cross - lingual embeddings to the second , masked language modeling stage of our approach ( § 2.2 ) . Specifically , we keep the aligned embeddings fixed ( frozen ) during XLM training and compare the performance of the final UNMT model to the proposed ( fine - tuned ) method . We point out that , after we transfer the trained MLM to an encoder - decoder model , all layers are trained for UNMT .

Sequential neural network architectures in their various forms have become the mainstay in abstractive summarization ( See et al . , 2017 ; Lewis et al . , 2020 ) . However , the quality of machine - produced summaries still lags far behind the quality of human summaries ( Huang et al . , 2020a ; Xie et al . , 2021 ; Cao et al . , 2022 ; Lebanoff et al . , 2019 ) . Due to their sequential nature , a challenge with neural summarizers is to capture hierarchical and inter - sentential dependencies in the summmarized document .

Figure 2 : The AdMIRaL pipeline . At hop t , given the claim and sentences E t from documents D t , a proof is generated to predict whether the evidence E t is sufficient for verification or whether additional evidence is needed . If sufficient , the retriever terminates , otherwise , our autoregressive retriever scores documents in the KB jointly with E t updating the documents to D t+1 , before they are passed to the sentence reranker to obtain E t+1 .

Deep learning has become the dominant approach to address most Natural Language Processing ( NLP ) tasks , including text classification . With sufficient and high - quality training data , deep learning models can perform incredibly well . However , in real - world cases , such ideal datasets are scarce . Often times , the available datasets are small , full of regular but irrelevant words , and contain unintended biases ( Wiegand et al . , 2019;Gururangan et al . , 2018 ) . These can lead to suboptimal models with undesirable properties . For example , the models may have biases against some sub - populations or may not work effectively in the wild as they overfit the imperfect training data .

Human Evaluation We conduct an expert evaluation on a subset of generated arguments with two researchers ( field of expertise is natural language processing ) not involved in this paper . Two aspects are evaluated : fluency and persuasiveness . We consider a sentence as fluent if it is grammatically correct , i.e. contains neither semantic nor syntactic errors , and arrange this as a binary task . To reduce subjectivity for the persuasiveness evaluation , the experts do not annotate single arguments but instead compare pairs ( Habernal and Gurevych , 2016 ) of generated and refer - ence data arguments ( see Section 5.2 ) . The experts could either choose one argument as being more persuasive or both as being equally persuasive . In total , the experts compared 100 ( randomly sorted and ordered ) argument pairs for persuasiveness and fluency ( 50 from both the Arg - CTRL REDDIT and the Arg - CTRL CC ) . A pair of arguments always had the same topic and stance . For fluency , only the annotations made for generated arguments were extracted and taken into account . Averaged results of both experts show that in 33 % of the cases , the generated argument is either more convincing ( 29 % ) or as convincing ( 4 % ) as the reference argument . Moreover , 83 % of generated arguments are fluent . The inter - annotator agreement ( Cohen , 1960 ) between the two experts is Cohen 's κ = .30 ( percentage agreement : .62 ) for persuasiveness and κ = .43 ( percentage agreement : .72 ) for fluency , which can be interpreted as " fair " and " moderate " agreement , respectively ( Landis and Koch , 1977 ) . As we compare to high - quality , curated data , the perceived persuasiveness of the generated arguments shows the potential of the work - further strengthened in the remainder of this section .

Computational efficiency is an important factor when designing pre - training tasks . A more efficient method enables models to train on a larger dataset for more steps . We calculate the feedforward floating point operations ( FLOPs ) for our method and TLM , respectively . In addition , we report the training latency in our training environment . We measure the latency with a total batch size of 512 on 8 Tesla V100 GPUs using PyTorch distributed data parallel .

Datasets . For ZH→EN ( News ) , we follow and use document parallel corpora from LDC as the training set , NIST2006 dataset as the development set and combination of NIST2002 , 2003NIST2002 , , 2004NIST2002 , , 2005NIST2002 , and 2008 as the test set . For ZH→EN ( TED ) , the training set is from the IWSLT 2014 and 2015 evaluation ( Cettolo et al . , 2012(Cettolo et al . , , 2015 . We use dev2010 as the development and combine tst2010 - 2013 as the test set . For FR→EN ( TED ) , the training set is from the 2015 evaluation ( Cettolo et al . , 2015 ) . We use dev2010 as the development and combine tst2010 - 2013 as the test set . More statistics and preprocessing of the experimental datasets are in Appendix A.

-DOCSTART- Aspect - Controlled Neural Argument Generation

Token type embedding represents the type of token , namely word ( C word ) or entity ( C entity ) .

We introduced Multimodal Quality Estimation for Machine Translation , where an external modality -visual information -is incorporated to featurebased and neural - based QE approaches , on sentence and document levels . The use of visual features extracted from images has led to significant improvements in the results of state - of - the - art QE approaches , especially at sentence level .

Neural entity tagging and slot filling . The neural - network based models we use for entity tagging and slot filling bear similarity to state - ofthe - art models for named entity recognition ( e.g. , Huang et al . , 2015;Lample et al . , 2016;Panchendrarajan and Amaresan , 2018;Lange et al . , 2019 ) . Other related work exists in the area of semantic role labeling ( e.g. , Roth and Lapata , 2015;Kshirsagar et al . , 2015;Hartmann et al . , 2017;Adel et al . , 2018;Swayamdipta et al . , 2018 ) .

2 . Infill . We fine - tune BART to fill the masked spans of input sentences . The model learns to generate hyperbolic words or phrases that are pertinent to the context .

To represent the full dataset , we use fastText MIMIC - III embeddings , which have been pretrained on deidentified patient notes from MIMIC - III ( Romanov and Shivade , 2018;Johnson et al . , 2016 ) . We represent each example as the average of its component token vectors . We proportionally adjust a subset of the hyperparameters used by Sakaguchi et al . ( 2020 ) to account for the fact that MedNLI contains far fewer examples than WINOGRANDE 2 : specifically , we set the training size for each ensemble , m , to 5620 , which represents ≈ 2 5 of the MedNLI combined dataset . The remaining hyperparameters are unchanged : the ensemble consists of n = 64 logistic regression models , the filtering cutoff , k = 500 , and the filtering threshold τ = 0.75 .

Due to its task - agnostic architecture , edge probing can be applied to a wide variety of unary ( by omitting tgt ) and binary labeling tasks in a unified manner , facilitating the cross - task comparison . The original setup has several limitations that we address in our implementation .

This work has been funded by the LOEWE initiative ( Hesse , Germany ) within the emergenCITY center .

Results of rationale generation : From Tab . 2 and Tab . 3 , we have the following observations : 1 ) Joint generation mode ( JGM ) achieves better performance than the separate generation mode ( SGM ) , which indicates the shared encoder has a promoting effect on rationale generation . 2 ) The performance of charge rationale generation is better than that of penalty rationale generation , which may be owing to the complexity of the discourse of penalty rationale . 3 ) The performance of PGN - JGM is better than C3VG , which proves the advantage of splitting the rationale generation and result prediction . 4 ) The Kappa coefficient between any two human annotators is over 0.78 ( substantial agreement ) , which indicates the quality of human evaluation .

• As mentioned in above , a few arguments that express little information and whose subjects are primarily pronouns , in which case our ACE module may be limited . For example , an argument is " no offense , but that is incredibly stupid / selfish . " . Since the sentence expresses only a small amount of information , semantic similarity may not fully reflect the correlation between sentences , which affects the ACE module to some extent , which may affect the ACE module to some extent . In this case , it might be better to use the adjacent context block directly .

-- -- -Input -- -- -First line of input contains two integers , $ N $ ( $ 1 \le N \le 1000 $ ) , the length of the message , and $ C $ ( $ 1 \le C \le 1000000000 $ ) , the number from the task description above .

We assess the performance of the MET on three datasets : the Amazon Customer Reviews Dataset , Reddit Corpus , and the Cornell Movie - Dialogs Corpus .

In this paper , we demonstrate that MedNLI suffers from the same challenge associated with annotation artifacts that its domain - agnostic predecessors have encountered : namely , NLI models trained on { Med , S , Multi}NLI can perform well even without access to the training examples ' premises , indicating that they often exploit shallow heuristics , with negative implications for out - of - sample generalization . Interestingly , many of the high - level lexical characteristics identified in MedNLI can be considered domain - specific variants of the more generic , classspecific patterns identified in SNLI . This observation suggests that a set of abstract design patterns for inference example generation exists across domains , and may be reinforced by the prompts provided to annotators . Creative or randomized priming , such as Sakaguchi et al . ( 2020 ) 's use of anchor words from WikiHow articles , may help to decrease reliance on such design patterns , but it appears unlikely that they can be systematically sidestepped without introducing new , " corrective " artifacts .

• Industry track chairs : Beata Beigman Klebanov , Jason Williams , and Sunayana Sitaram . An addition to this year 's ACL is the introduction of a separate industry track . This is motivated by two factors . First , ACL is held in North America this year ( and thus no NAACL ) , and NAACL has an established tradition of hosting an industry track . Second there was an increasing number of industry track submissions at EMNLP last year from previous years . We hope that a separate industry track can foster the dissemination of research on real - world applications in industry settings . Thanks to the industry track chairs for their efforts in coordinating all the logistics associated with this track .

Central Events Annotation We manually annotate central events on the public dataset EventSto - ryLine to investigate the effect of centrality . In specific , we annotate central events considering the following rules :

• Group 5 : AGNews . AG News is a topic classification dataset ( Zhang et al . , 2015 ) collected from various news sources . There are four topics in total . Similar to Group 4 , we conduct experiments with each single label for ID and others for OoD , respectively .

B4 . Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content , and the steps taken to protect / anonymize it ? Not applicable . Left blank .

We collected curricula from university study programs from different countries and categorized them into five computing disciplines : Computer Science ( CS ) , Computer Engineering ( CE ) , Information Technology ( IT ) , Information Science ( IS ) , and Software Engineering ( SE ) .

In summary , our analysis provides a new way of understanding the role of the demonstrations in in - context learning . We empirically show that the model ( 1 ) counter - intuitively does not rely on the ground truth input - label mapping provided in the demonstrations as much as we thought ( Section 4 ) , and ( 2 ) nonetheless still benefits from knowing the label space and the distribution of inputs specified by the demonstrations ( Section 5 ) . We also include a discussion of broader implications , e.g. , what we can say about the model learning at test time , and avenues for future work ( Section 6 ) .

To improve the models , previous work has looked into different techniques beyond standard model fitting . If the weaknesses of the training datasets or the models are anticipated , strategies can be tailored to mitigate such weaknesses . For example , augmenting the training data with genderswapped input texts helps reduce gender bias in the models ( Park et al . , 2018;Zhao et al . , 2018 ) . Adversarial training can prevent the models from exploiting irrelevant and/or protected features ( Jaiswal et al . , 2019;Zhang et al . , 2018 ) . With a limited number of training examples , using human rationales or prior knowledge together with training labels can help the models perform better ( Zaidan et al . , 2007;Bao et al . , 2018;Liu and Avci , 2019 ) .

Generally , deep text classifiers can be divided into two parts . The first part performs feature extraction , transforming an input text into a dense vector ( i.e. , a feature vector ) which represents the input . There are several alternatives to implement this part such as using convolutional layers , recurrent layers , and transformer layers . The second part performs classification passing the feature vector through a dense layer with softmax activation to get predicted probability of the classes . These deep classifiers are not transparent , as humans can not interpret the meaning of either the intermediate vectors or the model parameters used for feature extraction . This prevents humans from applying their knowledge to modify or debug the classifiers .

" male " , " males " , " boy " , " boys " , " man " , " men " , " gentleman " , " gentlemen " , " he " , " him " , " his " , " himself " , " brother " , " son " , " husband " , " boyfriend " , " father " , " uncle " , " dad " Female gender terms :

We summarize the differences between imagined and recalled stories in HIPPOCORPUS in Table 2 . For our narrative flow and lexicon - based analyses , 4 ATOMIC contains social and inferential knowledge about the causes ( e.g. , " X wants to start a family " ) and effects ( e.g. , " X throws a party " , " X feels loved " ) of everyday situations like " PersonX decides to get married " . 5 See liwc.wpengine.com/interpretingliwc-output/ for more information on LIWC variables . we perform paired t - tests . For realis and commonsense event measures , we perform linear regressions controlling for story length . 6 We Holmcorrect for multiple comparisons for all our analyses ( Holm , 1979 ) .

where H is the number of heads , head i is the output of i - th head and G h , i is the i - th entry of the gating variables G h ∈ R H . G h , i indicates whether the head i will be pruned . G h , i is set to 1 to retain that head and 0 if to drop it . Different pruning algorithms will have their own ways to determine the values of G h .

Given a trained amortized model , there is no randomness when generating explanation scores . However , there is still some randomness in the training process , including the training data , the random initialization of the output layer and randomness during update such as dropout . Therefore , similar to Section 4 , we study the sensitivity of the amortized model . Table 4 shows the results with different training data and random seeds . We observe that : 1 ) when using the same data ( 100 % ) , random initialization does not affect the outputs of amortized models -the correlation between different runs is high ( i.e. , 0.77 on MNLI and 0.76 on Yelp - Polarity ) . 2 ) With more training samples , the model is more stable .

( 2 ) sentence + context : target sentence with surrounding context ; ( 3 ) sentence + context + entities : additionally appending all entities in their canonical names as extracted in § 4.1 , same as our model 's input . We further consider two variants of our model as baselines . We first design a pipeline model , which first uses the node prediction module to identify salient entities for inclusion in the stance triplets .

Argument aspect detection is necessary for our argument generation pipeline , as it allows for a finegrained control over the generation process . We create a new dataset , as existing approaches either rely on coarse - grained frames or can not be applied by non - expert annonators in a scalable manner .

All BERT models are uncased BERT - base models with 12 layers , 768 hidden units , and 12 heads with default parameters , and trained on English Wikipedia and the BookCorpus . The probability p drop for attribute - OOV is set to 0.3 . Both candidate generation and re - ranking models are trained using the BERT Adam optimizer ( Kingma and Ba , 2015 ) , with a linear warmup for 10 % of the first epoch to a peak learning rate of 2 × 10 −5 and a linear decay from there till the learning rate approaches zero . 9 Candidate generation models are trained for 200 epochs with a batch size of 256 . Re - ranking models are trained for 4 epochs with a batch size of 2 , and operate on the top 32 candidates returned by the generation model . Hyperparameters are chosen such that models can be run on a single NVIDIA V100 Tensor Core GPU with 32 GB RAM , and are not extensively tuned . All models have the same number of parameters except the ones with attribute - separators which have 100 extra token embeddings ( of size 768 each ) .

Both layer and anchor task analysis reveal a prominent discrepancy between English and German role probing results : while the PropBank predicate layer utilization for English mostly relies on syntactic information , German PropBank predicates behave similarly to VerbNet and FrameNet . The lack of systematic cross - lingual differences between layer utilization for other probing tasks 6 allows us to rule out the effect of purely typological features such as word order and case marking as a likely cause . The difference in the number of role labels for English and German PropBank , however , points at possible qualitative differences in the labeling schemes ( Table 3 ) . The data for English stems from the token - level alignment in SemLink that maps the original PropBank roles to Verb - Net and FrameNet . Role annotations for German have a different lineage : they originate from the FrameNet - annotated SALSA corpus ( Burchardt et al . , 2006 ) semi - automatically converted to Prop - Bank style for the CoNLL-2009 shared task ( Hajič et al . , 2009 ) , and enriched with VerbNet labels in SR3de ( Mújdricza - Maydt et al . , 2016 ) . As a result , while English PropBank labels are assigned in a predicate - independent manner , German PropBank , following the same numbered labeling scheme , keeps this scheme consistent within the frame . We assume that this incentivizes the probe to learn semantic verb groupings and reflects in our probing results . The ability of the probe to detect subtle differences between formalism implementations constitutes a new use case for probing , and a promising direction for future studies .

Model . We use the model presented in Lee et al . ( 2018a ) with RoBERTa as a feature extractor .

Identity - related ( A I ) Potentially offensive or stereotyping terms towards minority identities ( e.g. , " n*gro " , " f*ggot " , " k*ke " , " wh*re " ) , as well as reclaimed slurs ( e.g. , " n*gga " ) ( Figure 1 , top left ) .

To mitigate the effect of clinical annotation artifacts , we employ AFLite , an adversarial filtering algorithm introduced by Sakaguchi et AFLite requires distributed representations of the full dataset as input , and proceeds in an iterative fashion . At each iteration , an ensemble of n linear classifiers are trained and evaluated on different random subsets of the data . A score is then computed for each premise - hypothesis instance , reflecting the number of times the instance is correctly labeled by a classifier , divided by the number of times the instance appears in any classifier 's evaluation set . The top - k instances with scores above a threshold , τ , are filtered out and added to the easy partition ; the remaining instances are retained . This process continues until the size of the filtered subset is < k , or the number of retained instances is < m ; retained instances constitute the difficult partition .

The amount of pretraining data and in - task training data are strongly correlated with overall task performance for most of the considered tasks ; this corroborates similar results from Wu and Dredze ( 2020 ) . Language similarity with English is also correlated with better in - task performance on all tasks except for dependency arc prediction , suggesting that some form of crosslingual signal supports in - language performance for linguistically similar languages .

The generation of questions from a sentence relies on the jsRealB text realizer ( Lapalme , 2021 ) which generates an affirmative sentence from a constituent structure . It can also be parameterized to generate variations of the original sentence such as its negation , its passive form and different types of questions such as who , what , when , etc . The constituency structure of a sentence is most often created by a user or by a program from data . In this work , it is instead built from a Universal Dependency ( UD ) structure using a technique developed for SR'19 ( Lapalme , 2019 ) . The UD structure of a sentence is the result of a dependency parse with Stanza ( Qi et al . , 2020 ) . We thus have a pipeline composed of a neural dependency parser , followed by a program to create a constituency structure used as input for a text realizer , both in JavaScript . Used without modification , this would create a complex echo program for the original affirmative sentence , but by changing parameters , its output can vary .

FairPrism is intended to be used by researchers and practitioners who wish to diagnose ( 1 ) the types of fairness - related harms that AI text generation systems cause , and ( 2 ) the potential limitations of mitigation methods . In this section , we suggest possible analyses , along with illustrative case studies .

We introduced a new NLP task , object use classification , which identifies whether an object mentioned in a sentence has been used or likely will be used . We introduced a gold standard dataset for this task and showed that all 3 categories ( Used , Anticipated Use , and No Use ) are common in real sentences . Then we presented a transformer - based architecture for this task that uses two types of data augmentation techniques ( synonym / hyponym replacement and back translation ) and also exploits exemplar sentences from FrameNet that correspond to an object 's prototypical function . The resulting classification model achieves reasonably good performance for this task , although there is room for improvement that we hope will inspire future work on this problem .

Given a training ID sequence x = { x i } N i=1 ∈ D in , the learning loss for prediction layer distillation w.r.t . x is formulated as the Kullback - Leibler divergence between the output probability distributions over the vocabulary V output by the teacher model and by the student model . Averaging over all tokens , we have :

2 ) as examples . The underlined words are with the top - k highest importance predicted by the proposed pipeline . Reference segment Gross margin from manufacturing operations as a percentage of manufacturing revenues increased to 27 % for the year ended December 31 , 2014 , from 23 % for the comparable prior year period . Target segment Gross margin from manufacturing operations as a percentage of manufacturing revenues decreased to 15 % for the year ended December 31 , 2016 from 23 % for the comparable prior year period .

▷ FreeLB training 15 Back - propagate L i + L e and accumulate gradient of parameters θ :

Zero - shot EL Linking to any DB This work ( Logeswaran et al . , 2019 ) ( Sil et al . , 2012 ) Test entities not seen during training Test KB schema unknown Out - of - domain test data Unrestricted Candidate Set and by stochastically removing attribute separators to generalize to unseen attributes ( Section 4.2 ) .

In this section , we present qualitative studies about semantic alignments between tokens for language modeling and machine translation tasks . We select three rare token from each datasets : " homepage " , " Werewolf " , and " policymakers " for WikiText-103 dataset , and " optimum " , " criminal " , and " happiness " for WMT14 En→De dataset . For each rare token , we extract the top-5 nearest neighbor token predicted by the cosine distance between token embeddings . Compared with baseline MLE method , AGG shows significant improvement to train semantic alignments for rare tokens . From Table 13 , we notice that the rare tokens trained with AGG are semantically well aligned and not biased about token frequency . Table 14 demonstrates that token embeddings trained with AGG also learn the cross - lingual semantic alignments between target language tokens .

In this subsection , we compare the empirical results of different variants of METRO - T0 . Table 4 shows the performance of each variant prompt - finetuned on T0 / T0+/T0++ Train and evaluated on T0 Eval .

As we know , the translation cost is expensive , and we provide parallel corpus in six languages , which limits the volume of translated headers . On the basis of our statistics , the average validating speed is 100 headers / hour and we spend 159.34 ⇤ 5 hours in total . This speed is much slower than the plain text translation since our translators need to read large amounts of different domain - specific contexts to help disambiguation . To this end , we make our best effort and translate 11,979 headers , spending 6,625 USD in total . According to our translators ' feedback , the context is quite helpful in understanding the meaning of headers . We will also release these contexts together with our schema translation dataset to facilitate further study .

Complete this story in 500 words . Miss Manette receives a letter from the bank informing her that information about her father 's small property has been discovered .

• SQuAD 1.1 : Stanford Question Answering Dataset ( Rajpurkar et al . , 2016 ) . Given a context paragraph and a question , the task is to select the span of text in the paragraph answering the question . The dataset contains 88k train examples from Wikipedia .

We rely on arguments in our daily lives to deliver our opinions and base them on evidence , making them more convincing in turn . However , finding and formulating arguments can be challenging . In this work , we present the Arg - CTRL - a language model for argument generation that can be controlled to generate sentence - level arguments for a given topic , stance , and aspect . We define argument aspect detection as a necessary method to allow this fine - granular control and crowdsource a dataset with 5,032 arguments annotated with aspects . Our evaluation shows that the Arg - CTRL is able to generate high - quality , aspectspecific arguments , applicable to automatic counter - argument generation . We publish the model weights and all datasets and code to train the Arg - CTRL . 1 Nuclear reactors produce radioactive waste ...

Basically , our model adopts a Transformer encoderdecoder architecture ( Vaswani et al . , 2017 ) , which takes the source language header with its corresponding context as inputs and generates the translation for the target language header as outputs . Specifically , we model the target header and its context as a directed graph and use the transformer self - attention to encode them over two predefined structural relationships and three entity types . Figure 2 depicts the overall architecture of our model via an illustrative example .

D2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students ) and paid participants , and discuss if such payment is adequate given the participants ' demographic ( e.g. , country of residence ) ? Not applicable . Left blank .

People living in bristol have complained about a vinegary whiff in the air .

When reasoning with LM prompting , the models should have the ability of semantic understanding ( e.g. , questions ) and complex reasoning ( e.g. , by generating reasoning processes ) ; however , we can not have both fish and bear 's paw ( Hendrycks et al . , 2021 ; Nogueira et al . , 2021 ; Lewkowycz et al . , 2022 ) . To tear up the obstacle , external reasoning engines lend a helping hand to LMs ( see Figure 5 ) .

We aim to train a model that is able to transfer argumentative information concisely within a single sentence . We define such an argument as the combination of a topic and a sentence holding evidence with a specific stance towards this topic ( Stab et al . , 2018b ) . Consequently , the following preprocessing steps ultimately target retrieval and classification of sentences . To evaluate different data sources , we use a dump from Common- We notice that many sentences are not relevant with regard to the document 's topic . To enforce topicrelevance , we decide to filter out all sentences that do not contain at least one token of the respective topic or its defined synonyms ( see App . B ) . We use the ArgumenText API 's 6 argument and stance classification models ( Stab et al . , 2018a ) to classify all sentences into argument or non - argument ( F 1 macro = .7384 ) , and remaining arguments into pro or con with regard to the topic ( F 1 macro = .7661 ) .

an unbiased estimate of ∇ θ L(θ ) . However , this algorithm is computationally expensive to run , since it requires k + 1 forward passes through the transformer to compute thep θ s ( once for the positive samples and once for each negative sample ) . We propose a much more efficient approach that replaces k input tokens with noise samples simultaneously shown in Algorithm 2 . It requires just

A line of machine translation research closely related to our task is the phrase - to - phrase translation , which considers phrases in multi - word expressions as their translation unit . Traditional phrase - based SMT models ( Koehn et al . , 2007;Haddow et al . , 2015 ) get phrase table translation probabilities by counting phrase occurrences and use local context through a smoothed n - gram language model . Recently , some works explore ways to adapt NMT models for phrase translation . For example , Wang et al . ( 2017 ) combined the phrase - based statistical machine translation ( SMT ) model into NMT and shown significant improvements on Chineseto - English translation data , explored the use of phrase structures for NMT systems by modeling phrases in target language sequences , and Feng et al . ( 2018 ) used a phrase attention mechanism to enhance the decoder in relevant source segment recognition . The main differences between these studies and our work are : ( 1 ) we do not rely on external phrase dictionaries or phrase tables ; and ( 2 ) we study how to make use of the schema context for word - sense disambiguation in the schema translation scenario .

Recently , research on generating arguments with language models gained more attention . use a sequence to sequence model ( Sutskever et al . , 2014 ) to generate argumentative text by attending to the input and keyphrases automatically extracted for the input from , for example , Wikipedia . Other work focuses on generating argumentative dialogue ( Le et al . , 2018 ) and counterarguments ( Hidey and McKeown , 2019 ; based on a given input sentence , or on generating summaries from a set of arguments ( Wang and Ling , 2016 ) . Contrarily , we train a language model that does not require a sentence - level input for generation and allows for direct control over the topic , stance , and aspect of the produced argument . Xing et al . ( 2017 ) design a language model that attends to topic information to generate responses for chatbots . Dathathri et al . ( 2019 ) train two models that control the sentiment and topic of the output of pre - trained language models at inference . Gretz et al . ( 2020a ) fine - tune GPT-2 on existing , labeled datasets to generate claims for given topics . However , the latter works do not explore generation for such a fine - grained and explicit control as proposed in this work . We show that argument generation requires the concept of argument aspects to shape the produced argument 's perspective and to allow for diverse arguments for a topic of interest .

Furthermore , using lexicon - based measures , we find that stories with high FREQUENCYOFRE - CALL tend to contain more self references ( Iwords ; Pearson 's |r| = 0.07 , p < 0.001 ) . Conversely , stories that are less frequently recalled are more logical or hierarchical ( LIWC 's ANALYTIC ; Pearson 's |r| = 0.09 , p < 0.001 ) and more concrete ( Pearson 's |r| = 0.05 , p = 0.03 ) .

Here , we introduce the GuessWhat ? ! visual dialogue game ( De Vries et al . , 2017 ) . We use this game as a running example to ground abstract theoretical concepts in practical application . Importantly , our theoretical study is more generally applicable ( i.e. , beyond just this example ) . Statistics on object distribution and dialogue length are provided in Figure 3 . After applying the labeling scheme and downsampling ( as just described ) , our dataset consists of about 3200 ( half with a = 1 ) when F is the protected attribute and 6400 ( half with a = 1 ) when M is the protected attribute . Note , this also indicates that the ratio of M to F in the original dataset is about 2 to 1 .

We used subsets of two datasets : ( 1 ) Yelp -predicting sentiments of restaurant reviews ( positive or negative ) and ( 2 ) Amazon Products -classifying product reviews into one of four categories ( Clothing Shoes and Jewelry , Digital Music , Office Products , or Toys and Games ) ( He and McAuley , 2016 ) . We sampled 500 and 100 examples to be the training data for Yelp and Amazon Products , respectively .

C1 . Did you report the number of parameters in the models used , the total computational budget ( e.g. , GPU hours ) , and computing infrastructure used ? In Appendix D .

Training sentence : We have some of the strongest gun laws in the country , but guns do n't respect boundaries any more than criminals do . Cosine similarity / edit distance / rel . overlap : 95.59 / 88 / 8 % Generated sentence : The radioactivity of the spent fuel is a concern , as it can be used to make weapons and has been linked to cancer in humans . Training sentence : However , it does produce radioactive waste , which must be disposed of carefully as it can cause health problems and can be used to make nuclear weapons Cosine similarity / edit distance / rel . overlap : 92.40 / 99 / 17 % the cases for Arg - CTRL REDDIT and in 74 % of the cases for Arg - CTRL CC . For the model that was not conditioned on aspects , however , it is only true in 8 % of the cases . It clearly shows the necessity to condition the model on aspects explicitly , implying the need for argument aspect detection , as the model is unable to learn generating aspect - related arguments otherwise . Moreover , without prior detection of aspects , we have no means for proper aggregation over aspects . We notice that for the model without prior knowledge of aspects , 79 % of all aspects in the training data appear in only one argument . For these aspects , the model will likely not pick up a strong enough signal to learn them .

As shown in fig . 6 , updating the teacher model first could lead to a lower entropy gap and faster convergence speed . We assume that the teacher could formulate an appropriate ' teaching plan ' for the student in this updating order .

Building on the success of monolingual pretrained language models ( LM ) such as BERT ( Devlin et al . , 2019 ) and RoBERTa ( Liu et al . , 2019 ) , their multilingual counterparts mBERT ( Devlin et al . , 2019 ) and XLM - R ( Conneau et al . , 2020 ) are trained using the same objectives - Masked Language Modeling ( MLM ) and in the case of mBERT , Next Sentence Prediction ( NSP ) . MLM is applied to monolingual text that covers over 100 languages . Despite the absence of parallel data and explicit alignment signals , these models transfer surprisingly well from high resource languages , such as English , to other languages . On the Natural Language Inference ( NLI ) task XNLI ( Conneau et al . , 2018 ) , a text classification model trained on English training data can be directly applied to the other 14 languages and achieve respectable performance . Having a single model that can serve over 100 languages also has important business applications .

We compare Electric against GPT-2 ( Radford et al . , 2019 ) , BERT ( Devlin et al . , 2019 ) , and two baseline systems that are bidirectional while only requiring a single transformer pass like Electric . TwoTower is a two - tower cloze model similar to Electric 's noise distribution , but of the same size as Electric . ELECTRA - TT is identical to ELECTRA except it uses a two - tower noise distribution rather than a masked language model . 5 The noise distribution probabilities and binary classifiers scores of ELECTRA can be combined to assign probabilities for tokens as shown in Appendix G of the ELECTRA paper .

In terms of teacher model , it should not only impart their current knowledge to the student , but also actively seek out new information and perspectives to improve their own understanding . As can be seen in fig . 2 ( c ) , LGTM allows for the effective transfer of knowledge from the teacher model by incorporating the teacher auxiliary loss . The validation accuracy of the teacher model keeps improving for LGTM , but drops quickly for Meta Distill .

We evaluate the sufficiency of translation policy on RWTH De→En alignment dataset 4 , where reference alignments are annotated by experts and seen as golden alignments 5 . The results are shown in Figure 5 . The oracle policy performs better than other methods in sufficiency evaluation and can even cover 75 % of the aligned source tokens under low latency . Wait - k policy is worse than our oracle policy under low latency because it may be forced to output translation before reading the aligned source tokens . MMA gets the worst performance in sufficiency evaluation ,

The fine - tuning on the CoNLL dataset significantly improves the performance on this dataset ( Table 1 ) . However , it generally degrades the performance on the other datasets ( Table 2 ) . This suggests that Wikipedia entity annotations are more suitable than the CoNLL dataset to train generalpurpose ED models .

The version of deepQuest for multimodal QE and scripts to convert document into sentencelevel data are available on https://github.com/ sheffieldnlp / deepQuest .

Beyond the quantitative evaluations above , we further qualitatively analyze the predictions of the best model from Table 3 to provide insights into our modeling decisions and suggest avenues for improvements .

Each row of the data consists of the German source sentence , its reference English translation ( it is not always accurate ! ) , and 1 to 4 machine translation outputs . The machine translation outputs are presented in a random order , to exclude the possibility of bias toward any specific method .

The task of coreference resolution is to find all textual expressions referring to the same real - world entities . We train on Ontonotes 5.0 ( Weischedel et al . , 2013 ) and test on the Winobias challenge dataset ( Zhao et al . , 2018 ) . Winobias consists of sentence pairs , pro - and anti - stereotypical variants , with individuals referred to by their profession . For example , " The physician hired the secretary be- cause he / she was busy . " is pro / anti - stereotypical , based on US labor statistics . 4 A coreference system is measured by the performance gap between the pro - and anti - stereotypical subsets .

Rolling out is costly for very long sequences , and the question of its usefulness necessarily arises . We study how rolling out for only a fixed number of tokens ( instead of until the end of the sequence ) influences the performance of PPL - MCTS . For this experiment , we use the CLS dataset and set the roll - out to 0 ( original result ) , 3 , 5 , 10 and 20 tokens . As one can note in Fig . 4 , only 5 tokens allows PPL - MCTS to be on par with GeDi on this dataset .

We provide details on the fine - tuning datasets below . All datasets are in English . GLUE data can be downloaded at https:// gluebenchmark.com/ and SQuAD data can be downloaded at https://rajpurkar . github.io/SQuAD-explorer/.

Table 1 shows the results of few - shot text classification , from which we have 3 observations . First , among the baselines with 770 M parameters , simply further training the model on our corpus with language modeling improves the performance ( ExtraLM ) . This is likely due to the higher domain diversity of our corpus . MetaICL is helpful on most datasets , which verifies the effectiveness of meta - training for ICL . Self - Sup fails to bring benefits on most datasets against VanillaICL , probably because the constrained label space of the Classification training task ( only contains " True " and " False " ) brings bias to the model 's output . This emphasizes the importance of using training objectives with little bias .

Recent work improves upon these pretrained models by adding cross - lingual tasks leveraging parallel data that always involve English . Conneau and Lample ( 2019 ) pretrain a new Transformerbased ( Vaswani et al . , 2017 ) model from scratch with an MLM objective on monolingual data , and a Translation Language Modeling ( TLM ) objective on parallel data . Cao et al . ( 2020 ) align mBERT embeddings in a post - hoc manner : They first apply a statistical toolkit , FastAlign ( Dyer et al . , 2013 ) , to create word alignments on parallel sentences . Then , mBERT is tuned via minimizing the mean squared error between the embeddings of English words and those of the corresponding words in other languages . Such post - hoc approach suffers from the limitations of word - alignment toolkits : ( 1 ) the noises from FastAlign can lead to error propagation to the rest of the pipeline ; ( 2 ) FastAlign mainly creates the alignments with word - level translation and usually overlooks the contextual semantic compositions . As a result , the tuned mBERT is biased to shallow cross - lingual correspondence . Importantly , both approaches only involve word - level alignment tasks .

In the error detection problem , we have to detect data points with wrong labels . Given a ( potentially noisy ) dataset Z , we have to rank data points in Z by how likely they are erroneous . Removing or correcting errors improves the performance and robustness of models trained on that dataset .

We have also done error analysis of model performance on original Hindi test data already present in XNLI and data obtained through translations from IndicTrans in Figure 3 ( Appendix ) . We observe a total of 82 % overlap in error consistency , 3 Further Analysis in appendix § F and we observe that the greatest number of correct overlaps is for the entailment label , whereas the greatest number of incorrect predictions is for the contradiction label . We see the maximum overlap in neutral prediction and the least overlap in contradiction prediction in terms of consistency . This demonstrates that the model performs identically on both the original Hindi data and the machinetranslated Hindi data , bolstering the legitimacy of our dataset .

Moreover , some advanced techniques are proposed to improve embedding models , such as graph encoders ( Schlichtkrull et al . , 2018 ; Shang et al . , 2019 ; Vashishth et al . , 2020 ; and regularizers ( Lacroix et al . , 2018b ) . Note that our proposals are orthogonal to these techniques , and one can integrate them for better performance .

7 Tes Bahasa Indonesia sebagai Bahasa Asing ( https : / / lbifib.ui.ac.id / archives / 105 ) We use TOPIK 8 for Korean , UKBI 9 and BIPA 10 for Indonesian , and TOEIC 11 and GRE 12 for English . We chose nouns and verbs from annotation questions and created multiple - choice questions whose answers are the nouns or the verbs in the annotation questions .

where x k is the value of the neuron k , g is a nonlinear activation function , w jk and b k are weights and bias in the network , respectively . We can see that the contribution of a single node j to the value of the node k is

To compare the model - level performance of evaluation methods , we leverage two ad - hoc training schema to synthesize a series of models with different capability ranks . Then , the evaluation methods are used to predict the ranking of trained models . Seven non - factual and factual evaluation methods have been examined , followed by a detailed discussion of their properties . The effectiveness of FacEval is also proven by showing a strong correlation with the factual correctness of summarization models .

x ⇒ ⟨aspect⟩ a ⟨opinion⟩ o ASTE :

Datasets . We conduct experiments on English - Macedonian ( En - Mk ) and English - Albanian ( En - Sq ) , as Mk , Sq are low - resource languages , where lexical - level alignment can be most beneficial . We use 3 K randomly sampled sentences of SETIMES ( Tiedemann , 2012 ) as validation / test sets . We also use 68 M En sentences from NewsCrawl . For Sq and Mk we use all the CommonCrawl corpora from Ortiz Suárez et al . ( 2019 ) , which are 4 M Sq and 2.4 M Mk sentences .

Is our query q effective ? To answer this question , we conduct experiments with three types of queries . The first is the regular query adopted in our COM - MRC . The second is an improper query by removing the keyword " first " . The third is

which means no query is provided . The experimental re- sults are reported in Table 9 . The performance of the improper query decreases by a mean 1.26 % . Compared with the improper query , a

query drops much more with a mean decrement of 2.60 % . This shows the effectiveness of our query .

• hash : Unique identifier .

We represent a student as a temporally - evolving sequence of questions and their responses . As in much previous KT work , we represent the student response as simply correct / incorrect , with special tokens < Y > and < N > . A student 's current state is thus represented as a sequence of all past question and response pairs :

We now consider the second case : Suppose we have a cipher and a key , but the cipher is nondeterministic . This case can arise in practice when the key of the cipher is found while combing through historical archives , for example . Alternatively , the key could have been found by a cryptanalyst by solving a part of the cipher . Although the cipher key exists in these scenarios , the nondeterministic segmentation makes it impossible to directly apply the key to recover the plaintext ; recall the ambiguous segmentation example of the word and from Section 2.4 . In this case , it is very challenging to manually recover the whole plaintext , especially when the cipher is very long .

We propose a new global ED model based on BERT .

However , controllable argument generation can also be used to support finding and formulating ( counter-)arguments for debates , for writing essays , to enrich one - sided discussions , and thus , to make discourse more diverse overall . For instance , anticipating opposing arguments is crucial for critical thinking , which is the foundation for any democratic society . The skill is extensively taught in school and university education . However , confirmation bias ( or myside bias ) ( Stanovich et al . , 2013 ) , i.e. the tendency to ignore opposing arguments , is an ever - present issue . Technologies like ours could be used to mitigate this issue by , for instance , automatically providing topic - and aspectspecific counter - arguments for all arguments of a given text ( this has been shown for single arguments in Section 7.2 ) . We believe that working on and providing access to such models is of major importance and , overall , a benefit to society .

Data Privacy and Bias : This research mainly focuses on translation using the Europarl ( Koehn , 2005 ) corpus , which is widely adopted in the community . There are no data privacy issues or bias against certain demographics with regard to this dataset .

We evaluated the generality and effectiveness of our approach on the large - scale DocRED dataset . Experimental results show that the proposed approach combines well with various recent DocRE models and significantly improves the performance . We further evaluated our approach on a dialogue relation extraction dataset , DialogRE ( Yu et al . , 2020 ) ; our SIEF yields consistent improvement , showing the generality of our approach in different domains .

m=1 are retrieved based on certain criterion C and NMT models the conditional probability of target sentence y conditioned on both source sentence x and translation memories M in a left - to - right manner :

We now use the scalar mixes of the role labeling probes as target tasks , and lower - level probes as anchor tasks to qualitatively explore the differences between how our role probes learn to represent predicates and semantic arguments 5 ( Fig . 3 ) . The results reveal a distinctive pattern that confirms our previous observations : while Verb - Net and FrameNet predicate layer utilization src is similar to the scalar mixes learned for ttype and lex.unit , the learned argument representations tgt and the PropBank predicate attend to the layers associated with dependency relation and POS probes . Aside from the PropBank predicate encoding which we address below , the pattern reproduces for English and German . This aligns with the traditional separation of the semantic role labeling task into predicate disambiguation followed by semantic argument identification and labeling , along with the feature sets employed for these tasks ( Björkelund et al . , 2009 ) . Note that the observation about the pipeline - like task processing within the BERT encoders thereby holds , albeit on a sub - task level .

Moreover , we observe that a DocRE model , trained on the entire document , may err when non - evidence sentences are removed . In Figure 1 , for example , we need to identify the relation " MemberOf " between the entities Brad Wilk and Rage Against the Machine . The evidence sentences are { 1,2 } , and humans can easily identify such a relation when reading sentences { 1,2 } only . However , the recent DocRE model GAIN identifies the relation " MemberOf " correctly from the entire document { 1,2,3 } , but predicts " not MemberOf " from sentences { 1,2 } . Intuitively , removing sentence { 3 } should not change the result , as this sentence does not provide information regarding whether " MemberOf " holds or not for the two entities . Such model behaviors are undesired , because it shows that the model is not robust and lacks interpretability .

We focus on reducing gender bias of CNN models trained on two datasets -Biosbias ( De - Arteaga et al . , 2019 ) and Waseem ( Waseem and Hovy , 2016 ) . For Biosbias , the task is predicting the occupation of a given bio paragraph , i.e. , whether the person is ' a surgeon ' ( class 0 ) or ' a nurse ' ( class 1 ) . Due to the gender imbalance in each occupation , a classifier usually exploits gender information when making predictions . As a result , bios of female surgeons and male nurses are often misclassified . For Waseem , the task is abusive language detection -assessing if a given text is abusive ( class 1 ) or not abusive ( class 0 ) . Previous work found that this dataset contains a strong negative bias against females ( Park et al . , 2018 ) . In other words , texts related to females are usually classified as abusive although the texts themselves are not abusive at all . Also , we tested the models , trained on the Waseem dataset , using another abusive language detection dataset , Wikitoxic ( Thain et al . , 2017 ) , to assess generalizability of the models . To quantify gender biases , we adopted two metrics -false positive equality difference ( FPED ) and false negative equality difference ( FNED ) ( Dixon et al . , 2018 ) .

Hyperparameters and training . The BiLSTM models are trained with the Adam optimizer ( Kingma and Ba , 2015 ) with a learning rate of 1e-3 . For fine - tuning the original BERT models , we follow the configuration published by Wolf et al . ( 2019 ) and use AdamW ( Loshchilov and Hutter , 2019 ) as optimizer and a learning rate of 4e-7 for sentence classification and 1e-5 for sequence tagging . When adding BERT tokens to the BiLSTM , we also use the AdamW optimizer for the whole model and learning rates of 4e-7 or 1e-5 for the BERT part and 1e-3 for the remainder . For regularization , we employ early stopping on the development set . We use a stacked BiLSTM with two hidden layers and 500 hidden units for all tasks with the exception of the experiment sentence de- tection task , where we found one BiLSTM layer to work best . The attention layer of the sentence detection model has a hidden size of 100 .

The destruction of a President with its collapse of executive authority was too staggering to contemplate .

U : What about a skirt to go with the grey jacket in front of me ?

Hyperparameters . We fine - tune all of our NMT models for 4 epochs with a batch size of 4 and a warmup rate of 0.2 . To avoid over - fitting , we set the early stopping patience on the validation set as 2 . In the context construction , we randomly select 5 cell values for each target column . The Adam optimizer ( Kingma and Ba , 2015 ) with ß1 = 0.9 , ß2 = 0.99 and ✏ = 1e-8 is adopted . We set the number of relation - aware layers as 2 , and we set the learning rate of the decoder and the relational aware layers as 3e-5 , and decrease the learning rate of the Transformer encoder to 4 times and 8 times smaller for M2M-100 and MBart-50M2 M respectively .

For our work , we consider the source and target languages as the X to Z domains to be aligned . Each training sample corresponds to a data point in a distribution and is represented by its sentencelevel encoding h _ 0 . Following prior work ( Pouran Ben Veyseh and Nguyen , 2022 ) , we estimate probability distributions P ( x ) and P ( z ) using a singlelayer FFNN and use Euclidean distance as the cost function :

( 1 ) R2 : Home State . We connect political actors with their home states with R2 :

Our goal is to construct a large - scale and generalpurpose dataset for extracting hyper - relational facts from natural language text . However , it is seldom practical to assume to have an ample amount of high - quality labeled samples in real applications , especially for complex tasks such as information extraction . Hence , we propose a weakly supervised ( Craven and Kumlien , 1999 ) data setting which enables us to collect a larger and more diverse training set than would be otherwise possible . To minimize the effect of noisy samples in evaluation , we then perform human annotation for a portion of the collected data and allocate it as the held - out set . In the following sections , we first introduce the process of collecting the distantly supervised data , followed by the human - annotated data portion .

where dz ) . Shaw et al . ( 2018 ) proposes an extension to selfattention to consider the pairwise relationships between input tokens by changing Equation ( 1 ) as follows :

Then we disabled features based on their average quality scores . The assumption was : if the scores of the disabled features correlated with the drop in the model predictive performance , it meant that humans could understand and accurately assess CNN features using word clouds . We used small training datasets so that the trained CNNs had features with different levels of quality . Some features detected useful patterns , while others overfitted the training data .

The detailed annotation results are shown in Figure 1 . There are several exciting findings : 1 ) the human annotations contain non - negligible factual errors at around 17 % ; 2 ) 36 % to 50 % of generated summaries from dialogue summarization models contain at least one factual error ; 3 ) three advanced dialogue summarization models perform worse than their baseline on factual consistency .

The optimal policy ensures that the SiMT model gets good latency - quality trade - offs ( Iranzo - Sánchez et al . , 2021 ) . The translation model plays a key role in searching for the optimal policy by identifying the number of source tokens to be read , maximizing the gain for the current translation . However , considering all possible numbers of source tokens for each target token would be computationally expensive and may not effectively balance latency and translation quality ( Zhang and Feng , 2023b ) . To address this issue , we employ binary search to determine the ideal number of source tokens to be read for each target token by evaluating the midpoint concavity of the interval .

To make the relevance propagation more stable , we add a small positive number ( as a stabilizer ) to the denominator of the propagation rule :

In - context learning , as first proposed by Brown et al . ( 2020 ) , is a straightforward parameter - free approach , where the downstream task of interest is expressed as natural text demonstrations and used to conditionally generate from a language model . Recently , Min et al . ( 2022a ) proposed Noisy Channel ( denoted as Channel ) that exploits the language generation capability of language models for discriminative tasks using the Bayes ' Theorem . We compare the two ICL methods on all three ( sensitivity , GLER , and the ground - truth label ICL accuracy ) measures .

As explained earlier , we want to know whether the learned features are valid and relevant to the classification task and whether or not they get appropriate weights from the next layer . This is possible by letting humans consider the word cloud(s ) of each feature and tell us which class the feature is relevant to . A word cloud receiving human answers that are different from the class it should support ( as indicated by W ) exhibits a flaw in the model . For example , if the word cloud in Figure 2 represents the feature f i in a sentiment analysis task but the i th column of W implies that f i supports the negative sentiment class , we know the model is not correct here . If this word cloud appears in a product categorization task , this is also problematic because the phrases in the word cloud are not discriminative of any product category . Hence , we provide options for the users to disable the features which correspond to any problematic word clouds so that the features do not play a role in the classification . To enable this to happen , we modify M c to be M c where p = M c ( f ) = softmax((W Q)f + b ) and Q ∈ R |C|×d is a masking matrix with being an element - wise multiplication operator . Initially , all elements in Q are ones which enable all the connections between the features and the output . To disable feature f i , we set the i th column of Q to be a zero vector . After disabling features , we then freeze the parameters of M f and fine - tune the parameters of M c ( except the masking matrix Q ) with the original training dataset D in the final step .

We thank John Hewitt , Yuhao Zhang , Ashwin Paranjape , Sergey Levine , and the anonymous reviewers for their thoughtful comments and suggestions . Kevin is supported by a Google PhD Fellowship .

As described in Section 2 , the data set curated by Mysore et al . ( 2019 ) contains 230 synthesis procedures annotated with entity type information . 11 We apply our models to this entity extraction task in order to estimate the degree of transferability of our findings to similar data sets . To the best of 11 our knowledge , there have not yet been any publications on the automatic modeling of this data set . We hence compare to the previous work of Mysore et al . ( 2017 ) , who perform action graph induction on a similar data set . 12 Our implementation of BiLSTM - CRF mat2vec+word2vec roughly corresponds to their BiLSTM - CRF system .

Baselines . We compare PROGRAMFC to seven baselines , categorized into three groups . ( i ) Pretrained models : BERT - FC ( Soleimani et al . , 2020 ) and LisT5 ( Jiang et al . , 2021 ) are two models that leverage BERT and T5 for fact verification , respectively . ( ii ) FC / NLI fine - tuned models : we choose three pretrained models that are fine - tuned on other fact - checking datasets or natural language inference ( NLI ) datasets . RoBERTa - NLI ( Nie et al . , 2020 ) uses fine - tuned RoBERTa - large on four NLI datasets ; DeBERTaV3 - NLI ( He et al . , 2021 ) We use these examples either for fine - tuning pre - trained models ( BERT - FC and LisT5 ) , for continuous fine - tuning the FC / NLI fine - tuned models , or as in - context examples for FLAN - T5 and Codex . For PROGRAMFC , we use them as in - context examples for reasoning program generation .

Frequency of recall . We find that the more an event is thought or talked about ( i.e. , higher FRE - QUENCYOFRECALL ) , the more linearly its story flows ( ∆ l ; |β| = 0.07 , p < 0.001 ) , and the fewer realis events ( |β| = 0.09 , p < 0.001 ) it contains .

Knowledge - grounded open - domain dialogue generation is crucial for building a knowledgeable dialogue system , which is beyond the wildest dreams in natural language process field . All our experiments are conducted on public available datasets to avoid ethical concerns . All terms for using these datasets are strictly followed in our study . There are no direct ethical concerns in our research .

The controller is a sequence generation model whose input is a linearized state and whose outputs are actions . The input sequence is the concatenation of H , T p , and X. The linearized T p is a series of steps , where the step premises are connected with " & " and the step conclusion comes after " →. " For each state , the controller predicts multiple candidate actions and their likelihood scores .

Pre - trained models have shown very good performances on a number of question answering benchmarks especially when fine - tuned on multiple question answering datasets at once . In this work , we propose an approach for generating a fine - tuning dataset thanks to a rule - based algorithm that generates questions and answers from unannotated sentences . We show that the state - of - the - art model UnifiedQA can greatly benefit from such a system on a multiple - choice benchmark about physics , biology and chemistry it has never been trained on . We further show that improved performances may be obtained by selecting the most challenging distractors ( wrong answers ) , with a dedicated ranker based on a pretrained RoBERTa model .

In this paper , we limit the proposed WhitenedCSE for sentence embedding learning . Conceptually , WhitenedCSE is potential to benefit contrastive learning on some other tasks , e.g. , self - supervised image representation learning and self - supervised vision - language contrastive learning . However , we did not investigate the self - supervised image representation learning because this domain is currently dominated by masked image modeling . We will consider extending WhitenedCSE for visionlanguage contrastive learning when we have sufficient training resources for the extraordinary largescale text - image pairs . Since the number of pages in the text is limited and our model does not have significant potential risks , we do not discuss this .

where L ( ) is a twice - differentiable loss function andη is the learnable learning rate of the model , which is optimized together withD. Given initial model parameters θ 0 , we can represent the model trained with the distilled datasetD , with the number of GD steps T , as

By adding adversarial perturbations to word embeddings , FreeLB generates virtual adversarial samples inside the region around input samples .

For CNNs , the classifiers we experiment with in this paper , each feature has one word cloud containing the n - grams , from the training examples , which were selected by the max - pooling of the CNNs . For instance , Figure 2 , corresponding to a feature of filter size 2 , shows bi - grams ( e.g. , " love love " , " love my " , " loves his " , etc . ) whose font size corresponds to the feature values of the bi - grams . This is similar to how previous works analyze CNN features ( Jacovi et al . , 2018;Lertvittayakumjorn and Toni , 2019 ) , and it is equivalent to back - propagating the feature values to the input using LRP and cropping the consecutive input words with non - zero LRP scores to show in the word clouds .

• stance : Original stance label of the argument towards the topic , taken from the UKP - Corpus ( Stab et al . , 2018b ) . Either " Argument_for " or " Argument_against " .

In this subsection , we apply SACL framework to the task of emotion recognition in conversations ( ERC ) , and present a sequence - based method SACL - LSTM . The overall architecture is illustrated in Figure 3 . With the guidance of SACL with CAT , the method can learn label - consistent and context - robust emotional features for better emotion recognition .

Generation At inference , we gather multiple generated arguments from a control code input by splitting the generated output text into sentences with NLTK ( Bird et al . , 2009 ) . We observe that for the first generated argument , the Arg - CTRL mostly outputs very short phrases , as it tries to incorporate the control code into a meaningful start of an argument . We prevent this by adding punctuation marks after each control code ( e.g. a period or colon ) , signaling the model to start a new sentence . In this fashion , we generate proand con - arguments up to the pre - defined training split size 7 for each topic of the UKP - Corpus , resulting in 7,991 newly generated arguments . We do this with both models and use the generated arguments as a basis for the following analysis and evaluation methods . Examples of generated arguments can be found in Tables 4 , 6 , and 7 ( as part of the evaluation , see Section 7 ) .

Aspect Detection We detect aspects on all remaining arguments . To speed up the detection on millions of sentences , we use BERT BASE instead of BERT LARGE ( see Table 3 ) .

Language Models We use two language models to train our proposed modified MMA model . Firstly , we use the pretrained XLM - RoBERTa ( Conneau et al . , 2019 ) model from Huggingface Transformers 1 model repository . Since the LM output can be very open - ended and might not directly suit / cater to our task and dataset , we finetune the head of the model using the MuST - C target text data for each task .

Formally , the NCE loss L(θ ) is

For the abbreviation headers , when translating " OS " ( the abbreviation of operation system ) and " Jan " ( the abbreviation of January ) , both Base and H2H fail to get the correct result . However , being aware of the context of " Jan " ( e.g. , Feb , Mar and Apr , etc . ) and " OS " ( e.g. , Computer , System , and Core , etc . ) , H2H+CXT and CAST can better understand and translate the abbreviations .

We propose a method for generating multiplechoice questions in order to fine - tune and improve UnifiedQA . This process is based on 3 steps . First , a set of sentences is being selected ( Section 2.1 ) from which a generic question generation system is applied ( Section 2.2 ) . Then a number of distractors are added to each question ( Section 2.3 ) .

This paper presents a new challenging information extraction task in the domain of materials science . We develop an annotation scheme for marking information on experiments related to solid oxide fuel cells in scientific publications , such as involved materials and measurement conditions . With this paper , we publish our annotation guidelines , as well as our SOFC - Exp corpus consisting of 45 openaccess scholarly articles annotated by domain experts . A corpus and an inter - annotator agreement study demonstrate the complexity of the suggested named entity recognition and slot filling tasks as well as high annotation quality . We also present strong neural - network based models for a variety of tasks that can be addressed on the basis of our new data set . On all tasks , using BERT embeddings leads to large performance gains , but with increasing task complexity , adding a recurrent neural network on top seems beneficial . Our models will serve as competitive baselines in future work , and analysis of their performance highlights difficult cases when modeling the data and suggests promising research directions .

To investigate , we identify a set of heuristics parameterized by ϕ(p ) and c , and characterized by the presence of a set of heuristic - specific Medical Subject Headings ( MeSH ) linked entities in the premise and hypothesis of each heuristic - satisfying example . These heuristics are described below ; specific MeSH features are detailed in the Appendix .

Figure 2 ( c ) shows an example . When the text inputs are given a prompt that is unlikely to be used in sentiment analysis texts , " [ yellow / green ] black . " , the data from different classes is not well separated in the feature space ( as compared to Figure 2 ( b ) ) . We believe that this is because models rarely encounter the text " yellow black " or " green black " prefixed in a sentiment - bearing text in the pretraining corpora , and that this language discrepancy limits the model 's ability to effectively represent the data . In contrast , expressions like " [ very / not ] pleased . " ( Figure 2 ( b ) ) are often used in context related to emotions and therefore appear more frequently together with sentiment - bearing text in the pre - training corpora . This makes it easier for the model to form a useful pre - trained feature space .

We report results on the test set of XNLI and MLQA and we do hyperparameter searching on the development set . All the experiments for translatetrain were done using the code - switching technique introduced in Section 2 .

open - access scientific publications about SOFCs and related research , annotated by domain experts .

We need a variety of different metrics to understand the effectiveness of our planning - LM approach .

Table 6 show examples in BOLD and StereoSet that probe for bias against a particular group , but differ in language due to the difference in sources the datasets are from .

For example , consider a multi - path local hierarchy with four labels : 1a , 1b , 2a and 2b , where 2a and 2b are the child labels of 1a and 1b , respectively . According to Eq . 5 , we can get u 1 = ŷ 1a + ŷ 1b and u 2 = ŷ 2a + ŷ 2b . The attention score α 21 in BERT can be calculated as :

Context Aware Schema Annotation . To reduce the translation effort , we first use Google translator 1 to automatically translate the English headers to five target languages , header by header . Then based on the Google translations , we recruit three professional translators for each language to manually check and modify the translations if inappropriate .

Figure 4 shows the distribution of average feature scores from one of the three CNN instances for the Yelp dataset . Examples of the word clouds from each rank are displayed in Figure 5 . We can clearly see dissimilar qualities of the three features . Some participants answered that the rank B feature in Figure 5 was relevant to the positive class ( probably due to the word ' delicious ' ) , and the weights of this feature in W agreed ( Positive : Negative = 0.137:-0.135 ) . Interestingly , the rank C feature in Figure 5 got a negative score because some participants believed that this word cloud was relevant to the positive class , but actually the model used this feature as evidence for the negative class ( Positive : Negative = 0.209:0.385 ) .

With the pre - trained token - level representations from BERT ( second half of Table 2 ) , the best model is BERT - BiRNN+Vis - annot - mult , achieving a Pear- BERT - BiRNN ) and their respective top-3 best performing multimodal variants ( + Vis ) . We refer the reader to the Appendix for the full set of results . Here , BERT , ann - mul and emb - mul2 correspond to the BERT - BiRNN , the BERT - BiRNN+Vis - annot - mult and the BiRNN+Vis - embed - mult2 models of Table 2 .

We now turn to the object of our investigation : role semantics . For further discussion , consider the following synthetic example :

we focus on the English language . As of the writing of this paper , there is no equivalent of Simple English Wikipedia for other languages on Wikipedia , and creating similar resources for other languages would require finding other resources .

The feature - based textual features contain 15 numerical scores , while the visual feature vector contains 4,096 dimensions . To avoid over - weighting the visual features , we reduce their dimensionality using Principal Component Analysis ( PCA ) . We consider up to 15 principal components in order to keep a balance between the visual features and the 15 text features from QuEst++ . We choose the final number of principal components to keep according to the explained variance with the PCA , so this number is treated as a hyperparameter . After analysing the explained variance for up to 15 kept principal components ( see Figure 4 in Appendix ) , we selected six numbers of principal components to train QE models with ( 1 , 2 , 3 , 5 , 10 , and 15 ) . As fusion strategy , we concatenate the two feature vectors .

( 2 ) the dialog can stay in the same node when the user asks for a clarification and the agent refers to an FAQ to answer it , and ( 3 ) the dialog can randomly jump to any other nodes in the flowchart with a very low probability . Papineni et al . , 2002 ) and perplexity to evaluate generation performance . As we have the labels for the documents over which the responses are grounded , we measure the performance of the retriever using the standard recall @ 1 ( R @ 1 ) and a task - specific metric called success rate ( SR ) ( Raghu et al . , 2021 ) . The success rate measures the fraction of the test dialogs for which the system retrieved the correct document for all the agent utterances in the dialog .

Media - source ideology values are from { Very liberal , Liberal , Slightly liberal , Moderate , Slightly conservative , Conservative , Very conservative , Un - known } . Post - hoc conversion : We further convert finegrained labels obtained in steps 3 through step 5 to coarse - grained labels according to the nature of each task . For sentiment annotation , we convert them as 3 - way labels . Specifically , we convert very positive and positive into one positive category , and similarly for very negative and negative . Then we merge slightly positive , neutral , and slightly negative into neutral . For ideological labels obtained in steps 4 and 5 , in light of the 5 - way annotation provided by AllSides , we also convert ours as 5way labels by merging very liberal and liberal into liberal , and similarly for very conservative and conservative .

• Entailment Module . We follow to implement the entailment module in a prefixed manner . All types of modules are implemented with a single T5 - large model ( Raffel et al . , 2020 ) . A type - specific prefix ( e.g. , " deductive substitution : " ) is added to specify which type of reasoning the model should perform . The model is trained on the entailment steps of the Entailment - Bank training split . The reason type labels of steps are provided by . Following Yang et al . ( 2022 ) , we take the hypothesis as additional input to encourage the module to generate a more relevant conclusion . The module is trained with a learning rate of 3e-5 and a batch size of 20 following .

El término viene de la palabra " ghost " , que en inglés es ' fantasma '

Here , N denotes the batch size , ( x i , x + i ) denotes two semantically similar sentences , and h i = E ( x i ) is the sentence embedding from encoder E. The key to using this loss is how to define the semantically similar pairs , which we elaborate on in the following section .

Comparing our model performance using vary- et al . ( 2020 ) . L is the number of Transformer layers , H m is the hidden size , H f f is the dimension of the feed - forward layer , A is the number of attention heads , and V is the vocabulary size .

Argument Generation Early approaches rely on rules from argumentation theory and user preference models ( Carenini and Moore , 2006;Zukerman et al . , 1998 ) . In a more recent work , Sato et al . ( 2015 ) construct rules to find arguments in a large data source , which are then filtered and ordered with a neural network based ranker . Baff et al . ( 2019 ) use a clustering and regression approach to assemble discourse units ( major claims , pro and con statements ) to argumentative texts . However , most of these approaches rely on hand - crafted features and do not generalize well . Moreover , they all require permanent access to large data sources and are not able to generate new arguments .

As the saying goes , " a chart is worth a thousand words " . Nowadays , tremendous amounts of tabular data written in various languages are widely used in Wikipedia pages , research papers , finance reports , file systems , and databases , which are informative . Schema translation is the task of automatically translating headers of tabular data from one language to another . High - quality schema translation plays an essential role in cross - lingual table ⇤ Work done during an internship at Microsoft Research .

D5 . Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data ? No response .

mentions sequentially using words and already resolved entities ( see Figure 1 ) . This sequential inference effectively accumulates the global contextual information and enhances the coherence of disambiguation decisions . We conducted extensive experiments using six standard ED datasets , i.e. , AIDA - CoNLL , MSNBC , AQUAINT , ACE2004 , WNED - WIKI , and WNED - CWEB . As a result , the global contextual information consistently improved the performance . Furthermore , we achieved new state of the art on all datasets except for WNED - CWEB . The source code and model checkpoint are available at https://github.com/ studio - ousia / luke .

Therefore , the teacher 's output serves as a dynamic learning target for each sample . By updating based on the student 's feedback in advance , the teacher is able to reach a state that is optimal for the student 's learning . In this case , the teacher could provide an appropriate learning signal . Leveraging this updated supervision signal , the student could make up for the ability gap faster . For the other two updating orders , the teacher has n't updated yet , lacking of making trade - offs between the samples that are more beneficial for generalization and those that are more challenging to learn from . This may lead to a certain degree of lag in knowledge transfer , resulting in a larger entropy gap between the student and the teacher .

Probable Cause Heuristic This heuristic applies when the premise contains clinical condition(s ) , the target class is neutral , and the generated hypothesis provides a plausible , often subjective or behavioral , causal explanation for the condition , finding , or event described in the premise ( e.g. , associating altered mental status with drug overdose ) .

Finally , we investigate to what extent do components introduced by us help in linking when there is training data available that links to the inference KB , KB test . We hypothesize that while attributeseparators will still be useful , attribute - OOV and attribute - shuffle will be less useful as there is a smaller gap between training and test scenarios , reducing the need for regularization .

R4C ( Inoue et al . , 2020 ) . R4C is another recent multi - hop QA dataset containing annotated evidence paths . The dataset contains 4.6k examples ( 2.4k train , 2.2k development ) constructed on top of HotpotQA , where the authors used crowdsourcing efforts to collect the evidence paths in the form of simple subject - verb - object natural language sentences . Again , we randomly split the development set ( there 's no test set given ) into our development and test set ( 1.1k samples each ) . We use the question as our query q and use the annotated evidence sentences as C q .

Appendix for " Global Entity Disambiguation with BERT " A Details of Proposed Model

the National Natural Science Foundation of China ( 62076008 ) and the Key Project of Natural Science Foundation of China ( 61936012 ) .

The results also show that our approach is better than neglecting it . We believe that ignoring them will lead to insufficient generalization ability of the model . In general , it is problematic to employ vague instances in positive - negative partition , and our multi - level fashion can facilitate the learning of vague samples .

Topic Units ( Lan et al . , 2019 ) 67.9 − 68.2 UHop ( Chen et al . , 2019 ) 68.5 − − NSM ( Liang et al . , 2017 ) 69.0 − − ReTrack ( Chen et al . , 2021 ) 71.0 − 71.6 STAGG ( Yih et al . , 2015 ) 71.7 63.9 − CBR ( Das et al . , 2021 ) 72.8 70.0 − QGG ( Lan and Jiang , 2020 ) 74.0 − − RNG - KBQA ( Ours ) 75.6 71.1 − ing BERT - base - uncased , and the generator using T5 - base . We also sample 96 negative candidates for each question , and feed the top-5 candidates to the generation model . The ranker is trained for 10 epochs and we run bootstrapping every 2 epochs ; the generator is trained for 20 epochs .

Unsupervised curation . We consider explicitly mining AT pairs from vip - AnT. Because this zeroresource method uses no human supervision , we refer to it as " unsupervised curation . " Concretely , for each video segment in AudioSet , we extract a video frame , and input that frame to the original CLIP image encoder . Then , we encode a large Unsupervised ( Zero - resource ) AC Audio - focused Captions originate from the training captions of AudioCaps and Clotho . We perform caption retrieval by using CLIP and the prompt " the sound of " . ( 1080078 aligned pairs ) example A balloon is rubbed quickly and slowly to make squeaking sounds .

To the best of our knowledge , both of our sentiment analysis datasets are the first of their kind , as no movie reviews of comparable size were collected before . For instance , a similar corpus published by YTU Kemik NLP Group 9 contains reviews of mere 105 movies classified in only 3 classes ( negative , positive , and neutral ) . Considering that modern NLP techniques require large corpora , our movie reviews datasets are sufficiently large and can thus be meaningfully used by the Turkish NLP community .

The first analysis involves comparing all methods on long - term success rate , which measures the percentage of control words in generated simulated roll - outs . To do this , we train a separate user model with the training dataset . We perform a roll - out per test example with 10 generated system responses and 10 generated user responses and compute the percentage of control words in the generated system responses . When counting the number of generated words , we compare word stems .

Each of the role labeling formalisms offers certain advantages and disadvantages ( Giuglea and Moschitti , 2006;Mújdricza - Maydt et al . , 2016 ) . While being close to syntax and thereby easier to predict , PropBank does n't contribute much semantics to the representation . On the opposite side of the spectrum , FrameNet offers rich predicatesemantic representations for verbs and nouns , but suffers from high granularity and coverage gaps ( Hartmann et al . , 2017 ) . VerbNet takes a middle ground by following grammatical criteria while still encoding coarse - grained semantics , but only focuses on verbs and core ( not modifier ) roles . SPR avoids the granularity - generalization trade - off of the categorical inventories , but is yet to find its way into practical NLP applications .

A simple answer to this question is concatenation of the values v i , given by

Then we obtain an attribution score per word , ω ( i )

In this section , we empirically validate the effectiveness of our PQA - ColBERT , with respect to ranking performance on passage ranking task . We report results for a single run .

Figure 4 : Pool selection ( for one student ) suffers worse question quality vs. latency trade - off than question generation , especially for sampling difficult questions .

Definition 2 Let M and M ′ be two TDA Mapper graphs with vertices V = { C 1 , . . . , C n } ; V ′ = { C ′ 1 , . . . , C ′ m } ; and edges E and E ′ , respectively . If m ̸ = n , then empty set padding is added to the smaller vertex set so that m = n. The distance

A solid body of evidence suggests that encoders like BERT capture syntactic and lexical - semantic properties , but only few studies have considered probing for predicate - level semantics ( Tenney et al . , 2019b;Kovaleva et al . , 2019 ) . To the best of our knowledge we are the first to conduct a cross - formalism probing study on role semantics , thereby contributing to the line of research on how and whether pre - trained BERT encodes higher - level semantic phenomena .

We evaluate our method on various tasks including language modeling , word similarity , and machine translation . In the language modeling task , we focus on verifying the diversity of the generated texts .

We build an amortized explanation model for text classification in two stages . In the first stage , we construct a training set for the amortized model . We compute reliable explanation scores as the reference scores for training using the existing SV estimator . As shown in Section 4 , SVS-25 is the most stable SV estimator and we use it to obtain reference scores . In the second stage , we train a BERT - based amortized model that takes the text as input and outputs the explanation scores using MSE loss .

As presented in Table 3 , our models achieve enhanced performance for rare entities . Furthermore , the global models consistently outperform the local model both for rare and frequent entities .

Models . We experiment with 12 models in total . We include 6 language models ( Table 1 ) , all of which are decoder - only , dense LMs . We use each LM with two inference methods , direct and channel , following Min et al . ( 2021a ) . The sizes of LMs vary from 774 M to 175B. We include the Figure 3 : Results when using no - demonstrations , demonstrations with gold labels , and demonstrations with random labels in classification ( top ) and multi - choice tasks ( bottom ) . The first eight models are evaluated on 16 classification and 10 multi - choice datasets , and the last four models are evaluated on 3 classification and 3 multi - choice datasets . See Figure 11 for numbers comparable across all models . Model performance with random labels is very close to performance with gold labels ( more discussion in Section 4.1 ) .

The echo chamber objective is motivated by the echo chamber phenomenon ( Jamieson and Cappella , 2008 ; Barberá et al . , 2015 ) , where social entities tend to reinforce their narratives by forming small and closely connected interaction circles . We simulate echo chambers by assuming that neighboring nodes in the HIN have similar representations while non - neighboring nodes have different representations . We firstly define the positive and negative neighborhood of entity e i :

• Real life problems & societal implications ( e.g. , hallucinations , biases , future job market ) ;

The schema translation dataset presented in this work is a free and open resource for the community to study the newly proposed translation task . English tables collected are from three sources . First , we collect all tables from the WikiTableQuestions dataset ( Pasupat and Liang , 2015 ) , which is a free and open dataset for the research of question answering task on semi - structured HTML ta - bles . Since all of the tables are collected from open - access Wikipedia pages , there is no privacy issue . Second , we collect 176 English tables from the search engines which are also publicly available and do not contain personal data . To Further enlarge our dataset , we select all tables from the training set and development set of the Spider dataset ( Yu et al . , 2018 ) , which is also a free and open dataset for research use . Since the tables from the Spider dataset are mainly collected from openaccess online csv files , college database courses and SQL websites , there is no privacy issue either . For the translation step , we hire professional translators to translate the collected English tables to five target languages and the details can be found in Section 2 .

• NonSim -General . Any other edit that does not contribute to ( Lexical , Syntactic , Discourse , Semantic ) simplification , but does not fit in any other category .

We partition politicians into left , center , and right ideologies , containing those whose ideology score is less than -0.2 , between -0.2 and 0.2 , and above 0.2 , respectively . The distribution of these scores is shown in Figure 3 . Finally , we discard tweets without images , leaving 57,093 tweets from 1,422 politicians as our final evaluation dataset . More details are summarized in Table 2 .

Our probing kit spans a wide range of probing tasks , from primitive surface - level tasks mostly utilized as anchors later to high - level semantic tasks that language en de aim to provide a representational upper bound to predicate semantics . We follow the training , test and development splits from the original SR3de , CoNLL-2009 and SPR data . The XNLI task is sourced from the development set and only used for scalar mix analysis . To reduce the number of labels in some of the probing tasks , we collect frequency statistics over the corresponding training sets and only consider up to 250 most frequent labels . Below we define the tasks in order of their complexity , Table 2 provides the probing task statistics , Table 3 compares the categorical role labeling formalisms in terms of granularity , and Table 4 provides examples . We evaluate the classification performance using Accuracy , while regression tasks are scored via R 2 .

For each sample x i , the embedding is defined as follows :

Given a biased training dataset , a text classifier may absorb the biases and produce biased predictions against some sub - populations . We hypothesize that if the biases are captured by some of the learned features , we can apply FIND to disable such features and reduce the model biases .

While our work illustrates the impact of formalism using a single task and a single probing framework , the influence of linguistic formalism per se is likely to be present for any probing setup that builds upon linguistic material . An investigation of how , whether , and why formalisms and their implementations affect probing results for tasks beyond role labeling and for frameworks beyond edge probing constitutes an exciting avenue for future research .

VerbNet follows a different categorization scheme . Motivated by the regularities in verb behavior , Levin ( 1993 ) has introduced the group - ing of verbs into intersective classes ( ILC ) . This methodology has been adopted by VerbNet : for example , the VerbNet class get-13.5.1 would include verbs earn , fetch , gain etc . A verb in Verb - Net can belong to several classes corresponding to different senses ; each class is associated with a set of roles and licensed syntactic transformations . Unlike PropBank , VerbNet uses a set of approx . 30 thematic roles that have universal definitions and are shared among predicates , e.g. Agent , Beneficiary , Instrument .

Fine - tuned : This approach simply generates responses using the fine - tuned language model M .

We also analyze the importance of local hierarchyaware text encoder by comparing it with two methods : multi - label and seq2seq . For multi - label , we fine - tune BERT as the multi - label classifier . For seq2seq , we fine - tune BERT as the seq2seq model following s2s - ft , where target labels are sorted according to their levels in the global hierarchy . Local hierarchy - aware text encoder achieves the best performance in Table 4 .

Table 4 presents the results for the documentlevel feature - based and BiRNN neural QE models . 1 The first section shows the official models from the WMT'18 QE Task 4 report ( Specia et al . , 2018a ) . The neural - based approach SHEF - PT is the winning submission , outperforming another neural - based approach ( SHEF - mtl - bRNN ) . For our BiRNN models ( second section ) , BiRNN+Visembed - conc performs only slightly better than the monomodal baseline . For the feature - based models ( third section ) , on the other hand , the baseline monomodal QuEst++ is outperformed by various multimodal variants by a large margin , with the one with two principal components ( QuEst+Vis-2 ) performing the best . The more PCA components kept , the worse the results ( see Appendix for full set of results ) . Figure 3 shows the Williams significance test for document - level QuEst++ on the WMT'18 dataset .

In this section , we elaborate on how we extend SimCSE to multilingual and illustrate our proposed mSimCSE in Figure 2 . We explore four different multilingual training strategies , including the unsupervised strategy , the English NLI supervised strategy , the parallel NLI supervised strategy , and the fully supervised strategy . The difference between different strategies is how to define a positive training pair . Here , both unsupervised and English NLI supervised strategies can be recognized as an " unsupervised " setting for multilingual training because both of them only use English data and do not use any parallel data .

Our model consists of approximately 440 million parameters . To reduce the training time , the parameters that are shared with BERT are initialized using BERT . The other parameters are initialized randomly . The model is trained via iterations over Wikipedia pages in a random order for seven epochs . To stabilize the training , we update only those parameters that are randomly initialized ( i.e. , fixed the parameters initialized using BERT ) at the first epoch , and update all parameters in the remaining six epochs . We implement the model using PyTorch ( Paszke et al . , 2019 ) and Hugging Face Transformers ( Wolf et al . , 2020 ) , and the training takes approximately ten days using eight Tesla V100 GPUs . We optimize the model using AdamW. The hyper - parameters used in the training are detailed in Table 4 .

We define a column header as H i = hh 1 , . . . , h n i , where h j is the jth token of the header in the source language .

Electric can produce PLLs for all input tokens in a single pass like a LM while being bidirectional like a masked LM . We use the PLLs from Electric for re - ranking the 100 - best hypotheses of a 5 - layer BLSTMP model from ESPnet ( Watanabe et al . , 2018 ) on the 960 - hour LibriSpeech corpus ( Panayotov et al . , 2015 ) following the same experimental setup and using the same n - best lists as Salazar et al . ( 2020 ) . Given speech features s and speech recognition model f the re - ranked output is arg max

Our goal is to mitigate metric bias while maintaining a considerable performance for evaluating text generation . However , existing bias mitigation methods usually modify all parameters of the PLM and suffers from high computational cost and catastrophic forgetting ( French , 1993 ) , which may lead to degraded performance . Instead , following , we insert lightweight neural adapters ( Houlsby et al . , 2019 ; Pfeiffer et al . , 2021 ) into the PLM layers . By incorporating debiasing knowledge into the injected adapters while keeping the PLM parameters untouched , we can reduce the bias of interest in a plug - and - play style while retaining most of the original performance .

It is well known that MP is the key bottleneck for scaling MPNNs to large graphs ( Jin et al . , 2021 ; Zhang et al . , 2022a ; Zhao et al . , 2022

comparing both generation models to a retrieval approach as a strong upper bound . The retrieval approach returns all arguments from the classified training data ( see Section 4 ) that match a given topic , stance , and aspect . Both the retrieval and generation approaches are evaluated against reference data from debate portals and compared via METEOR ( Lavie and Agarwal , 2007 ) and ROUGE - L ( Lin , 2004 ) metrics . The retrieval approach has an advantage in this setup , as the arguments are also of human origin and aspects are always explicitly stated within a belonging argument .

To train a QG model to generate the questions that cover as many contextual information as possible , we use the question that contains the most contextual arguments as the ground truth . For the example in Figure 1 , we choose the question ' Who used jets in the attack in hills ? ' , because it contains two arguments : ' jets ' and ' hills ' , the other three candidate questions listed above contain one or zero arguments . If more than one candidate question contains the most contextual arguments , we then pick the first one . The input and output examples for the QG model are as follows :

p ∈ { [ xReact ] , [ xIntent ] , [ xWant ] , [ xNeed ] , [ xEffect ] }

( 2 ) The test KB used by these works is different from our test KB . Each entry in the KB used by prior work simply consists of the name of the entity with a textual description , while each entity in our KB is represented via multiple attribute - value pairs . ( 3 ) These models exploit the homogeneous nature of the KBs and usually pre - train models on millions of mentions from Wikipedia . This is beneficial when the training and test KBs are Wikipedia or similar , but is beyond the scope of this work , as we build models applicable to arbitrary databases .

• Modal - Trans Tang et al . , 2021 ) builds a cyclic sequence - to - sequence model and learns bidirectional reconstruction .

What are the important entities in this document ? What are the important dates in this document ? What events are happening in this document ? What is the result of these events ? Please answer the above questions :

• Further fine - tune the dialogue response generator trained with ground - truth partner personas to adapt to noisy partner personas generated by the partner personas generator .

Table 4 shows the results on the GCDC Clinton and TOEFL P1 datasets . We can observe from the table that eliminating any type of edges would hurt the performance . The decline in performance is more significant when removing the EDS than eliminating the ESS . The results are reasonable because edges between documents and subgraphs are the key to connecting documents with similar structures , while edges between subgraphs are considered to further assist it ( Kondor et al . , 2009 ) .

as ELECTRA 's ( Clark et al . , 2020 ) , which adds some additional ideas from on top of the BERT codebase , such as dynamic masking and removing the next - sentence prediction task . We use the weight sharing trick from ELECTRA , where the transformers producing the proposal distribution and the main transformer share token embeddings . We do not use whole - word or n - gram masking , although we believe it would improve results too . We did no hyperparameter tuning , directly using the hyperparameters from ELECTRA - Base for Electric and our baselines . These hyperparameters are slightly modified from the ones used in BERT ; for completeness , we show these values in Table 3 . The hidden sizes , feed - forward hidden sizes , and number of attention heads of the two transformers T LTR and T RTL used to produce the proposal distribution are 1/4 the size of Electric . We chose this value because it keeps the compute comparable to ELECTRA ; running two 1/4 - sized transformers takes roughly the same compute as running one 1/3sized transformer , which is the size of ELECTRA 's generator . To make the compute exactly equal , we train Electric for slightly fewer steps than ELEC - TRA . This same generator architecture was used for ELECTRA - TT . The TwoTower baseline consists of two transformers 2/3 the size of BERT 's , which takes approximately the same compute to run . The Electric models , ELECTRA - Base , and BERT - Base all use the same amount of pre - train compute ( e.g. , Electric is trained for fewer steps than BERT due to the extra compute from the proposal distribution ) , which equates to approximately three days of training on 16 TPUv2s .

Our annotation scheme comprises 16 slot types relevant for SOFC experiments . Here we explain a few of these types for illustration . A full list of these slot types can be found in Supplementary Material Table 11 ; detailed explanations are given in the annotation guidelines published along with our corpus . PowerDensity , Resistance , WorkingTemperature : These slots are generally filled by mentions of type VALUE , i.e. , a numerical value plus a unit . Our annotation guidelines give examples for relevant units and describe special cases . This enables any materials scientist , even if he / she is not an expert on SOFCs , to easily understand and apply our annotation guidelines .

In this section , we detail the different approaches employed for document re - ranking : kNN , Cross - Encoder , and Rank Fusion .

( 3 ) At inference , passing the control code [ Topic ] [ Stance ] [ Aspect ] to the model will generate an argument that follows these commands .

Five most frequent aspects ( frequency ) Gun control right ( 30 ) , protect ( 18 ) , background checks ( 17 ) , gun violence ( 14 ) , criminal ( 13 ) Death penalty cost ( 16 ) , innocent ( 12 ) , retribution ( 10 ) , murder rate ( 9 ) , deterrent ( 8) Abortion right ( 21 ) , pain ( 10 ) , choice ( 10 ) , right to life ( 9 ) , risk ( 9 ) Marijuana legalization dangerous ( 16 ) , cost ( 13 ) , risk ( 12 ) , harm ( 10 ) , black market ( 9 ) General aspects dangerous ( in 8 of 8 topics ) , cost / life / risk / safety ( in 7 of 8 topics ) cross - topic splits using a leave - one - topic - out strategy . The cross - topic setup allows us to estimate the ranker 's performance on unseen topics of the UKP - Corpus .

One solution to this problem would be to finetune or retrain these models with additionnal human annotated data . However , this is expensive both in time and resources . Instead , a lot of work has been done lately on automatically generating training data for fine - tuning or even training completely unsupervised models for question answering . One commonly used dataset for unsupervised question answering is the extractive dataset SQUAD ( Rajpurkar et al . , 2016 ) . proposed a question generation method for SQUAD using an unsupervised neural based translation method . Fabbri et al . ( 2020 ) and further gave improved unsupervised performances on SQUAD and showed that simple rulebased question generation could be as effective as the previously mentioned neural method . These approches are rarely applied to multiple - choice questions answering in part due to the difficulty of selecting distractors . A few research papers however proposed distractor selection methods for multiple - choice questions using either supervised approaches ( Sakaguchi et al . , 2013;Liang et al . , 2018 ) or general purpose knowledge bases ( Ren and Q. Zhu , 2021 ) .

Elimination of templates Prior work uses human - authored templates to transform the inputoutput pair to a natural language sentence ( Zhong et al . , 2021 ; Mishra et al . , 2022 ; Wei et al . , 2022 ; Chen et al . , 2022 ) . They require expensive manual effort ( as 136 different templates are required for 136 tasks in this paper ) and cause unstable model performance due to many different ways of writing ( Mishra et al . , 2021 ) . We eliminate templates , using the given input ( or a concatenation of inputs if there are multiple ) and label words provided in the original datasets . 4 A comparison of inputoutput schemes from prior work and our approach is shown in Table 4 .

On stability , DecT also has consistently low variance and some baselines ( ICL , RLPrompt and PromptBoosting ) are unstable . Given the difficulty of few - shot PTM adaptation , it is of great significance that the adaptation method is robust to random seeds .

Although DecT is an output - side adaptation method , the choice of templates also affects the final performance . To assess the influence of templates , we conduct experiments on AG 's News and SST2 and show results in ( Raffel et al . , 2020 ) . We run each experiment over 5 random seeds and report average accuracy and standard deviation ( % ) . mance , DecT largely moderates the gaps between them . Additionally , we try two templates searched from RLPrompt ( Deng et al . , 2022 ) and they both achieve satisfying results . On SST2 , the template from RLPrompt is even better than manually designed ones . Therefore , we highlight that DecT is complementary with input - side adaptation algorithms , and they can work together for better performance .

Concurrent to our work , Chi et al . ( 2020 ) , Feng et al . ( 2020 and also leverage variants of contrastive learning for cross - lingual alignment . We focus on a smaller model and improve on it using as little parallel data as possible . We also explore code - switching during finetuning on downtream tasks to complement the post - pretraining alignment objectives .

The opaqueness of large pre - trained models like BERT ( Devlin et al . , 2019 ) and GPT ( Radford and Narasimhan , 2018 ) motivates developing explanation methods , which aim to attribute importance to particular input features ( Springenberg et al . , 2015 ; Bach et al . , 2015 ; Ribeiro et al . , 2016 ; Sundararajan et al . , 2017 ) , such as words in a textual input . Two main criteria for evaluating such methods are plausibility and faithfulness ( Jacovi and Goldberg , 2020 ) . Plausibility can be defined as the consistency between explanations and human expectations , while faithfulness is defined as the consistency between explanations and the model 's underlying decision - making process .

We use the implementation provided by ( Akyurek and Andreas , 2021 ) , increasing the number of training iterations from 8k to 15k for augmented training runs in COGS , SCAN datasets . For the ALCHEMY dataset , we optimize iteration count over { 8k , 15k , 25k , 50k } based on validation accuracy , and found 25k to be optimal . For the CLEVR dataset , we optimize itreation count over { 8k , 15k , 25k , 50k } for CLEVR and CLEVR - COGENT dataset based on CLEVR 's validation accuracy .

We adopt a model equivalent to the one used to predict words in MLM . Formally , we predict the original entity corresponding to a masked entity by applying softmax over all entities :

Ideally , the task chosen for a cross - formalism study should be encoded in multiple formalisms using the same textual data to rule out the influence of the domain and text type . While many linguistic corpora contain several layers of linguistic information , having the same textual data annotated with multiple formalisms for the same task is rare . We focus on role semantics -a family of shallow semantic formalisms at the interface between syntax and propositional semantics that assign roles to the participants of natural language utterances , determining who did what to whom , where , when etc . Decades of research in theoretical linguistics have produced a range of rolesemantic frameworks that have been operationalized in NLP : syntax - driven PropBank ( Palmer et al . , 2005 ) , coarse - grained VerbNet ( Kipper - Schuler , 2005 ) , fine - grained FrameNet ( Baker et al . , 1998 ) , and , recently , decompositional Semantic Proto - Roles ( SPR ) ( Reisinger et al . , 2015;White et al . , 2016 ) . The SemLink project ( Bonial et al . , 2013 ) offers parallel annotation for PropBank , VerbNet and FrameNet for English . This allows us to isolate the object of our study : apart from the rolesemantic labels , the underlying data and conditions for the three formalisms are identical . SR3DE ( Mújdricza - Maydt et al . , 2016 ) provides compatible annotation in three formalisms for German , enabling cross - lingual validation of our results . Combined , these factors make role semantics an ideal target for our cross - formalism probing study .

In our paper , we address this problem by developing multilingual word - level QE models which perform competitively in different domains , MT types and language pairs . In addition , for the first time , we propose word - level QE as a zero - shot crosslingual transfer task , enabling new avenues of research in which multilingual models can be trained once and then serve a multitude of languages and domains . The main contributions of this paper are the following : i We introduce a simple architecture to perform word - level quality estimation that predicts the quality of the words in the source sentence , target sentence and the gaps in the target sentence .

Recently , there have been new attempts to use explanations and human feedback to debug classifiers in general . Some of them were tested on traditional text classifiers . For instance , Ribeiro et al . ( 2016 ) showed a set of LIME explanations for individual SVM predictions to humans and asked them to remove irrelevant words from the training data in subsequent training . The process was run for three rounds to iteratively improve the classifiers . Teso and Kersting ( 2019 ) proposed CAIPI , which is an explanatory interactive learning framework . At each iteration , it selects an unlabelled example to predict and explain to users using LIME , and the users respond by removing irrelevant features from the explanation . CAIPI then uses this feedback to generate augmented data and retrain the model . While these recent works use feedback on lowlevel features ( input words ) and individual predictions , our framework ( FIND ) uses feedback on the learned features with respect to the big picture of the model . This helps us avoid local decision pitfalls which usually occur in interactive machine learning ( Wu et al . , 2019 ) . Overall , what makes our contribution different from existing work is that ( i ) we collect the feedback on the model , not the individual predictions , and ( ii ) we target deep text classifiers which are more complex than the models used in previous work .

In this section , we describe our annotation scheme and guidelines for marking information on SOFCrelated experiments in scientific publications .

We conduct experiments of translating schema from English ( En ) to five different languages , including Chinese ( Zh ) , French ( Fr ) , German ( De ) , Spanish ( Es ) , and Japanese ( Ja ) . The performances of different translation models are listed in Table 4 .

We denote the number of XL PE equipped heads as τ ∈ { 0 , . . . , H } . To perform the attention calculation ,

The location of the hotel was excellent . The room was clean and comfortable .

Following Gururangan et al . ( 2018 ) , to identify tokens that occur disproportionately in hypotheses associated with a specific class , we compute tokenclass pointwise mutual information ( PMI ) with add-50 smoothing applied to raw counts , and a filter to exclude tokens appearing less than five times in the overall training dataset .

During training , we freeze the pre - trained model 's parameters θ and update only the prefix parameters ϕ to optimize the following objective :

The input representation of a word or an entity is constructed by summing the token , token type , and position embeddings ( see Figure 2 ):

doctor 's office doctor 's office wiki / Physical_examination wiki / Stethoscope wiki / Safe - cracking wiki / Safe wiki / Stethoscope

Besides that , PTW Masking also does well in several NLU tasks in GLUE . PTW Masking is a time - variant strategy , which aggregates the advantages of both Random - Token Masking and Named Entities Masking . From the start of pretraining , models can learn from all the words equiprobably like Random - Token Masking . And at the later stage , models memorize more knowledge by masking important words instead of wasting time on predicting meanless words . Thus models can have better NLU ability with memorization of more knowledge under the condition of training with the same number of tokens .

Intuitively , the forgetting of the backdoor in the retraining process must be related to the way in which the backdoor is injected . Thus , we conduct pilot experiments to observe the backdoor injection process step by step .

• Reconstruction stream is in charge of the source - side reconstruction . This stream instead captures the full contextual information y 1 : m but does not include the source context to avoid information leaking .

The dataset is constructed from Wikipedia and Wikidata , containing 3053 documents for training , 1000 for development , and 1000 for test . In total , it has 132,375 entities and 56,354 relational facts in 96 relation types . More than 40 % of the relational facts require reasoning over multiple sentences . The standard evaluation metrics are F1 and Ign F1 , where Ign F1 refers to the F1 score excluding the relational facts in the training set .

Table 4a shows human analysis over evidentiality positive and negative labels obtained by our method over randomly selected samples . In particular , we randomly sample 50 Natural Questions development questions and sample 2 positive passages and 2 negative passages ( if applicable ) with answer strings for each question . The authors manually analyze ( i ) if the positive passages actually provide sufficient evidence to answer , and ( ii ) if the negative passages actually do not provide sufficient evidence to answer , despite the existence of the gold answer strings . We found that in 95 % of the mined positive passages provide sufficient evidence to answer , while only 4 % of the negative passages do not ; in other words , the predictions are correct 95 % of the positive passages and 96 % of the negative passages .

Models are trained and results are reported on the subset of sentences marked as experimentdescribing in the gold standard , amounting to 4,590 entity mentions in total . 9 The CRF baseline achieves comparable or better results than the Bi - LSTM with word2vec and/or mat2vec embeddings . However , adding subword - based embeddings ( bpe and/or BERT ) significantly increases performance of the BiLSTM , indicating that there are many rare words . Again , the best results are obtained when using BERT or SciBERT embeddings or when using the original SciBERT model . It is relatively easy for all model variants to recognize VALUE as these mentions usually consist of a number and unit which the model can easily memorize . Recognizing the types MATERIAL and DEVICE , in contrast , is harder and may profit from using gazetteer - based extensions .

For training , the ground truth partner personas p is used and we train our generator to maximise the likelihood P ( p | s , c ) . We generate the complete partner personas profiles in an one - off shot for all the dialogue samples .

We use T5 - large ( Raffel et al . , 2020 ) as the backbone PLM for all methods , and consider the following baselines to compare with our MODULARPROMPT :

[ CONTEXT ] Based on the conversation above , did anyone answer

Compared to existing methods , HBGL achieves significant improvements on all three datasets , while only parameters corresponding to label embeddings are required except BERT .

Let us consider a text classification task with |C| classes where C is the set of all classes and let V be a set of unique words in the corpus ( the vocabulary ) .

The starting point ( and baselines ) for our work are the state - of - the - art models for zero - shot entity linking , which we briefly describe here ( Wu et al . , 2020;Logeswaran et al . , 2019 ) . 2 Candidate Generation Our baseline candidate generation approach relies on similarities between mentions and candidates in a vector space to identify the candidates for each mention ( Wu et al . , 2020 ) using two BERT models . The first BERT model encodes a mention m along with its context c into a vector representation v m . v m is obtained from the pooled representation captured by the [ CLS ] token used in BERT models to indicate the start of a sequence . In this encoder , a binary ( 0/1 ) indicator vector is used to identify the mention span . The embeddings for this indicator vector ( indicator embeddings ) are added to the token embeddings of the mention as in Logeswaran et al . ( 2019 ) .

Concerning the abusive language detection task , on average , the MTurk participants ' responses suggested us to disable 12 out of 30 CNN features . Unlike Biosbias , disabling features based on MTurk responses unexpectedly increased the gender bias for both Waseem and Wikitoxic datasets . However , we found one similar finding to Biosbias , that many of the CNN features captured n - grams which were both abusive and related to a gender such as ' these girls are terrible ' and ' of raping slave girls ' , and these features were not yet disabled . So , we asked one annotator to disable the features using the new " brutal " policy -disabling all which involved gender words even though some of them also detected abusive words . By disabling 18 out of 30 features on average , the gender biases were reduced for both datasets ( except FPED on Wikitoxic which stayed close to the original value ) . Another consequence was that we sacrificed 4 % and 1 % macro F1 on the Waseem and Wikitoxic datasets , respectively . This finding is consistent with ( Park et al . , 2018 ) that reducing the bias and maintaining the classification performance at the same time is very challenging .

This paper presents a new method for improving the zero - shot generalization of T5 - like text - to - text Transformers by incorporating model - generated signals in the pretraining process . METRO - T0 , the model sufficiently trained using our redesigned pretraining method , is highly parameter efficient and compute efficient . We hope that the success of our approach could inspire further work on efficient big LM pretraining and prompt - based learning .

Controllable language models like the CTRL ( Keskar et al . , 2019 ) allow to condition the model at training time to certain control codes . At inference , these can be used to direct the model 's output with regard to content or style . We build upon this architecture to control argument generation based solely on a given topic , stance , and argument aspect . For instance , to enforce focus on the aspect of cancer for the topic of nuclear energy , we input a control code " Nuclear Energy CON cancer " that creates a contra argument discussing this aspect , for instance : " Studies show that people living next to nuclear power plants have a higher risk of developing cancer . " .

Since the target texts contain lesser than 250k examples , we use additional data augmentation techniques to upsample the target data . We also use additional data to avoid overfitting on the MuST - C target text . Details have been provided in B.2.1 .

The micro F1 - score achieved by the fastText classifier significantly exceeds that of the majority class baseline , confirming the findings of Romanov and Shivade ( 2018 ) , who report a micro - F1 score of 61.9 but do not identify or analyze artifacts :

The BLEU scores of models with CTC , OAXE and CoCO loss on different languages pairs are shown in Table 4 .

• PubMed : We use the same pre - processing script in https : / / github.com / HHousen / A rXiv - PubMed - Sum . We remove the instances with article have less 3 sentences or abstract have less 2 sentences . We also remove three special tokens : newlines , < S > and < / S > .

Analyzing deep NLP models -There has been substantial work in gaining better understanding of complex , deep neural NLP models . By visualizing dense hidden vectors , Li et al . ( 2016 ) found that some dimensions of the final representation learned by recurrent neural networks capture the effect of intensification and negation in the input text . Karpathy et al . ( 2015 ) revealed the existence of interpretable cells in a character - level LSTM model for language modelling . For example , they found a cell acting as a line length counter and cells checking if the current letter is inside a parenthesis or a quote . Jacovi et al . ( 2018 ) presented interesting findings about CNNs for text classification including the fact that one convolutional filter may detect more than one n - gram pattern and may also suppress negative n - grams . Many recent papers studied several types of knowledge in BERT ( Devlin et al . , 2019 ) , a deep transformer - based model for language understanding , and found that syntactic information is mostly captured in the middle BERT layers while the final BERT layers are the most task - specific ( Rogers et al . , 2020 ) . Inspired by many findings , we make the assumption that each dimension of the final representation ( i.e. , the vector before the output layer ) captures patterns or qualities in the input which are useful for classification . Therefore , understanding the roles of these dimensions ( we refer to them as features ) is a prerequisite for effective human - in - the - loop model debugging , and we exploit an explanation method to gain such an understanding . Explaining predictions from text classifiers -Several methods have been devised to generate explanations supporting classifications in many forms , such as natural language texts , rules ( Ribeiro et al . , 2018 ) , extracted rationales ( Lei et al . , 2016 ) , and attribution scores ( Lertvittayakumjorn and Toni , 2019 ) . Some explanation methods , such as LIME ( Ribeiro et al . , 2016 ) and SHAP ( Lundberg and Lee , 2017 ) , are model - agnostic and do not require access to model parameters . Other methods access the model architectures and parameters to generate the explanations , such as DeepLIFT ( Shrikumar et al . , 2017 ) and LRP ( layer - wise relevance propagation ) ( Bach et al . , 2015;Arras et al . , 2016 ) . In this work , we use LRP to explain not the predictions but the learned features so as to expose the model behavior to humans and enable informed model debugging .

Figure 12 shows performance gap between using gold labels and using random labels per dataset . We find that the trend that the gap is smaller than previously thought is consistant across most datasets . Nonetheless , there are a few outlier datasets where performance gap is non - negligible , such as finan - cial_phrasebank and a few hate speech detection datasets . Future work may investigate on which tasks the model makes more use of the correctly paired training data .

We use TLM for word - level alignment . TLM is an extension of MLM that operates on bilingual data - parallel sentences are concatenated and MLM is applied to the combined bilingual sequence . Different from Conneau and Lample ( 2019 ) , we do not reset positional embeddings when forming the bilingual sequence , and we also do not use language embeddings . In addition , the order of S en i and S tr i during concatenation is determined by the random input shuffling from the sentence - level alignment step and we add a [ SEP ] token between S en i and S tr i . We randomly mask 15 % of the WordPiece tokens in each combined sequence . Masking is done by using a special [ MASK ] token 80 % of the times , a random token in the vocabulary 10 % of the times , and unchanged for the remaining 10 % . TLM is performed using the query encoder of MoCo . Our final PPA model is trained in a multi - task manner with both sentence - level objective and TLM :

In addition , we fine - tune the original ( uncased ) BERT ( Devlin et al . , 2019 ) as well as SciBERT ( Beltagy et al . , 2019 ) models on our dataset . Sci - BERT was trained on a large corpus of scientific text . We use the implementation of the BERT sentence classifier by Wolf et al . ( 2019 ) that uses the CLS token of BERT as input to the classification layer . 5 Finally , we compare the neural network models with traditional classification models , namely a support vector machine ( SVM ) and a logistic regression classifier . For both models , we use the following set of input features : bag - of - words vectors indicating which 1 - to 4 - grams and part - of - speech tags occur in the sentence . 6 Entity mention extraction . For entity and concept extraction , we use a sequence - tagging approach similar to ( Huang et al . , 2015;Lample et al . , 2016 ) , namely a BiLSTM model . We use the same input representation ( stacked embeddings ) as above , which are fed into a BiLSTM . The subsequent conditional random field ( CRF , Lafferty et al . , 2001 ) output layer extracts the most probable label sequence . To cope with multi - token entities , we convert the labels into BIO format .

Our approach is also amenable to online learning , as decisions about productivity are revised as more training data is evaluated . Replacing existing exceptions with more general rules when possible is concordant with Yang 's ( 2016 ) Maximize Productivity learning strategy , where the most general valid rule is adopted over narrower competitors .

D4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ?

Fig . 3 reports the results of different τ for Head XL SANs . With increasing of XL PE - informed heads , the best BLEU is achieved when # heads = 4 , which is therefore left as the default setting for HeadXL . Then , the BLEU score gradually decreases as the # System Architecture BLEU # Param . number of APE - informed heads decrease ( τ ↑ ) , indicating that sequential position embedding is still essential for SANs .

4 . We use community detection techniques to automatically identify regions of interest in large Mapper graphs .

To help better understand the domains of the collected tables , we firstly use a 44 - category ontology presented in Wikipedia : WikiProject Council / Directory as our domain category . Then we randomly sample 500 tables in the training set and manually label the domains . According to our statistics , our dataset covers all 44 domains . In detail , the Sports , Countries , Economics , and Music topics together comprise 44.6 % of our dataset , but the other 55.4 % is composed of broader topics such as Business , Education , Science , and Government .

We compare two End2End methods : First , we directly fine - tune a multi - lingual model on ⟨D src , S tgt ⟩ ( DialogSumX ) and ⟨ { Q tgt ; D src } , S tgt ⟩ ( QMSumX ) , marked as E2E ; Second , inspired by Bai et al . ( 2021 ) , where an End2End model first generates mono - lingual summary and then cross - lingual summary in an auto - regressive way and shows good performance in few - shot setting , we fine - tune a multi - lingual model on ⟨D src , { S src ; S tgt } ⟩ ( DialogSumX ) and ⟨ { Q tgt ; D src } , { S src ; S tgt } ⟩ ( QMSumX ) , marked as E2 M ( M means mixed ) .

Considering all the three runs , Figure 6 ( top ) shows the average macro F1 score of the original model ( the blue line ) and of each modified model . The order of the performance drops is AB > A > AC > BC > B > Original > C. This makes sense because disabling important features ( rank A and/or B ) caused larger performance drops , and the overall results are consistent with the average fea- ture scores given by the participants ( as in Figure 4 ) . It confirms that using word clouds is an effective way to assess CNN features . Also , it is worth noting that the macro F1 of the model slightly increased when we disabled the low - quality features ( rank C ) . This shows that humans can improve the model by disabling irrelevant features .

Impact of schema - aware training data

In Figure 3 , we compare the per - task performance of PICL and MetaICL because they share the most similar setting where human - annotated downstream datasets are used . We observe that PICL outperforms MetaICL on about 3/4 of evaluation tasks , indicating that compared to fine - tuning directly on downstream tasks , pre - training on intrinsic tasks constructed from the general plain - text corpus brings better ICL ability and ensures higher generalization performance across a broad range of tasks ( see Appendix E.2 for more details ) .

often small or even negative . REFILL continues to yield positive gains even for smaller workload sizes -In Figure 3 , we plot the fraction of the total SQL workload used on the x - axis and the EM of the fine - tuned parsers averaged across all the four groups , on the y - axis . When using the data synthesized by REFILL , the performance of the parser improves steadily with an increasing size of the SQL workload . In contrast , the baseline SQL - to - Text generation methods fail to provide significant improvements . Interestingly , the data synthesized by REFILL using the 30 % SQL workload leads to better downstream performance of the adapted parsers than any of the baselines utilizing 70 % SQL workload for SQL - to - Text generation .

We extensively evaluate our efficient pretrained models on well - established downstream tasks ( e.g. , Wang et al . , 2018 ; Tjong Kim Sang and De Meulder , 2003 . ) We find that our modifications result in almost no drop in downstream performance , while providing substantial pretraining and inference speedups ( § 3 ) . While efficient attention variants are a promising research direction , this work presents a different and simple approach to making transformers efficient , with minimal changes in architecture .

-DOCSTART- For both PPA and finetuning on downstream tasks , we use the AdamW optimizer with 0.01 weight decay and a linear learning rate scheduler . For PPA , we use a batch size of 128 , mBERT max sequence length 128 and learning rate warmup for the first 10 % of the total iterations , peaking at 0.00003 . The MoCo momentum is set to 0.999 , queue size 32000 and temperature 0.05 . Our PPA models are trained for 10 epochs , except for the 2 M setting where 5 epochs are trained . On XNLI , we use a batch size of 32 , mBERT max sequence length 128 and finetune the PPA model for 2 epochs . Learning rate peaks at 0.00005 and warmup is done to the first 1000 iterations . On MLQA , mBERT max sequence length is set to 386 and peak learning rate 0.00003 . The other parameters are the same as XNLI . Our experiments are run on a single 32 GB V100 GPU , except for PPA training that involves either MLM or TLM , where two such GPUs are used . We also use mixed - precision training to save on GPU memory and speed up experiments .

We also propose a method for solving nondeterministic substitution ciphers with existing keys using a lattice and a pretrained language model . Our method achieves a TER of 1.12 % on the IA cipher , a real historical cipher that has not been fully solved until this work .

In our experiments , we observed that multilingual QE models deliver excellent results on the language pairs they were trained on . In addition , the multilingual QE models perform well in the majority of the zero - shot scenarios where the multilingual QE model is tested on an unseen language pair . Furthermore , multilingual models perform very well with few - shot learning on an unseen language pair when compared to training from scratch for that language pair , proving that multilingual QE models are effective even with a limited number of training instances . While we centered our analysis around the F1 - score of the target words , these findings are consistent with the F1 - score of the target gaps and the F1 - score of the source words too . This suggests that we can train a single multilingual QE model on as many languages as possible and apply it on other language pairs as well . These findings can be beneficial to perform QE in low - resource languages for which the training data is scarce and when maintaining several QE models for different language pairs is arduous .

Input Graph . We model a target header and its context as a directed graph to represent their entity types and structural relations . Firstly , we induce two kinds of edges to denote the structural relationships between the target header and its context : sibling header ( i.e. , an edge point from tokens in S to tokens in the target header . ) , and belonging value ( i.e. , an edge point from tokens in V to tokens in the target header . ) . In this sense , it could incorporate the structural information into the contextualized representation of the target header .

Models for argument and claim generation have been discussed in our related work and are widely available . Gretz et al . ( 2020a ) suggest that , in order to allow for a fine - grained control over claim / argument generation , aspect selection needs to be handled carefully , which is what we have focused on in this work . The dangers of misuse of language models like the CTRL have been extensively discussed by its authors ( Keskar et al . , 2019 ) . The ethical impact of these works has been weighed and deemed justifiable . Argument generation - and natural language generation as a whole - is subject to dual use . The technology can be used to create arguments that can not be distinguished from human - made arguments . While our intentions are to support society , to foster diversity in debates , and to encourage research on this important topic , we are aware of the possibility of harmful applications this model can be used for . For instance , the model could be used to generate only opposing ( or supporting ) arguments on one of the pretrained topics and aspects and , as such , bias a debate into a certain direction . Also , bots could use the generated arguments to spread them via social media . The same is true , however , for argument search engines , which can be used by malicious parties to retrieve ( and then spread ) potentially harmful information .

The prediction for the action chain A requires rigorous layer - by - layer logical reasoning ability . Considering the symbolic network 's reasoning ability ( Yi et al . , 2018 ; Li et al . , 2020c ) , we design a traditional graph theory based symbolic module for searching targeted nodes ( actions ) .

We measure the retrieval performances of models with standard ranking metrics , which are calculated by ranks of correctly retrieved triplets . In particular , we use Hits @ K which measures whether retrieved Top - K triplets include a correct answer or not , and Mean Reciprocal Rank ( MRR ) which measures the rank of the first correct triplet for each input text and then computes the average of reciprocal ranks of all results . Following exiting document retrieval work ( Xiong et al . , 2021 ; Jeong et al . , 2022 ) , we consider top-1000 retrieved triplets when calculating MRR , since considering ranks of all triplets in KGs are computationally prohibitive .

While absolute performance is secondary to our analysis , we report the probing task scores on respective development sets in Table 5 . We observe that grammatical tasks score high , while core role labeling lags behind -in line with the findings of Tenney et al . ( 2019a ) 3 We observe lower scores for German role labeling which we attribute to the lack of training data . Surprisingly , as we show below , this does n't prevent the edge probe from task en de * token.ix 0.95 ( 0.93 ) 0.92 ( 0 . learning to locate relevant role - semantic information in mBERT 's layers .

We evaluate four different aspects of our question generation model : ( i ) successful control for difficulty , ( ii ) novelty , ( iii ) fluency , and ( iv ) latency .

To this end , we focus on the attention mechanism , which is the core component of transformers ( Vaswani et al . , 2017 ) . Several current studies utilized supervision of the attention probabilities to effectively train the model ( Liu et al . , 2016 ; Mi et al . , 2016 ) . Moreover , it is also used for the model distillation to efficiently transfer the knowledge of a transformer model to another one via attention probabilities ( Aguilar et al . , 2020 ; Jiao et al . , 2020 ; Sun et al . , 2020 ; Wang et al . , , 2021 . Inspired by this , we propose distilled attention labels , which are the supervision of attention probabilities optimized as a part of the distilled dataset , to enhance the effectiveness of the distilled dataset for training the transformer models .

For sentence - level , we observe on the one hand quite significant improvements with a gain of almost 8 points in Pearson 's r over BiRNN , our monomodal baseline without pre - trained word embedding . multimodal variants achieve better performance compared to the monomodal BiRNN baseline , with a peak when the visual features are fused with the word embedding representations by elementwise multiplication . On the other hand , we do not observe any gain in using visual features on the WMT'19 test set compared to our monomodal baseline with pre - trained word - embedding ( BERT - BiRNN ) . Here that the BERT - BiRNN baseline model already performs very well . According to the task organisers , the mean MQM value on the WMT'19 test set is higher than on the WMT'18 test set , but actually closer to the training data ( Fonseca

Experiment slot filling . Table 7 shows the macro - average F1 scores for our different models on the slot identification task . 10 As for entity typing , we train and evaluate our model on the subset of sentences marked as experiment - describing , which contain 4,263 slot instances . Again , the CRF baseline outperforms the BiLSTM when using only mat2vec and/or word2vec embeddings . The addition of BERT or SciBERT embeddings improves performance . However , on this task , the BiLSTM model with ( Sci)BERT embeddings outperforms the fine - tuned original ( Sci)BERT model . Compared to the other two tasks , this task requires more complex reasoning and has a larger number of possible output classes . We assume that in such a setting , adding more abstraction power to the model ( in the form of a BiLSTM ) leads to better results . For a more detailed analysis , Table 8 shows the slot - wise results for the non - neural CRF baseline and the model that performs best on the development set : BiLSTM with SciBERT embeddings . As in the case of entity mention detection , the models do well for the categories that consist of numeric mentions plus particular units . In general , model performance is also tied to the frequency of the slot types in the dataset . Recognizing the role a material plays in an experiment ( e.g. , AnodeMaterial vs. CathodeMaterial ) remains challenging , possibly requiring background domain knowledge . This type of information is often not stated explicitly in the sentence , but introduced earlier in the discourse and would hence require document - level modeling .

Transformer - based ED . Several recent studies have proposed ED models based on Transformer ( Vaswani et al . , 2017 ) trained with a large entity - annotated corpus obtained from Wikipedia ( Broscheit , 2019;Ling et al . , 2020;Cao et al . , 2021;Barba et al . , 2022 ) . Broscheit ( 2019 ) trained an ED model based on BERT by classifying each word in the document to the corresponding entity . Similarly , addressed ED using BERT by classifying mention spans to the corresponding entities . Ling

We propose a global entity disambiguation ( ED ) model based on BERT ( Devlin et al . , 2019 ) . To capture global contextual information for ED , our model treats not only words but also entities as input tokens , and solves the task by sequentially resolving mentions to their referent entities and using resolved entities as inputs at each step . We train the model using a large entity - annotated corpus obtained from Wikipedia . We achieve new state - of - the - art results on five standard ED datasets : AIDA - CoNLL , MSNBC , AQUAINT , ACE2004 , and WNED - WIKI . The source code and model checkpoint are available at https : //github.com / studio - ousia / luke .

The results on SUPER - NATURALINSTRUCTIONS are shown in achieves higher overall instruction following performance than the baselines , outperforming a larger model with about 4x parameters .

t ← t + 1 23 : end while the score of recently discovered nodes so the algorithm prefers to continue them . The decay function needs to be monotonic . Hence , we define the decay function as a power function :

We propose different ways to integrate visual features in our two monomodal QE approaches ( Sections 3.1 and 3.2 ) . We compare each proposed model with its monomodal QE counterpart as baseline , both using the same hyperparameters .

On MuCGEC , we submit the results of our systems to the public evaluation website 10 . On NLPCC , we implement the tools provided by Zhang et al . ( 2022 ) to compute the P ( Precision ) , R ( Recall ) , and F 0.5 of the output on char - level . Also , we report word - level results on NLPCC - test for reference with previous works .

In Appendix B , we further show that automatic metrics like ROUGE and BERTScore ( Zhang et al . , 2019 ) that are primarily used for evaluating long document summarization fail to penalize coherence errors in summaries . Better tools for both automatic and human evaluation are needed .

Linking dataset entities to geospatial concept is one integral part of our proposed methodology . Ongoing geospatial semantics research mostly focuses on extracting spatial and temporal entities ( Kokla and Guilbert , 2020 ; Purves et al . , 2018 ) . The usual approach is to first extract geo - location concepts ( i.e. geotagging ) from semi - structured as well as unstructured data and then linking those entities to location based knowledge ontology ( i.e. geocoding ) . In ( Gritta et al . , 2019 ) , the authors propose a task - metric - evaluation framework to evaluate existing NER based geoparsing methods . The primary findings suggest that NER based geo - tagger models in general rely on instant word - sense while avoiding contextual information .

Environmental sound provides rich perspectives on the physical world . For example , if we hear : joyful laughing , a playful scream , and a splash ; we not only can visualize literal objects / actions that might have given rise to the audio scene , but also , we can reason about plausible higher - level facets , e.g. , a child speeding down a water slide at a water park , splashing through the water ( see Figure 1 ) . * Work was partially done during an internship at AI2 .

For future work , it would be interesting to extend FIND to other NLP tasks , e.g. , question answering and natural language inference . This will require some modifications to understand how the features capture relationships between two input texts .

Traditional contrastive learning only considers positive and negative samples , and their instances are usually not fine - grained . In our semanticaware contrastive learning , we need to deal with multi - level instances , and use special semanticaware similarities .

Modeling cross - lingual divergence There has been many works modeling cross - lingual divergence ( e.g. , reordering ) in statistical machine translation ( Nagata et al . , 2006;Durrani et al . , 2011Durrani et al . , , 2013 . However , it is difficult to migrant them to neural machine translation . Kawara et al . ( 2018 ) pre - reordered the source sentences with a recursive neural network model . Chen et al . ( 2019a ) learned the reordering embedding by considering the relationship between the position embedding of a word and SANS - calculated sentence representation . showed that SANs in machine translation could learn word order mainly due to the PE , indicating that modeling cross - lingual information at position representation level may be informative . Thus , we propose a novel cross - lingual PE method to improve SANs .

Tables 1 and 2 report the results of our approach alongside those reported by previous state - of - theart methods on CLINC150 , SST , ROSTD , and 20NewsGroups . It can be seen that our proposed method outperforms the prior methods with a large margin in most experiments , achieving an improvement of up to 9.13 , 20.73 , 38.71 points in terms of AUROC , AUPR , and FAR95 , respectively , on CLINC150 . This well demonstrates the effectiveness of the proposed approach .

Nevertheless , FIND has some limitations . First , the word clouds may reveal sensitive contents in the training data to human debuggers . Second , the more hidden features the model has , the more human effort FIND needs for debugging . For instance , BERT - base ( Devlin et al . , 2019 ) has 768 features ( before the final dense layer ) which require lots of human effort to perform investigation . In this case , it would be more efficient to use FIND to disable attention heads rather than individual features ( Voita et al . , 2019 ) . Third , it is possible that one feature detects several patterns ( Jacovi et al . , 2018 ) and it will be difficult to disable the feature if some of the detected patterns are useful while the others are harmful . Hence , FIND would be more effective when used together with disentangled text representations ( Cheng et al . , 2020 ) .

-DOCSTART- An Exploratory Analysis of Multilingual Word - Level Quality Estimation with Cross - Lingual Transformers

The year - to - year nature of financial reports allows us to take advantage of the differences between a company 's documents in consecutive years . These differences may reveal complex but insightful relationships within a pair of documents . To better understand these relationships , we investigate them through rationales ( represented by the word importance ) , which are considered essential signals in financial reports .

Entity disambiguation ( ED ) refers to the task of assigning mentions in a document to corresponding entities in a knowledge base ( KB ) . This task is challenging because of the ambiguity between mentions ( e.g. , " World Cup " ) and the entities they refer to ( e.g. , FIFA World Cup or Rugby World Cup ) . ED models typically rely on local contextual information based on words that co - occur with the mention and global contextual information based on the entity - based coherence of the disambiguation decisions . A key to improve the performance of ED is to effectively combine both local and global contextual information ( Ganea and Hofmann , 2017;Le and Titov , 2018 ) .

• Vote ( Mou et al . , 2021 ) proposes to align statements on social networks with voting records .

These Figure 1 illustrates the three instantiations of f . In all cases , attribute - value pairs are ordered in descending order of the frequency with which they appear in the training KB . Finally , since both the candidate generation and candidate re - ranking models we build on use BERT , the techniques discussed here can be applied to both stages , but we only focus on re - ranking .

D4 . Was the data collection protocol approved ( or determined exempt ) by an ethics review board ? No response .

We build on this line of research , pioneered by REALM and RAG , and propose a new approach that we call Re 2 G ( Retrieve , Rerank , Generate ) , which combines both neural initial retrieval and reranking into a BART - based sequenceto - sequence generation .

To provide more context to the automated evaluation , we also use knowledge graph embeddings . We rely on the KGvec2go Web API ( Portisch et al . , 2020 ) created from the resources WordNet , Wiktionary , DBpedia , and WebIsALOD . We average the four returned similarity scores based on the different resources , called KB score in the following .

The key difference between the vanilla Transformer decoder and ours is the in - parallel cross - attention layer which allows better integration of knowledge encoded in the two sources . Concretely , in - parallel cross attentions are implemented as follows :

Evaluation . We evaluate the generated summaries using lexical - overlap metrics , specifically ROUGE-1 / 2 / L ( Lin , 2004 ) , and embeddingsimilarity metrics , specifically BERTSCORE ( Zhang et al . , 2020b ) . Besides , we resort to more precise human studies to evaluate the consistency of generated summaries and source documents . See Appendix A for more useful evaluation details .

We count psychologically relevant word categories using the Linguistic Inquiry Word Count ( Pennebaker et al . , 2015 , LIWC ;) , focusing only on the cognitive processes , positive emotion , negative emotion , and I - word categories , as well as the ANALYTIC and TONE summary variables . 5 Additionally , we measure the average concreteness level of words in stories using the lexicon by Brysbaert et al . ( 2014 ) .

B4 . Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content , and the steps taken to protect / anonymize it ? We did not discuss this as the datasets are commonly used NLP benchmarks that do not contain personal data .

For all eight topics , we show the generated argument with the highest and lowest argument quality score in tables 11 ( Arg - CTRL CC ) and 12 ( Arg - CTRL REDDIT ) . Text in bold shows the given control code , text afterwards represents the generated argument . Numbers in brackets after the text show the quality score as predicted by the argument quality model .

Our focus is on linking entities to unseen KBs with arbitrary schemas . One solution is to annotate data that can be used to train specialized models for each target KB of interest , but this is not scalable . A more generic solution is to build entity linking models that work with arbitrary KBs . We follow this latter approach and build entity linking models that link to target KBs that have not been observed during training . 1 Our solution builds on recent models for zero - shot entity linking ( Wu et al . , 2020;Logeswaran et al . , 2019 ) . However , these models assume the same , simple KB schema during training and inference . We generalize these models to handle different KBs during training and inference , containing entities represented with an arbitrary set of attribute - value pairs . This generalization relies on two key ideas . First , we convert KB entities into strings that are consumed by the models for zero - shot linking . Central to the string representation are special tokens called attribute separators , which represent frequently occurring attributes in the training KB(s ) , and carry over their knowledge to unseen KBs during inference ( Section 4.1 ) . Second , we generate more flexible string representations by shuffling entity attributes before converting them to strings ,

By comparing the translations for headers with special tokenization , we can see that all fine - tuned models , including H2H , H2H+CXT , and CAST can accurately translate headers in CamelCase or underscore tokenizations , while Base fails to skip the underscore and can not translate " Debt " in the middle of " AccessedDebtService " .

To further demonstrate the intuition behind our framework , we randomly sample 1,000 examples from test sets of En→De and En→Es directions and use t - SNE ( Van der Maaten and Hinton , 2008 ) to visualize the sentence embedding of translation memories and target sentence encoded by our CMM . The result is shown in Figure 4 and one interesting observation is that although the target side of testset is never exposed to the model , the representation of translation memories are uniformly distributed around the target sentence in the latent semantic space .

Does familiarizing the LM with the task format using a few - shot evaluation setting substantially improve performance ( § 4 ) ? We find that the fewshot evaluation ( using up to 64 examples ) does not substantially improve the LMs ' performance for most tasks except Social IQa . Moreover , using the few - shot / in - context demonstration setting fails to bridge the gap between the LM and current SOTA .

B3 . Did you discuss if your use of existing artifact ( s ) was consistent with their intended use , provided that it was specified ? For the artifacts you create , do you specify intended use and whether that is compatible with the original access conditions ( in particular , derivatives of data accessed for research purposes should not be used outside of research contexts ) ? Left blank .

Thus , each piece of multi - modal sample is manually annotated with desire category , sentiment category ( i.e. , positive , neutral and negative ) and emotion category ( happiness , sad , neutral , disgust , anger and f ear ) . The annotation model is AnnotationModel = ( DesireCategory , Sentiment - Category , EmotionCategory , DataSource ) .

Operationally , NCE can be viewed as follows :

Assessing Discourse Coherence . Table 1 shows the experimental results on GCDC dataset 2 . The first three rows ( Li and Jurafsky , 2017;Mesgar and Strube , 2018;Lai and Tetreault , 2018 ) in the first block show the performance of embeddingbased models , and the last four rows ( Mesgar and Strube , 2016;Moon et al . , 2019;Jeon and Strube , 2020a , b ) in the same block are the state - of - theart models based on XLNet . With the pre - trained model as the encoder , the latter four models outperform embedding - based methods by a large margin .

We here present the results of our inter - annotator agreement study , which we perform in order to estimate the degree of reproducibility of our corpus and to put automatic modeling performance into perspective . Six documents ( 973 sentences ) have been annotated independently both by our primary annotator , a graduate student of materials science , and a second annotator , who holds a Ph.D. in physics and is active in the field of materials science . The label distribution in this subset is similar to the one of our overall corpus , with each annotator choosing EXPERIMENT about 11.8 % of the time . Identification of experiment - describing sentences . Agreement on our first task , judging whether a sentence contains relevant experimental information , is 0.75 in terms of Cohen 's κ ( Cohen , 1968 ) , indicating substantial agreement according to Landis and Koch ( 1977 ) . The observed agreement , corresponding to accuracy , is 94.9 % ; expected agreement amounts to 79.2 % . Table 2 shows precision , recall and F1 for the doubly - annotated subset , treating one annotator as the gold standard and the other one 's labels as predicted . Our primary annotator identifies 119 out of 973 sentences as experiment - describing , our secondary annotator 111 sentences , with an overlap of 90 sentences . These statistics are helpful to gain further intuition of how well a human can reproduce another annotator 's labels and can also be considered an upper bound for system performance .

Complete this story in 500 words . The narrator , a Yale graduate , moves to New York to learn the bond business . He visits his second cousin , Daisy , and her husband , Tom , for dinner . During the dinner , Daisy mentions she wants to go back to Chicago the next day .

We evaluate PAR and competitive baselines on roll call vote prediction and present model performance in Table 2 . PAR achieves state - of - the - art performance , outperforming existing baselines that model political actors in different ways . As a result , PAR learns high - quality representations of political actors that provide political knowledge and augment the vote prediction process .

The input form of BERT without context is

Predicted Program : fact_1 = Verify ( " Tritonia is a name for a plant genus . " ) fact_2 = Verify ( " Phyteuma is a name for a plant genus . " ) label = Predict ( fact_1 and fact_2 )

For the sentence - level QE task , each document of the dataset was split into sentences ( lines ) , where every sentence has its corresponding MQM score computed in the same way as for the document . We note that this variant is different from the official sentence - level track at WMT since for that task visual information is not available .

Diversity : To evaluate the diversity of generated text , we consider Dist - n ( Li et al . , 2016 ) and Self - BLEU ( S - BL ) ( Zhu et al . , 2018 ) . More metrics details are described in Appendix A.3 .

Analyzing model learning throughout pretraining requires access to intermediate training check - points , rather than just the final artifact . We replicate the base version of XLM - R and save a number of checkpoints throughout the training process . Our pretraining setup primarily follows that of the original XLM - R , with the exception that we use a smaller batch size ( 1024 examples per batch instead of 8192 ) due to computational constraints . All other hyperparameters remain unchanged .

Accuracy results in Table 2 have a 95 % Wald confidence interval of ±2.8 % . The first row of Table 2 presents the accuracy results of a vanilla UnifiedQA large model on SciQ. The second line shows the accuracy when UnifiedQA is fine - tuned over the full training corpus . Our objective is thus to get as close as possible to this accuracy score using only un - supervised methods . The results using Wikipedia are the only ones that are unsupervised and therefore are the ones directly comparable to UnifiedQA with no fine - tuning or other unsupervised methods . Table 2 : Accuracy on SciQ by UnifiedQA fine - tuned on our synthetic datasets . " SciQ data " refers to the questions generated using the support paragraphs in SciQ while " Wikipedia data " refers to questions generated using sentences harvested from Wikipedia . All scores are averaged over 3 independent runs ( including the complete question generation process and the final Uni - fiedQA fine - tuning ) .

In the future , we would like to explore whether such speaker prompting can improve models in other person - centered tasks , e.g. , coreference resolution ( especially for datasets explicitly testing gender biases ) or sentiment analysis . Using techniques such as data augmentation , we can explicitly guide models away from biases learned during training . With ethical considerations in mind , our work advances the state - of - the - art in building more adaptable and person - aware NLP technologies . Yuheng Zhang , Ruoxi Jia , Hengzhi Pei , Wenxiao Wang , Bo Li , and Dawn Song . 2020

In them , the M LP ap , M LP cl and M LP ac are the MLP applied to predict the probabilities of all appearance , clothing , and action classes ( p ap , p cl , and p ac ) , respectively . We employ the cross - entropy loss function to train the prediction of commonsense knowledge .

Our Twitter - Para is a pre - processed dataset based on ( Xu et al . , 2014 ( Xu et al . , , 2015 . In the original dataset ( Xu et al . , 2014 ( Xu et al . , , 2015 , there are some input sentences that have no corresponding references , so we drop such input - candidate pairs to create Twitter - Para . Specifically , the human - annotated score ranges from 0∼1.0 , where higher scores mean better quality . The basic statistics of Twitter - Para are listed in

The network then splits into two branches . The primary ( bottom ) branch consists of a neural network model g , with parameters θ 3 , which produces promotional tone predictionsŷ T 1 andŷ T 2 for the two samples :

Random effects via regularization . We are now prepared to introduce random effects into the transformer via hierarchical prefix - tuning . Critically , instead of assuming that all values of a particular feature have independent fixed effects ( e.g. that the language associated with one genre is independent of other genres ) , we would like to assume they are drawn from a common distribution :

Multilinguality Multilinguality allows training a single model to perform a task from and/or to multiple languages . Even though this has been applied to many tasks Zampieri , 2020 , 2021 ) including NMT ( Nguyen and Chiang , 2017;Aharoni et al . , 2019 ) , multilingual approaches have been rarely used in QE . Shah and Specia ( 2016 ) explore QE models for more than one language where they use multitask learning with annotators or languages as multiple tasks . They show that multilingual models led to marginal improvements over bilingual ones with a traditional black - box , feature - based approach . In a recent study , Ranasinghe et al . ( 2020b ) show that multilingual QE models based on transformers trained on high - resource languages can be used for zeroshot , sentence - level QE in low - resource languages .

-DOCSTART- Position encoding ( PE ) , an essential part of self - attention networks ( SANs ) , is used to preserve the word order information for natural language processing tasks , generating fixed position indices for input sequences . However , in cross - lingual scenarios , e.g. , machine translation , the PEs of source and target sentences are modeled independently . Due to word order divergences in different languages , modeling the cross - lingual positional relationships might help SANs tackle this problem . In this paper , we augment SANs with crosslingual position representations to model the bilingually aware latent structure for the input sentence . Specifically , we utilize bracketing transduction grammar ( BTG)-based reordering information to encourage SANs to learn bilingual diagonal alignments . Experimental results on WMT'14 English⇒German , WAT'17 Japanese⇒English , and WMT'17 Chinese⇔English translation tasks demonstrate that our approach significantly and consistently improves translation quality over strong baselines . Extensive analyses confirm that the performance gains come from the cross - lingual information .

In addition , the API provides the names of the legislators who cosponsored a bill and when this cosponsorship occurred . We automatically match the cosponsors ' names to their BioGuide ID . In cases where automated matching was not possible -e.g . , because legislators signed with their nicknames - we resorted to manual matching . As discussed in Section 1 , we assign cosponsorship their official label . Cospsonsorships recorded at the bill 's introduction are active and those recorded after its introduction are passive .

5 For a fair comparison , we exclude the WNLI following the previous work ( Devlin et al . , 2019 ) .

The Rules grouping method used in the second stage of the CT - Rules model , relied on categoryspecific statistics in the training portion of SWIPE . Categories were split into two sub - groups : contiguous and global . For each category , we analyzed the percentage of annotated of edits of the given category that were contiguous ( adjacent ) in their operation group . For each edit category , if a majority of annotated cases were contiguous , the edit category was labeled as contiguous , otherwise , it was labeled as global . For categories marked as contiguous , the model generated groups for predicted operation types based on contiguous boundaries ( identical to the Adjacent grouping method ) , and all operations of a given global category were organized into a single group .

Our model is based on BERT LARGE ( Devlin et al . , 2019 ) . The parameters shared with BERT are initialized using BERT , and the other parameters are initialized randomly . We treat the hyperlinks in Wikipedia as entity annotations and randomly mask 30 % of all entities . We train the model by maximizing the log likelihood of entity predictions . Further details are described in Appendix A.

ii We explore multilingual , word - level quality estimation with the proposed architecture . We show that multilingual models are competitive with bilingual models .

All results are from a single run . The random seed for python , numpy and pytorch was 42 .

We formulate multiple - choice tasks as text - to - text by concatenating the human - authored cartoon descriptions with the choices as input : the target is simply the letter corresponding to the answer , e.g. , E. For explanation , we autoregressively generate the explanations conditioned on the descriptions / captions .

Separate scalar mixes . To enable fine - grained analysis of probing results , we train and analyze separate scalar mixes for source and target wordpieces , motivated by the fact that the classifier might utilize different aspects of their representation for prediction 1 . Indeed , we find that the mixing weights learned for source and target wordpieces might show substantial -and linguistically meaningful -variation . Combined with regressionbased objective , separating the scalar mixes allows us to scrutinize layer utilization patterns for semantic proto - roles .

The goal is to identify the emotion label y i for each utterance u i from the pre - defined emotions Y .

Language modeling ( Dai and Le , 2015;Radford et al . , 2018;Peters et al . , 2018 ) and cloze modeling ( Devlin et al . , 2019;Baevski et al . , 2019 ; have proven to be effective pre - training tasks for NLP . Unlike Electric , these methods follow the standard recipe of estimating token probabilities with an output softmax and using maximumlikelihood training .

When working with clinical data , two key ethical objectives include : ( 1 ) the preservation of pa - tient privacy , and ( 2 ) the development of language and predictive models that benefit patients and providers to the extent possible , without causing undue harm . With respect to the former , MedNLI 's premises are sampled from de - identified clinical notes contained in MIMIC - III ( Goldberger et al . , 2000 ( June 13;Johnson et al . , 2016 ) , and the hypotheses generated by annotators do not refer to specific patients , providers , or locations by name . MedNLI requires users to complete Health Insurance Portability and Accountability Act ( HIPAA ) training and sign a data use agreement prior to being granted access , which we have complied with .

PropBank assumes a predicate - independent labeling scheme where predicates are distinguished by their sense ( get.01 ) , and semantic arguments are labeled with generic numbered core ( Arg0 - 5 ) and modifier ( e.g. AM - TMP ) roles . Core roles are not tied to specific definitions , but the effort has been made to keep the role assignments consistent for similar verbs ; Arg0 and Arg1 correspond to the Proto - Agent and Proto - Patient roles as per Dowty ( 1991 ) . The semantic interpretation of core roles depends on the predicate sense .

We use the same line type ( dashed or solid ) for models trained on the same data . Using the same data , the performance of the two different directions of models can not be compared directly because the target language is different , causing the BLEU calculation to be different .

We introduce HIPPOCORPUS , 1 a dataset of 6,854 diary - like short stories about salient life events , to examine the cognitive processes of remembering and imagining . Using a crowdsourcing pipeline , we collect pairs of recalled and imagined stories written about the same topic . By design , authors of recalled stories rely on their episodic memory to tell their story .

Beyond Conventional Entity Linking There have been several attempts to go beyond such conventional settings , e.g. by linking to KBs from diverse domains such as the biomedical sciences ( Zheng et al . , 2014;D'Souza and Ng , 2015 ) and music ( Oramas et al . , 2016 ) or even being completely domain and language independent Onoe and Durrett , 2020 ) . Lin et al . ( 2017 ) discuss approaches to link entities to a KB that simply contains a list of names without any other information . Sil et al . ( 2012 ) use databaseagnostic features to link against arbitrary databases . However , their approach still requires training data from the target KB . In contrast , this work aims to train entity linking models that do not rely on training data from the target KB , and can be trained on arbitrary KBs , and applied to a different set of KBs . Pan et al . ( 2015 ) also do unsupervised entity linking by generating rich context representations for mentions using Abstract Meaning Representations ( Banarescu et al . , 2013 ) , followed by unsupervised graph inference to compare contexts . They assume a rich target KB that can be converted to a connected graph . This works for Wikipedia and adjacent resources but not for arbitrary KBs . Logeswaran et al . ( 2019 ) introduce a novel zeroshot framework to " develop entity linking systems that can generalize to unseen specialized entities " . Table 1 summarizes differences between our framework and those from prior work .

where the binary label L indicates whether the response is relevant to the personas .

The shared limitation of parameter - efficient techniques is that they are not computation - efficient ; These methods choose to directly inherit the pretrained weights of the backbone model and add some extra modules , which increases the computational cost of these methods during inference .

The noise distribution is trained simultaneously with Electric using standard maximum likelihood estimation over the data . The model producing the noise distribution is much smaller than Electric to reduce the computational overhead .

In PATS , we perturb the parameters of all the encoder layers except the Layer Normalization layers .

M6Rec ( Cui et al . , 2022 ) employs prompt tuning of pretrained language models for building a unified framework . M6Rec fully utilizes text inputs to generalize to any domains / systems and has the ability to perform zero - shot learning . Since they did not release pretrained M6 ( Lin et al . , 2021 ) , we used Huggingface RoBERTa to implement it . 3

FIND is suitable for any text classification tasks where a model might learn irrelevant or harmful features during training . It is also convenient to use since only the trained model and the training data are required as input . Moreover , it can address many problems simultaneously such as removing religious and racial bias together with gender bias even if we might not be aware of such problems before using FIND . In general cases , FIND is at least useful for model verification .

We find that default BPE does not perform better than the 2 - digit baseline , with a SegER score of about 34 % . However , as noted in Section 3.1 , most cipher elements in historical ciphers are one or two digits long . In our random sample of three historical ciphers , about 95 % of cipher tokens are one and two - digit ( as shown in Table 1 ) . Longer elements appear less often ( less than 5 % of tokens in our test ciphers ) . Thus , we limit BPE piece length to a maximum of 2 digits . This improves the SegER score by reducing it to about 11 % . We call this model " BPE 2 " in Table 2 .

In the specific implementation , which candidates satisfy the question meanings best are determined by neural models . In the training process , we take relations that appear on shortest paths between mentioned entities and answers as positive samples . In particular , the relation that entails part of or precedes are filtered according to the KG schema in the training process and are predicted by neural models during the test process . Queries for the questions of multiple constraints are the conjunction of the grounding result of each constraint and queries for the questions with no temporal constraints are unrestricted basic query graphs .

Models for entity linking typically consist of two stages that balance recall and precision .

Figure 6 : Plot of 500 runs where we first apply MP and then continue with 34 iterations of random projections . The shades of red indicate various levels of confidence ( 10 levels ) in WEAT scores ( y - axis ) after a certain number of iterations ( x - axis ) : darker red shades reflect WEAT scores occurring with a higher probability .

In this work , we present the first effort to leverage a hybrid knowledge - transfer approach for the cross - lingual event detection task . We propose a teacher - student framework complemented by a hierarchical training - sample selection scheme that effectively constrains the student - training process to pseudo - labeled target - language samples that are similar to their source - language counterparts . Our HKT - CLED model sets a new state - of - the - art performance on the most popular benchmarking datasets ACE05 and ACE05 - ERE , and obtains substantial performance improvements on the recentlyreleased , and more diverse , MINION dataset with an average improvement of +7.74 F1 points across 7 distinct target languages . We believe these results demonstrate our model 's robustness and applicability and validate our claim that combining the benefits of the direct transfer and data transfer approaches is beneficial for cross - lingual learning .

• Lastly , we strongly encourage future work to report the variance of the observed results across different design choices . This can provide an indication of the robustness of the language models ' performance on commonsense benchmarks .

Through what mechanism does a Trojan attack affect an NLP model ?

We cast each hypothesis string in the MedNLI training dataset to lowercase . We then use a scispaCy model pre - trained on the en_core_sci_lg corpus for tokenization and clinical named entity recognition ( CNER ) ( Neumann et al . , 2019a ) . One challenge associated with clinical text , and scientific text more generally , is that semantically meaningful entities often consist of spans rather than single tokens . To mitigate this issue during lexical analysis , we map each multi - token entity to a single - token representation , where sub - tokens are separated by underscores .

Moreover , with the development of automatic text generation technologies such as InstructGPT ( Ouyang et al . , 2022 ) and ChatGPT 3 , the risk of automatically generated content to society ( e.g. , generating fake news or fake reviews of products ) is increasing . Therefore , we further adapt our model to distinguish texts generated by AI models and human experts . By conducting experiments on the Human ChatGPT Comparison Corpus ( HC3 ) , we observe that our model beats human evaluators and shows excellent capability in the pair - expert task .

This loss is minimized whenp θ matches the data distribution p data ( Gutmann and Hyvärinen , 2010 ) . A consequence of this property is that the model learns to be self - normalized such that Z θ ( x \t ) = 1 .

where e b i1 and e b i2 are the time embeddings for p b i1 and p b i2 ( same for the target ) . We use Time2Vec ( Kazemi et al . , 2019 ) for time embeddings , and we jointly learn embeddings for the buyer and the target . Price Encoder . As in Du and Tanaka - Ishii ( 2020 ) and Kostkova et al . ( 2017 ) , we use a Gated Recurrent Unit ( Cho et al . , 2014 , GRU ) to encode the price variations over time . We implement two separate GRU b and GRU t for the buyer and the target . At time i , the GRU b 's output consists of :

SOFC - Exp Corpus . Our corpus consists of 45

We use 1 NVIDIA TitanXP GPU with 12 GB of memory available . Because the maximum input sequence length of the GPT-2 model we use is 1024 tokens , we resize all inputs to the last 1024 tokens before training . We report results for an LM - KT model trained for 13k steps with the default batch size of 2 and learning rate of 5e-5 , and a Question Generation model trained for 25k steps with the same batch size and learning rate . The total compute time to train both models was 2.5 hours for each language learning task .

2 . The distribution of the input text , i.e. , the underlying distribution that x 1 ... x k are from .

Considerable progress in domain - agnostic NLI has been facilitated by the development of largescale , crowdworker - constructed datasets , including the Stanford Natural Language Inference corpus ( SNLI ) , and the Multi - Genre Natural Language Inference ( MultiNLI ) corpus ( Bowman et al . , 2015;Williams et al . , 2017 ) . MedNLI is a similarlymotivated , healthcare - specific dataset created by a small team of physician - annotators in lieu of crowdworkers , due to the extensive domain expertise required ( Romanov and Shivade , 2018 ) . Poliak et al . ( 2018 ) , Gururangan et al . ( 2018 ) , Tsuchiya ( 2018 ) , andMcCoy et al . ( 2019 ) empirically demonstrate that SNLI and MultiNLI contain lexical and syntactic annotation artifacts that are disproportionately associated with specific classes , allowing a hypothesis - only classifier to significantly outperform a majority - class baseline model . The presence of such artifacts is hypothesized to be partially attributable to the priming effect of the example hypotheses provided to crowdworkers at annotation - time . Romanov and Shivade ( 2018 ) note that a hypothesis - only baseline is able to outperform a majority class baseline in MedNLI , but they do not identify specific artifacts .

Teacher 's auxiliary loss Inspired by ( Pham et al . , 2021 ) , in order to balance the trade - off between self - evolution and transferability of the teacher We observe that for LGTM , student model does not suffer from overfitting ( thanks to distillation influence ) , and the teacher can balance its own evolution and effective knowledge transfer ( thanks to auxiliary loss ) .

The statistics of the ED datasets used in our experiments are provided in Table 6 .

• We conduct three human experiments that demonstrate the effectiveness of FIND in different scenarios . The results not only highlight the usefulness of our approach but also reveal interesting behaviors of CNNs for text classification .

' picked up a nasty h pylori ' from an casual doctor date , i know i should have gotten to know the person better . took a while until they showed up . had severe upset stomach , occasional diarrhea , nausea and slow but steady weight loss . took a long time and several doctors to diagnose my steadily worsening condition . tried prevpak first , seemed to work at first butmy infection came back . the new gi then prescribed pylera after my 3rd endoscopy . pylera has worked , it 's been a year and i am still h pylera negative . but it 's been brutal and F ( s adv , l = " 6.0 " ) = 0.97 Cos . = -0.19

In this section , we first describe our experiment setup including datasets and baselines in Sec . 5.1 . Then we compare our proposed LGTM to meta distillation to gain some basic understanding of how to incorporate the student 's feedback in Sec . 5.2 . To further verify the effectiveness of our method , in Sec . 5.3 we compare to 10 widely adopted knowledge distillation baselines and show consistently better results . Then we demonstrate how distillation influence works in Sec . 5.4 , followed by ablation studies of LGTM in Sec . 5.5 .

To verify this , we further conduct experiments on PubMed ( Cohan et al . , 2018 ) , a long - document

The untrained mBERT baseline expectedly underperforms ; however , we note good baseline results on surface - level tasks for English , which we attribute to memorizing token identity and position : although the weights are set randomly , the frozen encoder still associates each wordpiece input with a fixed random vector . We have confirmed this assumption by scalar mix analysis of the untrained mBERT baseline : in our experiments the baseline probes for both English and German attended almost exclusively to the first few layers of the encoder , independent of the task . For brevity , here and further we do not examine baseline mixing weights and only report the scores .

Further Analysis Note that a direct attribution of the gain in translation quality to better translation of idioms specifically is challenging . Further , similarity - based quality metrics such as COMET - QE themselves might be penalizing non - literalness , even though they are less likely to do this than surface - level metrics such as BLEU or ChrF ( Papineni et al . , 2002b ; Popović , 2015 ) . Therefore , while a natural monolingual dataset presents a useful testbed for investigating figurative compositionality abilities , an explicit comparison of figurative compositionality between the systems is very difficult . Therefore , we also conduct experiments on synthetic data , where we explicitly control the finegrained attributes of the input sentences . We do this by allocating most of the variation among the input sentences to certain constituent expressions in synthetic data generation .

Our probing framework is implemented using AllenNLP . 2 We train the probes for 20 epochs using the Adam optimizer with default parameters and a batch size of 32 . Due to the frozen encoder and flat model architecture , the total runtime of the main experiments is under 8 hours on a single Tesla V100 GPU . In addition to pre - trained mBERT we report baseline performance using a frozen untrained mBERT model obtained by randomizing the encoder weights post - initialization as in Jawahar et al . ( 2019 ) .

To improve the quality of the constructed data , we derive an approach to filter out instances that are less informative to ICL . We consider the following score to measure the informativeness of an instance based on the perplexity difference of the paragraphs in the instance before and after they are concatenated as a sequence :

2 . Candidate Reranking : This stage ranks the candidates in E by how likely they are to be the correct entity . Unlike candidate generation , models for re - ranking are typically more complex and oriented towards generating a high - precision ranked list since the objective of this stage is to identify the most likely entity for each mention . This stage is evaluated using precision@1 ( or accuracy ) i.e. whether the highest ranked entity is the correct entity .

In this section , we present the algorithm to train our model , METRO - T0 .

We employ three datasets for text classification and three datasets for natural language inference as described below . In our experiments , we randomly sample 1000 instances .

Comparing our model without word - level alignment , i.e. , -TLM , to the baseline mBERT in Table 2 , we get 2 - 4 % improvement in the zero - shot setting and 1 - 2 % improvement in translate - train as the amount of parallel data is increased . These are relatively large improvements considering the fact that only sentence - level alignment is used . This also conforms to our intuition that sentence - level alignment is a good fit here since XNLI is a sentencelevel task .

Next , we introduce an information fusion module to enable the interaction between textual ( z T ) and graph ( z G ) hidden states , in order to obtain the fused representation , z ′ . We implement two information fusion operations : ( 1 ) addition , i.e. , z ′ = z T + z G , and ( 2 ) gating mechanism between z T and z G as in Zhao et al . ( 2018 ) except that we use GELU ( • ) as the activation function . The operation selection is determined by downstream tasks .

To train both our LM - KT knowledge tracing model and our question generation model , we use the pre - trained OpenAI GPT-2 model from the HuggingFace Transformers library ( Wolf et al . , 2020 ) . For question generation , we modify the library to add a linear layer and the modified loss function for question generation from Section 3 .

The influence of group size . In WhitenedCSE , we divide the representation into k groups . However , we know the size of group controls the degree of whitening , and has a great effect on the effectiveness of WhitenedCSE , so we carry out an experiment with k varying from 32 to 384 . As shown in Tab . 4 , we can see that the best performance is achieved when k = 384 , and the second best performance is achieved when k = 128 . When k takes other values , the performance will drop slightly .

Question Generation We frame question generation as finetuning a new autoregressive LM . Given random samples of students and questions from a held - out set not used to train LM - KT , we can construct a new dataset D consisting of s i d i < G > q i sequences , where < G > is a special generation token and d i = p θ KT ( < Y>|s i , q i ) is the continuous difficulty value assigned by LM - KT . We learn a linear layer to map the continuous input difficulty into a difficulty control vector c d of dimension matching the LM word - embeddings , which we append to the token embeddings . Unlike LM - KT , we train our question generation model p θ QG to minimize the loss only on the question text , which only appears after the < G > token . If t g is the token index of < G > , then our modified loss is :

We obtain 393,423 biographies from Ravfogel et al . ( 2020 ) which is a subset of the original corpus of describing people with 28 different occupations . 9 We split the data in the same 65 % training , 10 % development and 25 % test splits as used by and Ravfogel et al . ( 2020 ) . Like Ravfogel et al . ( 2020 ) , we use logistic classifiers that take one of three representations of the biographies as input : ( 1 ) one - hot BOW , ( 2 ) averaged FastText embeddings ( Joulin et al . , 2017 ) and ( 3 ) the last hidden state of BERT over the [ CLS ] token .

We performed basic pre - processing of transcripts in each dataset by which we removed speech artifact descriptions and converted non - ASCII characters to plain text . We also excluded portions of transcripts that represented speech that did not belong to the participant .

Inspired by recent work on discourse modeling ( Kang et al . , 2019;Nadeem et al . , 2019 ) , we use language models to assess the narrative linearity of a story by measuring how sentences relate to their context in the story . We compare the likelihoods of sentences under two generative models ( Figure 2 ) . The bag model makes the assumption that every sentence is drawn independently from the main theme of the story ( represented by E ) . On the other hand , the chain model assumes that a story begins with a theme , and sentences linearly follow each other . 3 . ∆ l is computed as the difference in negative loglikelihoods between the bag and chain models :

In § 4.5 , we will demonstrate our comparative studies between the aforementioned methods and GlobEnc .

Finally , we create a held - out set of Duolingo questions for both French and Spanish learners to create the training data for our question generation model . From a set of random student states , we select questions from this set and use a trained LM - KT model to assign the difficulty score . In practice , this held - out set can come from any source , not just Duolingo data .

We conduct an ablation study on the IMDb dataset . As shown in Table 3 , we can find : 1 ) variational learning further enhances control accuracy and diversity with slight PPL loss , which is worthwhile since the generated text is already fluent enough ( close to ground truth PPL ) . 2 ) pseudo labels lead to a significant improvement . 3 ) soft pseudo text outperforms the hard one on controllability and diversity but with marginal fluency loss . Solely hard pseudo text in ST limits model coverage , while the soft one brings a smoother noise and helps push the learned boundary .

When computing token - class pointwise mutual information ( PMI ) , we exclude tokens that appear less than five times in the overall training dataset 's hypotheses . Then , following Gururangan et al . ( 2018 ) , who apply add-100 smoothing to raw counts to highlight particularly discriminative token - class co - occurrence patterns , we apply add-50 smoothing to raw counts . Our approach is similarly motivated ; our choice of 50 reflects the smaller state space associated with a focus on the clinical domain .

SANs can be implemented with multi - head attention mechanism , which requires extra splitting and concatenation operations . Specifically , W Q , W K , W V and Q , K , V in Eq . ( 3 ) is split into H sub - matrices , yielding H heads . For the h - th head , the output is computed by :

• sentence : The argument .

Here we provide some additional details of all the datasets used in this work . Complexity of Styles in Datasets : As discussed in Section 5.2 , we consider three tasks - sentiment , discourse and fine - grained text style transfer . As prepossessing , we remove non - essential special characters and lowercase all sentences . Except for the Yelp dataset , no pruning is done based on sentence length . The vocab size during training was limited at 25k unless mentioned otherwise .

( 2 ) A number of firms have taken steps to make their online business more efficient and more efficient . New York - based Gartner says that new companies such as AT & T , Bell , IBM ...

Our main theoretical result , and the foundation of our modeling approach , can be stated as follows : in any language understanding task that can be modeled compositionally , data for the task exhibits symmetries in the sense of Definition 1 . We explain , formalize , and prove this statement below .

MedNLI Is Not Immune : Natural Language Inference Artifacts in the Clinical Domain

Entity linking consists of disambiguating entity mentions M from one or more documents to a target knowledge base , KB , containing unique entities . We assume that each entity e ∈ KB is represented using a set of attribute - value pairs

The average compression ratio from EW to SEW page in SWIPE document pairs is 0.87 , suggesting that SEW pages are not significantly shorter than their EW matches . In fact , 26 % of document pairs have a compression ratio larger than 1 , indicating that is not infrequent for the simplification of a document to be longer than the original document .

As we can see , QuEst+Vis-2 model outperforms the baseline with p - value = 0.002 . Thus , visual features significantly improve the performance of featurebased QE systems compared to the monomodal QE counterparts .

Algorithm 2 Efficient NCE loss estimation

Step 2b : Training the ranker We use BERT ( Devlin et al . , 2019 ) and MT - DNN 2 ( base and large ) to train a ranker . For training , we create five splits : ( 1 ) one in - topic split using a random subset from all four topics and ( 2 ) four

3 . Everything Is Fine Heuristic : a premisehypothesis pair satisfies this heuristic if the hypothesis contains one or more of the same MeSH entities as the premise ( excluding the patient entity , which appears in almost all notes ) and also contains : ( 1 ) a negation word or phrase ( e.g. , does not have , no finding , no , denies ) ; or ( 2 ) a word or phrase that affirms the patient 's health ( e.g. , normal , healthy , discharged ) .

6 https : / / github.com / google-research / google-r esearch / tree / master / rouge 7 https : / / github.com / Tiiiger / bert_score 8 https : / / github.com / abisee / pointer-generator 9 https : / / github.com / artmatsak / cnn-dailymail 10 https : / / github.com / EdinburghNLP / XSum

• We propose a technique to disable the learned features which are irrelevant or harmful to the classification task so as to improve the classifier . This technique and the word clouds form the human - debugging framework -FIND .

Due to the similarities between different languages , such as words , grammar , and semantics , multilingual models ( Devlin et al . , 2019 ; Conneau and Lample , 2019 ; Conneau et al . , 2020 ; Chi et al . , 2022 ) have been shown to generalize to unseen languages in a wide range of tasks . In machine translation , multilingual models exhibit the ability to perform zero - shot transfer in that they can translate on unseen language pairs ( Zoph et al . , 2016 ; Johnson et al . , 2017 ) . With the help of self - supervised learning , model can better acquire language - invariant representation , thus improving zero - shot machine translation .

where W g is a trainable parameter . During training process , we set γ t to the ground - truth labelγ t .

Figure 1 : An illustrative example of schema translation from English to Chinese . 1 -4 denotes headers with abbreviation , polysemy , verb - object phrase and special symbol , respectively .

The comparison of FULL , DENSE , and DENSE10 illustrates how well our data approximates the real distribution of annotations for each tangram , and the advantage of DENSE . Figure 4 shows the complete distribution of values . Comparing DENSE10 and DENSE , the rankings of the tangrams are largely the same with the additional annotations : for SND , Spearman 's rank correlation coefficient is r ( 72 ) = .78 , p ≪ .001 ; for PND , r ( 72 ) = .87 , p ≪ .001 ; for PSA , r ( 72 ) = .76 , p ≪ .001 . The tangrams sampled for DENSE represent well the distribution of tangrams along the different measures , as illustrated by the red highlights in Figure 4 .

Because the original dataset was intended for per - token error prediction , each question has per - token information that includes whether the student translated the token correctly , as well as Universal Dependencies tags such as part of speech and morphology labels . We use the full question text , rather than individual tokens , for our task , and combine the labels such that if a Duolingo user incorrectly translated one or more tokens in a question , the entire question is marked incorrect . We do not use any additional features .

For each heuristic , we subset the complete dataset to find pattern - satisfying premise - heuristic pairs . We use this subset when performing the χ 2 tests .

Although a model 's scale is important for its performance , it comes with additional efficiency and computational costs , among other considerations ( Bommasani et al . , 2021 ) . Fine - tuning large pre - trained language models is usually timeconsuming and expensive , and it also requires a large number of manually annotated examples . A possible direction that alleviates these requirements is to use adapter - based models ( Rebuffi et al . , 2017 ; Pfeiffer et al . , 2021 ) and other techniques for efficient training ( Lester et al . , 2021 ; Ben Zaken et al . , 2022 ) .

For neural prediction functions , the simplest - toimplement confidence function is likely MAX - PROB , shown here applied to a three - way sentiment analysis task :

To investigate the use of NLP tools for studying the cognitive traces of recollection versus imagination in stories , we collect and release HIP - POCORPUS , a dataset of imagined and recalled stories . We introduce measures to characterize narrative flow and influence of semantic vs. episodic knowledge in stories . We show that imagined stories have a more linear flow and contain more commonsense knowledge , whereas recalled stories are less connected and contain more specific concrete events . Additionally , we show that our measures can uncover the effect in language of narrativization of memories over time . We hope these findings bring attention to the feasibility of employing statistical natural language processing machinery as tools for exploring human cognition . Figure 4 : We extract phrases from the main themes of recalled ( left ) and imagined ( right ) stories , using RAKE ( Rose et al . , 2010 ) ; size of words corresponds to frequency in corpus , and color is only for readability .

We conceptualize dataset artifacts at the lexical level as emergent correlations between tokens and labels in input data , consistently to lexical annotation artifacts in NLI ( Gururangan et al . , 2018 ) . As such , given a target class c , we formally define lexical artifacts L c as the set of highly - discriminating 2 tokens for c , which comprise authentic artifacts A c -items that potentially carry useful information for the class at hand -and spurious artifacts S c -items that are spuriously ( or undesirably ) associated to the target class -such that L c = A c ∪ S c . In the context of hate speech detection , we consider the hateful class as c unless otherwise specified and simplify the notation ( i.e. , from " • c " to " • " ) .

We compare two traditional baselines : Majority and Random ; and five traditional methods for ob - taining text representations : Topic Modeling using BERT topic ( Grootendorst , 2022 ) , TF - IDF ( Sammut and Webb , 2010 ) , W ord2vec ( Mikolov et al . , 2013 ) , Glove ( Pennington et al . , 2014 ) , and BERT ( Devlin et al . , 2019 ) 5 . For Topic Modeling , we select five topics and match them with our computing careers . Additionally , we fine - tuned BERT embeddings with our training data in two ways :

In this work , we proposed a multiple - choice question generation method that can be used to fine - tune the state - of - the - art UnifiedQA model and improve its performance on an unseen and out of domain dataset . Our contributions are :

On the other hand , neural models ( Transformerbased and BiLSTM - CRF ) performed better . All of them performed better on the test set than on the development set , which shows good generalization ability . The BiLSTM - CRF model fed with different combinations of Transformer - based word embeddings and subword embeddings outperformed multilingual BERT and Spanish monolingual BETO . The model fed with codeswitch , BPE , and character embeddings ranked first and was significantly better than the result obtained by the model fed with BETO+BERT , BPE , and character embeddings .

XNLI is an evaluation dataset for cross - lingual NLI that covers 15 languages . The dataset is human - translated from the development and test sets of the English MultiNLI dataset . Given a sentence pair of premise and hypothesis , the task is to classify their relationship as entailment , contradiction , and neutral . For zero - shot cross - lingual transfer , we train on the English MultiNLI training set , and apply the model to the test sets of the other languages . For translatetrain , we train on translation data that come with the dataset 4 .

. We therefore hypothesise here that the highly dimensional and contextualised word - level representations from BERT are already enough and do not benefit from the extra information provided by the visual features .

For all neural - based models , we experiment with the all three integration strategies ( ' embed ' , ' annot ' and ' last ' ) and all three fusion strategies ( ' conc ' , ' mult ' and ' mult2 ' ) presented in Section 3.2 . This leads to 6 multimodal models for each BiRNN and BERT - BiRNN . In Tables 2 and 4 , as well as in Figures 2 and 3 , we report the top three performing models . We refer the reader to the Appendix for the full set of results .

In a similar architecture , but with multi - task learning , report that multilingual QE models outperform bilingual models , particularly in less balanced quality label distributions and lowresource settings . However , these two papers are focused on sentence - level QE and to the best of our knowledge , no prior work has been done on multilingual , word - level QE models .

Note that since COMET - QE is the state - of - theart quality estimator , it is a very strong baseline for the reranking stage where the goal is to find a better translation . The fact that we can match its hallucinatory rate reduction by analyzing model inner workings has value from different perspectives .

Since SciQ is a multiple - choice dataset , we must add distractors to each question we generate , to match the format of SciQ. A simple solution to this problem is to select random distractors among answers to other similar questions generated from the dataset of sentences we gathered . Obviously , selecting random distractors may lead to a fine - tuning dataset that is too easy to solve . Therefore , we propose another strategy that selects hard distractors for each question . To do so , starting from our synthetic dataset with random distractors , we finetune RoBERTa ( Liu et al . , 2019 ) using the standard method of training for multiple choices question answering . Each pair question / choice is fed to RoBERTa and the embedding corresponding to the first token ( " [ CLS ] " ) is given to a linear layer to produce a single scalar score for each choice . The scores corresponding to every choice for a given question are then compared to each other by a softmax and a cross - entropy loss . With this method , RoBERTa is trained to score a possible answer for a given question , based on whether or not it is a credible answer to that question . For each question , we then randomly select a number of candidate distractors from the answers to other questions and we use our trained RoBERTa to score each of these candidates . The 3 candidates with the highest scores ( and thus the most credible answers ) are selected . The idea is that during this first training , RoBERTa will learn a large amount of simplistic logic . For example , because of the initial random selection of distractors , it is highly unlikely that even one of the distractors will be close enough to the question 's semantic field . Furthermore , a lot distractors have an incorrect grammar ( eg : a distractor might be plural when the question expects a singular ) . Therefore , in this initial training , RoBERTa might learn to isolate the answer with a corresponding semantic field or the one with correct grammar . The re - selection then minimizes the amount of trivial distractors and models trained on this new refined dataset will have to focus on deeper and more meaningful relations between the questions and the answers . The process is better shown in Figure 4 , and an example of refined distractors can be found in Figure 3 .

We train a realis event tagger ( using BERT - base ; Devlin et al . , 2019 ) on the annotated literary events corpus by Sims et al . ( 2019 ) , which slightly outperforms the original author 's models . We provide further training details in Appendix B.1 .

In this section , we focus on the last step in a parsingas - tagging pipeline , which is to decode a tag sequence into a tree . The goal of the decoding step is to find a valid sequence of tags t * that are assigned the highest probability under the model for a given sentence , i.e. ,

We mainly evaluate our approach in Machine Translation ( MT ) . We select the most popular MT benchmark -WMT14 English - German ( En - De ) translation task , which is also a touchstone for seq2seq evaluation , as our main test bed . To compare with previous work , we also evaluate WMT14 English - French ( En - Fr ) translation . We follow the standard way to train and evaluate evaluate WMT14 En - De and En - Fr . As Ott et al . ( 2018 ) , we use a joint source - target dictionary of 32 K Byte Pair Encoding ( BPE ) for En - De , and 40 K BPE for En - Fr . We mainly use sacreBLEU ( Post , 2018 ) for evaluation .

The structure of the final dataset is described in Section F. For reproducibility of results , we create fixed splits for in - and cross - topic experiments . 9 , we show the synonyms used for filtering prior to the argument and stance classification step . We filtered out all sentences that did not contain tokens from the topic they belong to or any synonyms defined for this topic .

subjective experience of their own gender , which encompasses a diverse range of experiences and expressions ( WHO , 2021 ; NIH ; Prince , 2005 ) , eg . cisgender , transgender , non - binary etc . Historically , there are several cultures where gender is understood as a spectrum , for example , the Bugis people of Indonesia recognize five genders ( Davies , 2007 ) . While there are nations that legally acknowledge gender exclusively as a binary ( female or male ) ( EqualDex , 2022 ) , an increasing number of jurisdictions recognize gender as a broader concept , including the USA ( U.S. Dept of State , 2022 ; EqualDex , 2022 ) .

For the first task , on average , 14.33 out of 30 features were disabled and the macro F1 scores of the 20Newsgroups before and after debugging are 0.853 and 0.828 , respectively . The same metrics of the Religion dataset are 0.731 and 0.799 . This shows that disabling irrelevant features mildly undermined the predictive performance on the indistribution dataset , but clearly enhanced the performance on the out - of - distribution dataset ( see Figure 8 , left ) . This is especially evident for the Atheism class for which the F1 score increased around 15 % absolute . We noticed from the word clouds that many prominent words for the Atheism class learned by the models are person names ( e.g. , Keith , Gregg , Schneider ) and these are not applicable to the Religion dataset . Forcing the models to use only relevant features ( detecting terms like ' atheists ' and ' science ' ) , therefore , increased the macro F1 on the Religion dataset .

Intelligent and adaptive online education systems aim to make high - quality education available for a diverse range of students . However , existing systems usually depend on a pool of hand - made questions , limiting how finegrained and open - ended they can be in adapting to individual students . We explore targeted question generation as a controllable sequence generation task . We first show how to fine - tune pre - trained language models for deep knowledge tracing ( LM - KT ) . This model accurately predicts the probability of a student answering a question correctly , and generalizes to questions not seen in training . We then use LM - KT to specify the objective and data for training a model to generate questions conditioned on the student and target difficulty . Our results show we succeed at generating novel , wellcalibrated language translation questions for second language learners from a real online education platform .

( 1.00 ) death penalty CON god . And yet , while the Church exhorts civil authorities . ( 0.41 ) school uniforms CON resources . The lack of a uniform system for the collection and use of data , as well as insufficient funding to collect it , are major barriers that limit access to information on student achievement in schools with high concentrations of students from low - income families . ( 0.92 ) school uniforms PRO fun : The kids are having a lot of fun . ( 0.38 ) gun control PRO homicides . In addition to being an effective crime deterrent and reducing suicides , research has shown that defensive firearm use prevents more than 80 percent of all gun - related violent crimes including homicides , robberies , assaults , rapes , and aggravated assaults [ 1 ] . ( 0.96 ) gun control PRO prevent ; You liberals need to wake up and smell the coffee . ( 0.43 ) marijuana legalization PRO safer : Legalizing cannabis will help reduce crime rates ( especially violent crimes ) and make society safer overall . ( 0.96 ) marijuana legalization PRO benefits . Decrease amount of police officers needed 6 . ( 0.37 ) minimum wage PRO poor : Raising the minimum wage will not only benefit those working full time but also reduce government expenditures on social services such as food stamps and Medicaid which disproportionately affect the poor . ( 0.97 ) minimum wage CON cost : If you raise the price of a Big Mac to $ 10 and then pay an extra dime or two per burger so that it 's still only $ 9 ... well , maybe your business is n't worth saving at all because there just wo n't be anyone willing to eat out after they ' ve had their fill ( unless perhaps they 're on welfare ) and now have no job and ca n't afford health care eitherbut I doubt very seriously that most people making those kinds of wages / salaries would suddenly decide not to work if forced to shoulder the added cost . ( 0.44 ) 12 : Generated arguments with the Arg - CTRL REDDIT . Text in bold shows the given control code , text afterwards represents the generated argument . Numbers in brackets after the text show the quality score as predicted by the argument quality model .

The article discusses how Rod Blagojevich , a former Illinois governor , has let his hair go while serving his prison sentence . The photos show the former governor with white hair , rather than the black hair that was his trademark as a politician .

In faithfulness evaluation , erasure - based methods examine the drop in prediction scores by removing the important tokens from the input ( Section 2.1.1 ) . However , the drop in the prediction scores may be the result of the altered , out - of - distribution inputs ( Bastings and Filippova , 2020 ) . To overcome this problem , we design a new strategy to evaluate faithfulness by relying on cross - lingual models and datasets . Before diving into details , let us recall Corrolary 2 from Jacovi and Goldberg ( 2020 ) .

We also compare with Mesgar and Strube ( 2016 ) , which feeds subgraphs as input features . For a fair comparison , we input document representations from XLNet to this model , equip it with a two - layer DNN and softmax layer for feature extraction and classification . Furthermore , we compare our method against existing state - of - the - art models for each task to evaluate the effectiveness of our approach .

We base our new aspect detection dataset on the UKP Sentential Argument Mining Corpus ( UKP - Corpus ) by Stab et al . ( 2018b ) , as it already contains sentence - level arguments and two of the control codes we aim to use : topics and stance labels . More precisely , it contains 25,474 manually labelled sentences for eight controversial topics in English . Each sample consists of a topic and a sentence , labelled as either being supporting , attacking , or no argument towards the given topic . As we are only interested in arguments , we do not consider the non - argumentative sentences .

In this section , we explain how we achieve over 23fps real - time inference , by using MediaPipe Holistic for generating poses ( as an ISLR encoder ) and our pose - based models ( as decoder ) that recognizes the sign at any given window .

BERT and related pre - training methods ( Baevski et al . , 2019;Lan et al . , 2020 ) train a large neural network to perform the cloze task . These models learn the probability p data ( x t |x \t ) of a token x t occurring in the surrounding context

a CRF model using the token , its lemma , part - ofspeech tag and mat2vec embedding as features . 7

( 2 ) the category of each action in the action chain A. The δ ( . ) outputs the prediction , and the Θ is the learnable parameter .

Quality Estimation ( QE ) for Machine Translation ( MT ) ( Blatz et al . , 2004;Specia et al . , 2009 ) aims to predict the quality of a machine - translated text without using reference translations . It estimates a label ( a category , such as ' good ' or ' bad ' , or a numerical score ) for a translation , given text in a source language and its machine translation in a target language ( Specia et al . , 2018b ) . QE can operate at different linguistic levels , including sentence and document levels . Sentence - level QE estimates the translation quality of a whole sentence , while document - level QE predicts the translation quality of an entire document , even though in practice in literature the documents have been limited to a small set of 3 - 5 sentences ( Specia et al . , 2018b ) .

.. q j m a j m , a i ∈ { < Y>,<N > } LM - KT Given the sequential nature of student learning over time , we can easily frame knowledge tracing as an autoregressive language modeling task . Given a dataset D of students s 1 , s 2 , ... , s |D| , we employ the standard training objective of finding the parameters θ KT that minimizes

We introduce Electric , an energy - based cloze model for representation learning over text . Like BERT , it is a conditional generative model of tokens given their contexts . However , Electric does not use masking or output a full distribution over tokens that could occur in a context . Instead , it assigns a scalar energy score to each input token indicating how likely it is given its context . We train Electric using an algorithm based on noise - contrastive estimation and elucidate how this learning objective is closely related to the recently proposed ELECTRA pre - training method . Electric performs well when transferred to downstream tasks and is particularly effective at producing likelihood scores for text : it reranks speech recognition n - best lists better than language models and much faster than masked language models . Furthermore , it offers a clearer and more principled view of what ELECTRA learns during pre - training .

While covering similar domains , Wikidata and the TAC - KBP Reference KB have different schemas . Wikidata is more structured and entities are associated with statements represented using attribute - value pairs , which are short snippets rather than full sentences . The TAC - KBP Reference KB contains both short snippets like these , along with the text of the Wikipedia article of the entity . The two KBs also differ in size , with Wikidata containing almost seven times the number of entities in TAC KBP .

We summarize the paper 's main claims in the last two paragraphs of the introduction A4 . Have you used AI writing assistants when working on this paper ?

We thank the four anonymous reviewers whose feedback and suggestions helped improve this manuscript . The first author was supported by the National Institute of Standards and Technology 's ( NIST ) Professional Research Experience Program ( PREP ) . This research was also supported by the DARPA KAIROS program . The views and conclusions contained in this publication are those of the authors and should not be interpreted as representing official policies or endorsements of NIST , DARPA , or the U.S. Government .

For fair comparison , we keep the Transformer decoder unchanged and validate different position representation strategies on the encoder . We conduct all experiments on the TRANSFORMER - BIG with four V100 GPUs .

One classical way to compress the training dataset is data selection . Data selection methods choose a subset of effective training samples on the basis of a number of heuristic measures , for example , cluster centers ( Sener and Savarese , 2018 ) , diversity ( Aljundi et al . , 2019 ) , and likelihood of models ( Moore and Lewis , 2010 ) . Although the data selection methods effectively work for efficient model training and several applications , such as active learning ( Sener and Savarese , 2018 ) and continual learning ( Aljundi et al . , 2019 ) , their performance is clearly restricted because they rely on the existence of representative samples that are effective for model training in the original dataset .

We evaluate Electric on GLUE and SQuAD ( Rajpurkar et al . , 2016 ) , where Electric substantially outperforms BERT but slightly under - performs ELECTRA . However , Electric is particularly useful in its ability to efficiently produce pseudo - likelihood scores ( Salazar et al . , 2020 ) for text : Electric is better at re - ranking the outputs of a speech recognition system than GPT-2 ( Radford et al . , 2019 ) and is much faster at re - ranking than BERT because it scores all input tokens simultaneously rather than having to be run multiple times with different tokens masked out . In total , investigating Electric leads to a more principled understanding of ELECTRA and our results suggest that EBMs are a promising alternative to the standard generative models currently used for language representation learning .

We compare our proposed method with the following strong baselines .

where sequence x ( j ) contains the full s j d j < G > q j sequence . At test time , we generate tokens w 1 ... w n conditioned on the s j d j < G > prefix .

Figure 1 : Example input and outputs for our LM - based knowledge tracing model ( middle ) and question generation model ( bottom ) for an online reverse language translation task ( top ) . A question in this task consists of a target phrase for the student , in this case a Spanish learner , to translate ( e.g. " the woman " ) .

We share the parameters in the two - stream mechanism to retain more information about the translation decoding procedure for reconstruction . Computationally , we denote the representation of t - th layer in Translation and Reconstruction streams as h and g , respectively . The first layer representations of the two streams are both set as the precedent word embedding , i.e. h 0 t = g 0 t = e ( y t ) . For each layer n = 1 , 2 , ... , N , the representations of two streams are calculated as follows : 3

Inspired by Shaw et al . ( 2018 ) , we model the target header and its context as a labeled directed graph and use the same formulation of relationaware self - attention as Shaw et al . ( 2018 ) . Here

Our proposed XL PE intuitively encourages SANs to learn bilingual diagonal alignment , so has the 2 Replace PEXL in Eq . ( 9 ) with PEIN - XL in Eq . ( 8) . potential to induce better attention matrices . We explore this hypothesis on the widely used Gold Alignment dataset 3 and follow Tang et al . ( 2019 ) to perform the alignment . The only difference being that we average the attention matrices across all heads from the penultimate layer ( Garg et al . , 2019 ) . The alignment error rate ( AER , Och and Ney 2003 ) , precision ( P ) and recall ( R ) are reported as the evaluation metrics . Tab . 3 summarizes the results . We can see : 1 ) XL PE allows SANs to learn better attention matrices , thereby improving alignment performance ( 27.4 / 26.9 vs. 29.7 ) ; and 2 ) combining the two strategies delivers consistent improvements ( 24.7 vs. 29.7 ) .

In this work , we focus on self - supervised , alignment - oriented training tasks using minimum parallel data to improve mBERT 's cross - lingual transferability . We propose a Post - Pretraining Alignment ( PPA ) method consisting of both wordlevel and sentence - level alignment , as well as a finetuning technique on downstream tasks that take pairs of text as input , such as NLI and Question Answering ( QA ) . Specifically , we use a slightly different version of TLM as our word - level alignment task and contrastive learning ( Hadsell et al . , 2006 ) on mBERT 's [ CLS ] tokens to align sentence - level representations . Both tasks are self - supervised and do not require pre - alignment tools such as FastAlign . Our sentence - level alignment is implemented using MoCo ( He et al . , 2020 ) , an instance discrimination - based method of contrastive learn- ing that was recently proposed for self - supervised representation learning in computer vision . Lastly , when finetuning on NLI and QA tasks for non - English languages , we perform sentence - level codeswitching with English as a form of both alignment and data augmentation . We conduct controlled experiments on XNLI and MLQA ( Lewis et al . , 2020 ) , leveraging varying amounts of parallel data during alignment . We then conduct an ablation study that shows the effectiveness of our method . On XNLI , our aligned mBERT improves over the original mBERT by 4.7 % for zero - shot transfer , and outperforms Cao et al . ( 2020 ) while using the same amount of parallel data from the same source . For translate - train , where translation of English training data is available in the target language , our model achieves comparable performance to XLM while using far fewer resources . On MLQA , we get 2.3 % improvement over mBERT and outperform XLM - R Base for zero - shot transfer .

COMET leverages contextual word embeddings of the source sentence , MT hypothesis , and reference ( or human post - edition ) extracted from pretrained cross - lingual models . The embeddings are combined and fed into a feed - forward network . It 's a quality estimation system and is trained with human assessments ( DA , HTER , MQM ) .

The values displayed diagonally across section I of Table 2 show the results for supervised , bilingual , word - level QE models where the model was trained on the training set of a particular language pair and tested on the test set of the same language pair . As can be seen in section V , the architecture outperforms the baselines in all the language pairs and also outperforms the majority of the best systems from previous competitions . In addition to the target word F1 - score , our architecture outperforms the baselines and best systems in target gaps F1 - score and source words F1 - score too as shown in Tables 5 and 6 . In the following sections we explore its behaviour in different multilingual settings .

We construct the dataset in two steps : collecting 3,158 English tables and then manually translating the schema of English tables to other languages . ( Pasupat and Liang , 2015 ) , in which they randomly select 2,108 multidomain data tables in English from Wikipedia with at least eight rows and five columns . Secondly , we manually collect 176 English tables from the search engine covering multiple domains like retail , education , and government . At last , we select all the tables that appear in the training set and development set from the Spider dataset ( Yu et al . , 2018 ) , which contains 200 databases covering 138 different domains . Finally , we obtained 3,158 tables with 11,979 headers in total .

Hypothesis : The air tasted through boiling armor -the taste of betrayal .

In the zero - shot setting , removing MoCo ( -MoCo ) performs similarly to -TLM , where we observe an accuracy drop of about 1 % compared to our full system . In translate - train , -MoCo outperforms -TLM and even matches the full system performance for 250k .

All our experiments were carried out on a single server with one NVIDIA Quadro GP100 GPU . The total computation time for generating and scoring translations was less than 24 hours .

As the GLUE benchmark results indicate ( Table 6 ) , MABEL preserves semantic knowledge across downstream tasks . On average , MABEL performs marginally better than BERT ( 82.0 % vs. 81.8 % ) , but not as well as BERT fine - tuned beforehand on the NLI task with MNLI and SNLI data ( BERT - NLI ) , at 82.0 % vs. 82.1 % . Other bias mitigation baselines lag behind BERT , but the overall semantic deterioration remains minimal .

Local ED Model . Our local ED model takes words and N [ MASK ] tokens corresponding to the mentions in the document . The model then computes the embedding m ′ e ∈ R H for each [ MASK ] token using Eq.(2 ) and predicts the entity using softmax over the K entity candidates :

Our held - out test bed is the TAC - KBP 2010 data ( LDC2018T16 ) which consists of documents from English newswire , discussion forum and web data ( Ji et al . , 2010 ) . 4 The target KB ( KB test ) is the TAC - KBP Reference KB and is built from English Wikipedia articles and their associated infoboxes ( LDC2014T16 ) . 5 Our primary training and validation data is the CoNLL - YAGO dataset ( Hoffart et al . , 2011 ) Table 2 describes the sizes of these various datasets along with the number of entities in their respective KBs .

We split our gold standard data set into roughly 70 % for training , 15 % for development and 15 % for testing . We also made sure that the objects in the test set do not appear in the training set . Our fine - tuning framework is based on the RoBERTabase model ( Liu et al . , 2019 ) . For the hyperparameters , we used a max sequence length of 192 , a batch size of 8 , learning rate initialized as 2e-5 , and train for 15 epochs . Each result is averaged over three runs with different random seeds . We report overall accuracy as well as precision , recall and F1 scores macro - averaged over the 3 classes .

We report the macro - averaged recall ( R ) , mean reciprocal rank ( MRR ) , and mean average precision ( MAP ) , which are the average values of the corresponding metrics over a set of n queries . Note that as those metrics are computed for a filter set of size k = |F q | ≪ |C| ( and not on the entire list of articles in C ) , we report them with the suffix " @ k " .

• MNLI : Multi - genre Natural Language Inference ( Williams et al . , 2018 ) . Given a premise sentence and a hypothesis sentence , the task is to predict whether the premise entails the hypothesis , contradicts the hypothesis , or neither . The dataset contains 393k train examples drawn from ten different sources . using dev - set model selection to choose the test set submission may alleviate the high variance of fine - tuning to some extent , such model selection is still not sufficient for reliable comparisons between methods ( Reimers and Gurevych , 2018 ) .

we can only use the ability to use electronic products without game function .

Caption : Son , we finally attracted the fifty - foot - and - over demographic ! Both bad : The humanauthored explanation misses the direct reference to the movie " Attack of the 50 - Foot - Woman " ( 1958 ) , and the machine focuses on non - sequiturs like age 18 / viewers / etc .

The similarity metrics used in our experiments are L 2 and cos . For the sake of convenience , in the following text , we abbreviate cos and L 2 square similarity invariance as similarity invariance .

A substitution cipher is a cipher that is created by substituting each plaintext character with another character according to a substitution table called the key . We define major terms in the following subsections .

Furthermore , the most robust setting agrees with the form of MLS since they both allocate zero probability to the source category 's tokens , which further proves the robustness of MLS . in each bin .

Training Document Generation We create the final training documents for the argument generation model by concatenating all arguments that have the same topic , stance , and aspect ( i.e. the same control code ) . Further , we aggregate all arguments that include an aspect with the same stem into the same document ( e.g. arguments with cost and costs as aspect ) . To cope with limited hardware resources , we restrict the total number of arguments for each topic and stance to 100,000 ( i.e. 1.6 M over all eight topics ) . Also , as some aspects dominate by means of quantity of related arguments and others appear only rarely , we empirically determine an upper and lower bound of 1,500 and 15 arguments for each document , which still allows us to retrieve the above defined amount of training arguments .

We introduce the XM3600 dataset as a benchmark for evaluating the performance of multilingual image captioning models . The images in the dataset are geographically diverse , covering all inhabited continents and a large fraction of the world population . We believe this benchmark has the potential to positively impact both the research and the applications of this technology , and enable ( among other things ) better accessibility for visually - impaired users across the world , including speakers of lowresource languages .

1 . The input - label mapping , i.e. , whether each input x i is paired with a correct label y i .

• MULTIVERS ( Wadden et al . , 2022b ) , formerly known as LongChecker , uses the Long - Former ( Beltagy et al . , 2020 ) for claim verification to address the long input evidence problem . We use a model checkpoint finetuned on FEVER . 6

In OR - QuAC , we compare our models with previously proposed dense retrieval approaches in conversational search , CQE ( Lin et al . , 2021b ) and ConvDR ( Yu et al . , 2020 ) . Both of them utilize the standalone question q ′ t to mine hard negatives and knowledge distillation from off - the - shelf retrievers trained on ODQA , regarding it as a teacher model . Although they were not tested on QReCC , we can indirectly compare them with others using DPR with CQR Negs instead .

Our architecture relies on the XLM - R transformer model ( Conneau et al . , 2020 ) to derive the representations of the input sentences . XLM - R has been trained on a large - scale multilingual dataset in 104 languages , totalling 2.5 TB , extracted from the CommonCrawl datasets . It is trained using only RoBERTa 's ( Liu et al . , 2019 ) masked language modelling ( MLM ) objective . XML - R was used by the winning systems in the recent WMT 2020 shared task on sentence - level QE ( Ranasinghe et al . , 2020a;Lee , 2020 ; . This motivated us to use a similar approach for wordlevel QE .

Machine translation evaluation has conventionally relied on reference , where outputs are compared against translations written by humans . This is in contrast to the reference - free manner in which translation quality is directly assessed with the source text . Reference - free evaluation ( Napoles et al . , 2016;Thompson and Post , 2020;Agrawal et al . , 2021 ) has the potential to free the evaluation model from the constraints of labor - intensive annotations , allowing it to pivot easily to new domains . In this way , reference - free evaluation metrics are substantially more scalable and have lately been in the spotlight .

Then , we define three sorts of entity types to distinguish the target header from its context . Specifically , for a token in the target header , we assign a special edge Target point to itself , denoting the entity type . For tokens in S and V , we assign them different edges point to themselves , e.g. , Header , and Value respectively . Figure 2 illustrates an example graph ( with actual edges and labels ) and its induced relational matrix R. Initial Token Embedding . We obtain the initial token embedding by a pre - trained transformer encoder before feeding it to the ration - aware transformer . To obtain the input sequence , each element in S and V are firstly concatenated with a vertical bar " | " . Then , the target header H , the rest of the headers S , and the selected cell values V are concatenated by a separator symbol " [ sep ] " . At last , following , an additional source language token " hsrci " is added at the front to help the pretrained model identify the source language . The encoder then transforms the final input sequence into a sequence of embedding

Drafting counter - arguments is an important skill for debating , to provide constructive feedback , and to foster critical thinking . We lean onto the work of Wachsmuth et al . ( 2018 ) who describe a counterargument as discussing the same aspect as an initial argument , but with a switched stance . Hence , given our defined control codes , our model is especially fit for counter - argument generation . Unlike current models for this task , we do not require a specific dataset with argument and counterargument pairs ( Hidey and McKeown , 2019 ; . Also , in contrast to the model by that implicitly integrates inputrelated " Keyphrases " into the process of counterargument generation , our model is able to concentrate on every aspect of the input explicitly and with a separate argument , allowing for more transparency and interpretability over the process of counter - argument generation . We exemplary show how the combination of aspect detection and controlled argument generation can be successfully leveraged to tackle this task . For that , we manually compose initial arguments for the topics nuclear energy and school uniforms . Then , we automatically detect their aspects and generate a counterargument for each aspect by passing the topic , opposite stance of the original argument , and one of the aspects into the Arg - CTRL CC . For both topics , the Arg - CTRL CC produces meaningful counterarguments based on the detected aspects ( see Table 7 ) .

where y j denotes the jth token of the generated output y and T is the length of the final output . The generator is based on the T5 architecture and uses cross attentions to model the interactions between retrieved passages .

Finally , we also conduct human studies to compare summaries of GPT-3 w / o and w / SumCoT. Results ( as shown in Table 6 ) indicate that the Sum - CoT technique further improves the performance of the standard zero - shot paradigm in all dimensions , particularly coherence and relevance .

We conducted experiments on Quora 1 , Twitter ( Lan et al . , 2017 ) and MSCOCO ( Lin et al . , 2014 ) benchmark datasets , and followed the same setting in previous works ( Lin et al . , 2014 ; Liu et al . , 2020 ; Ding et al . , 2021 ) . For meta - learning , we choose a different source task 's labeld train - set from the target task to randomly construct meta tasks . Appendix Table 4 describes more details .

For each of these 466 target words , we also obtain a list of words from WordNet , which are 1 Levenshtein distance away . We treat this word list as the pseudo - match list . We also consider the number of tokenizations for each target word , excluding their pseudo - match list as well as by excluding all those which are equally close to or closer to a word in the pseudo - match list than they are to the target word . We also compute the statistics of those with exact matches .

A simple yet effective stateless scoring function as an alternative to more complicated length - adjusted counterparts is devised , and we show that it works well and helps further in finding diverse texts .

We thank Tilman Beck and Nandan Thakur for their support in the human evaluation ( Section 7.1 ) . This work has been supported by the German Research Foundation within the project " Open Argument Mining " ( GU 798/25 - 1 ) , associated with the Priority Program " Robust Argumentation Machines ( RATIO ) " ( SPP-1999 ) .

• SST : Stanford Sentiment Treebank ( Socher et al . , 2013 ) . The tasks is to determine if the sentence is positive or negative in sentiment .

Given the source src and target tgt wordpieces encoded as e src and e tgt , our goal is to predict the label y.

1 . Candidate generation : The objective of this stage is to select K candidate entities E ⊂ KB for each mention m ∈ M , where K is a hyperparameter and K < < |KB| .

To test whether a QE model trained on a particular language pair can be generalised to other language pairs , different domains and MT types , we performed zero - shot quality estimation . We used the QE model trained on a particular language pair and evaluated it on the test sets of the other language pairs . Non - diagonal values of section I in Table 2 show how each QE model performed on other language pairs . For better visualisation , the nondiagonal values of section I of Table 2 show by how much the score changes when the zero - shot QE model is used instead of the bilingual QE model . As can be seen , the scores decrease , but this decrease is negligible and is to be expected . For most pairs , the QE model that did not see any training instances of that particular language pair outperforms the baselines that were trained extensively on that particular language pair . Further analysing the results , we can see that zero - shot QE performs better when the language pair shares some properties such as domain , MT type or language direction . For example , En - De SMT ⇒ En - Cs SMT is better than En - De NMT ⇒ En - Cs SMT and En - De SMT ⇒ En - De NMT is better than En - Cs SMT ⇒ En - De NMT .

Although compression has the ability to identify bias in most cases , some metrics still show little or no correlation with compression rate . These results suggest that gender information comprises only one facet of embedded bias in the representations . Other factors that may influence these metrics are not considered or measured , such as the connection between a name and a profession .

We estimate the importance of each sentence for a specific entity pair . Low - scored sentences will be treated as non - evidence , and in principle , can be removed without changing DocRE predictions .

The two other selection methods extract sentences from SciQ itself and therefore are not entirely unsupervised but rather simulate a situation where we have access to unannotated texts that precisely describe the domains of interest such as a school book for example . The SciQ dataset includes a support paragraph for each question ( see Figure 1 ) . Pooled together , these support paragraphs provide us with a large dataset of texts about the domains of interest . We gather the paragraphs corresponding to all questions and split them into sentences to produce a large set of sentences that are no longer associated with any particular question but cover all the topics found in the questions . We compare two different setups . In the first one , we include all the sentences extracted from the train , validation and test sets thus simulating a perfect selection of sentences that cover all the knowledge expressed in the questions . Still , we only use the support paragraphs and not the annotated questions themselves . As compared to the classical supervised paradigm , this setting removes all annotation costs for the application developer , but it still requires to gather sentences that are deemed useful for the test set of interest . We then compare this setup with another one , where only the sentences from the train set are included . This scenario arguably meets more practical needs since it would suffice to gather sentences close to the domain of interest . The number of sentences for each dataset is presented in Table 1 .

In addition to token - level decoding attention , we propose a graph - selection attention mechanism ( GSA ) to inform the decoder with learned hierarchical information , while realizing the sentencelevel content selection . In each decoding step t , our decoder first obtains a graph context vector , c t G , which entails the global information of the latent hierarchical graph . We first compute the graph - level attention distribution a t G by ,

son 's r of 0.602 . This shows that even when using better word presentations , the visual features help to get further ( albeit modest ) improvements . Table 3 shows an example of predicted scores at the sentence - level for the baseline model ( BiRNN ) and for the best multimodal BiRNN model ( BiRNN+Vis - embed - mult2 ) . The multimodal model has predicted a closer score ( -0.002 ) to the gold MQM score ( 0.167 ) than the baseline model ( -0.248 ) . The French translation is poor ( cumulative - split is , for instance , not translated ) as the low gold MQM score shows . However , the ( main ) word stopwatch is correctly translated as chronomètre in French . Since the associated picture indeed represents a stopwatch , one explanation for this improvement could be that the multimodal model may have rewarded this correct and important part of the translation .

In summary , our work contributes to the research on Automated Text Scoring with the following main findings : ( i ) text spans contained in an answer that increase the answer quality , compared to those decreasing answer quality , are more likely to be identified by human graders ; ( ii ) there exists a certain level of alignment between DL - based graders and human graders regarding the words they think are important in assessing answer quality no matter whether they agree on the quality score of an answer ; and ( iii ) the important words detected by DL - based graders can be potentially used to facilitate human grading , though more research efforts are required to understand how these words are to be utilized by human graders .

To ensure the annotation quality , we aggregate annotated results from three students for every dataset using majority vote . If all three students annotate differently from each other for an instance , we introduce a fourth student to arbitrate .

Further research on the effect of preprinting on peer review . We find that the preprinted papers have consistently higher ratings ( for both Soundness , Excitement , and reviewer confidence ) , get more recommendations for awards , and a higher acceptance rate . There are several possible underlying causes ( from reviewer biases to higher initial paper quality and benefits of community feedback ) , which likely all contribute to this effect . Since these factors necessitate different actions if they were the major contributor to the observed effect , for informed policy decisions it is necessary to establish how they intermix . We observe however that although the present 1 - month embargo policy does not solve this problem , it is effective at mitigating it , since we only had 13.8 % such papers .

Finally , we perform cross - entropy loss on all query samples . Note that in the multi - label setting , as an utterance may have multiple labels , we need to consider |C| labels for each query sentence . The classification loss is calculated as :

Per MedNLI 's data use agreement requirements , we do not attempt to identify any patient , provider , or institution mentioned in the de - identified corpus . Additionally , while we provide AFLite easy and difficult partition information for community use in the form of split - example ids and a checksum , we do not share the premise or hypothesis text associated with any example . Interested readers are encouraged to complete the necessary training and obtain credentials so that they can access the complete dataset ( Romanov and Shivade , 2018;Goldberger et al . , 2000 ( June 13 ) .

[ SPEAKER ] 's question : [ QUESTION ] Respond " yes " if answered , " no " otherwise .

The record producer that produced the bluegrass album was born on 22 June , 1944 . This album inspired a Tony award winning musical . This musical had a character that was originated by Carmen Cusack .

Similarly , we use Eq . ( 3)∼ ( 5 ) to calculate multiple heads of SANs .

The court held that the defendant A made up facts , concealed the truth and defrauded others ' property for the purpose of illegal possession . The amount was large , and his behavior constituted the crime of fraud .

NLI ( Falke et al . , 2019 ) is an entailment - based method which takes the maximal entailment probability between summary and document sentence as the factual consistency score . As no dialogue - based entailment model is available , we compute the entailment probability between reference and generated summaries with a BERT - based entailment model trained on SNLI and MultiNLI datasets .

Results on PubMed and arXiv datasets are shown in Table 2 . Our models strongly outperform both extractive and abstractive baselines , suggesting the effectiveness of unifying section segmentation with summarization . The LEAD baseline , however , does not perform as well on long documents as it does on news articles . It is interesting to note that our models are trained with indirect signals , i.e. , binary sentence labels derived from reference summaries , and they remain quite effective at capturing salient content on long documents .

The results of multi - lingual STS benchmark are shown in Table 3 . For unsupervised XLM - R and mBERT without finetuning , we try several pooling methods and find that averaging over the first and the last layers yields the best results on STS . The poor results of pre - trained language models mean that the sentence embeddings of pre - trained language models do not capture the semantics in the cosine similarity space well . With unsupervised mSimCSE pre - training , it enhances the semantics of monolingual STS tasks , i.e. " ar - ar " and " es - es " .

Perturbed Layer We further study how perturbing representations from different layers affect generation performance . We conduct experiments on AdvGrad by gradually moving δ x or δ y from word embeddings to high - layer representations of the encoder or decoder , separately . In this procedure , the other side perturbation remains fixed on the word embedding . The results are illustrated in Figure 2 , where the horizontal axis ( Layer i d ) indicates the perturbed layer and the longitudinal axis reports the corresponding ROUGE - L score . 2 Because both BART encoder and decoder are composed of 12 transformer layers , we denote layer 0 as the word embeddings and layer 12 as the output representations of the encoder or decoder .

1,1 , ... , x m k i , t } are input modality sequences and m 1 , m 2 denote the two modality types , some modality inputs are missing with probability p ′ . Following Ma et al . ( 2021 ) , we assume that modality m 1 is complete and the random missing only happens on modality m 2 , which we call the victim modality . Consequently , we can divide the training set into the complete and missing splits , denoted as

We use the rank - based measures to evaluate the quality of the prediction including Mean Reciprocal Rank ( MRR ) and Hits@N. Their detailed definitions are introduced below :

Finally , we show ablation result for our codeswitching in translate - train . On average , codeswitching provides an additional gain of 1 % . ( Lewis et al . , 2020 ) 74.9 54.8 62.2 68.0 48.8 61.1 61.6 XLM - R Base ( Conneau et al . , 2020 ) 77.1 54.9 60.9 67.4 59.4 61.8 63.6 Translate - train mBERT from ( Lewis et al . , 2020 ) 77.7 51.8 62 Training mBERT with Word Alignments Cao et al . ( 2020 ) post - align mBERT embeddings by first generating word alignments on parallel sentences that involve English . For each aligned word pair , the L 2 distance between their embeddings is minimized to train the model . In order to maintain original transferability to downstream tasks , a regularization term is added to prevent the target language embeddings from deviating too much from their mBERT initialization . Our approach post - aligns mBERT with two self - supervised signals from parallel data without using pre - alignment tools . Wang et al . ( 2019 ) also align mBERT em- Table 5 : Ablation Study on XNLI . 250k , 600k , 2 M refer to the maximum number of parallel sentence pairs per language used in PPA . MoCo refers to our sentence - level alignment task using contrastive learning . TLM refers to our word - level alignment task with translation language modeling . CS stands for code - switching . We conduct an additional study repl TLM w/ MLM , which means instead of TLM training , we augment our sentence - level alignment with regular MLM on monolingual text . This ablation confirms that the TLM objective helps because of its word alignment capability , not because we train the encoders with more data and iterations .

linguistic knowledge ( Reif et al . , 2019 ) . Indeed , even an informal inspection of layer - wise intrasentence similarities ( Fig . 1 ) suggests that these models capture elements of linguistic structure , and those differ depending on the layer of the model . A grounded investigation of these regularities allows to interpret the model 's behaviour , design better pre - trained encoders and inform the downstream model development . Such investigation is the main subject of probing , and recent studies confirm that BERT implicitly captures many aspects of language use , lexical semantics and grammar ( Rogers et al . , 2020 ) .

We combined instances from all the language pairs and built a single word - level QE model . Our results , displayed in section II ( " All " ) of Table 2 , show that multilingual models perform on par with bilingual models or even better for some language pairs . We also investigate whether combining language pairs that share either the same domain or MT type can be more beneficial , since it is possible that the learning process is better when language pairs share certain characteristics . However as shown in sections III and IV of Table 2 , for the majority of the language pairs , specialised multilingual models built on certain domains or MT types do not perform better than multilingual models which contain all the data . Section IV shows the results of the state - of - the - art methods and the best system submitted for the language pair in that competition . NR implies that a particular result was not reported by the organisers . Zero - shot results are coloured in grey and the value shows the difference between the best result in that section for that language pair and itself .

Besides an accurate solution to the J(A , B ) , whether TFWSVD can obtain a performance gain is also decided by the properties of the target matrix W itself . TFWSVD is to capture the different importance of parameters . However , if the parameters in W equally contributed to the model performance , then the standard SVD should be good enough . Driven by these factors , we are interested in this question : Is there a method that can " foresee " when SVD will fail , and TFWSVD can help retain performance ?

Finally , we measure the calibration of our LM - KT models for both Spanish and French ( from En - glish ) learners , which is the crucial property for our downstream generation task . We bin our test data by predicted question difficulty , and plot the fraction of true correct answers in each bin . Figure 2 shows that LM - KT is well - calibrated , for both Spanish and French , meaning the predicted difficulty matches the empirically observed proportion of correct answers .

Crowdworker - constructed natural language inference ( NLI ) datasets have been found to contain statistical artifacts associated with the annotation process that allow hypothesis - only classifiers to achieve better - than - random performance ( Poliak et al . , 2018;Gururangan et al . , 2018;Tsuchiya , 2018 ) . We investigate whether MedNLI , a physician - annotated dataset with premises extracted from clinical notes , contains such artifacts ( Romanov and Shivade , 2018 ) . We find that entailed hypotheses contain generic versions of specific concepts in the premise , as well as modifiers related to responsiveness , duration , and probability . Neutral hypotheses feature conditions and behaviors that co - occur with , or cause , the condition(s ) in the premise . Contradiction hypotheses feature explicit negation of the premise and implicit negation via assertion of good health . Adversarial filtering demonstrates that performance degrades when evaluated on the difficult subset . We provide partition information and recommendations for alternative dataset construction strategies for knowledge - intensive domains .

All datasets and their splits used in the experiments are listed in Table 1 . We will explain each of them in the following sections . For each classification task , we ran and improved three models , using different random seeds , independently of one another , and the reported results are the average of the three runs . Regarding the models , we used 1D CNNs with the same structures for all the tasks and datasets . The convolution layer had three filter sizes [ 2 , 3 , 4 ] with 10 filters for each size ( i.e. , d = 10 × 3 = 30 ) . All the activation functions were ReLU except the softmax at the output layer . The input documents were padded or trimmed to have 150 words ( L = 150 ) . We used pre - trained 300 - dim GloVe vectors ( Pennington et al . , 2014 ) as non - trainable weights in the embedding layers . All the models were implemented using Keras and trained with Adam optimizer . We used iNNvestigate ( Alber et al . , 2018 ) to run LRP on CNN features . In particular , we used the LRP - propagation rule to stabilize the relevance scores ( = 10 −7 ) . Finally , we used Amazon Mechanical Turk ( MTurk ) to collect crowdsourced responses for selecting features to disable . Each question was answered by ten workers and the answers were aggregated using majority votes or average scores depending on the question type ( as explained next ) .

Entity mention detection and type assignment .

The distant supervision process relies on matching entities in a sentence with facts from the knowledge graph . To detect and identify the named entities in the articles , we use the DBpedia Spotlight ( Mendes et al . , 2011 ) entity linker . For the extraction of temporal and numerical entities , we use the spaCy 1 tool .

• We truncate longer input sequences token by token , if the input is formed from multiple sequences ( see Section B ) , i.e. , pairs , we start from the longest one .

• We have shown that simple unsupervised methods could be used to finetune existing multipurpose question answering models ( in our case UnifiedQA ) to new datasets or domains .

( t ) i is the i th row of W ( t ) , representing the label word embedding of class C i . To make the adaptation more robust , we add an instance - level attention mechanism via coefficient α j i . Such attention score is to measure the informative degree of each support instance . According to previous works ( Gao et al . , 2019a ; Dong et al . , 2020 ) , instances are not equally informative , and we should make those informative ( resp . noisy instances ) contribute more ( resp . less ) during fast - tuning to improve the robustness . We calculate α j i as shown below :

Figure 2 shows the annotation guidelines for the Amazon Mechanical Turk study . Figure 3 shows one example of a HIT with two aspects selected . Selected aspects are highlighted in the sentence . We did not allow to choose overlapping aspects . If the aspect was not found in the first list provided by the learned ranker , crowdworkers could choose from as second list with the remaining 1 - 4 - grams of the sentence ( aspect candidates starting or ending with stopwords , as well as candidates with punctuation and numbers , were removed from the list ) . Additional checkboxes were added to choose from if the sentence contained no aspect or the aspect was not explicitly mentioned . Figure 4 shows a ranked list of aspect candidates for an example .

Among these methods , SimCSE may be viewed as our direct baseline , because WhitenedCSE may be viewed as being transformed from SimCSE by adding the SGW and replacing the dual - positive contrastive loss with multi - positive contrastive loss .

Given a source sentence S , all the edits suggested by single models constitute a candidate set A , and the number of edit spans is denoted as m. An edit span means the start - end pair of an edit 's position in the sentence . The set of all the edits ( from different single models ) on the i - th edit span ( including " noop " ) is denoted as A i . Thus , we can divide A = m i=1 A i , where A i = { e i j | j = 1 , 2 , ... , |A i | } , and e i j means the j - th edit on the i - th edit span .

Based on the same experiment setup , we train the models with other simple schedules ( shown in Figure 6 ) for 200k steps using the linear learning rate decay and finetune them on the SQuAD v1.1 . The results on SQuAD v1.1 dev set are presented in Table 4 . We find that cosine is the best compared with those alternatives . Step Masking ratio

Online education platforms can increase the accessibility of educational resources around the world . However , achieving equitable outcomes across diverse learning needs benefits from systems that are adaptive and individualized to each student ( Doroudi and Brunskill , 2019 ) . Traditionally , adaptive education methods involve planning over a pool of pre - made questions ( Atkinson , 1972;Hunziker et al . , 2018 ) . These are naturally limited by the diversity and coverage of the pool , as well as the scaling capacity of curriculum planning algorithms . Recent approaches , such as procedural generation for personalized programming games ( Valls - Vargas et al . , 2017 ) , are limited to well - specified small domains . We address these limitations by leveraging recent success in deep generative models , in particular language models ( LMs ) .

We therefore collect the dataset through two phases : ( 1 ) simulating multimodal dialog flows with templated utterances -thereby programmatically generating fine - grained - scene - grounded annotations and systematically ensuring the diversity of the conversations , and ( 2 ) manual paraphrasing , which ensures the naturalness of utterances with a significantly less annotation overhead ( Rastogi et al . , 2020 ; Shah et al . , 2018 ) .

The performance of HAN is also in general better than non - meta - path - based approaches , especially on macro - F1 , again demonstrating the effectiveness of meta - paths on HIN embedding . The superior performance of MetaFill on both link prediction and node classification collectively proves the effectiveness of using text filling to generate meta - paths .

The KILT benchmark has been recently introduced to evaluate the capabilities of pre - trained language models to address NLP tasks that require access to external knowledge . We evaluate on four diverse tasks from KILT : slot filling , question answering , fact checking and dialog . Figure 1 shows examples of these tasks . Re 2 G makes significant gains on all four tasks , reaching the top of the KILT leaderboards and establishing a new state - of - the - art .

Classification . Both methods aim to remove linearly encoded information . Ravfogel et al . ( 2020 ) test to what extent the non - linear encoding of gender remains intact by running a 1 - hidden - layered MLP with ReLU activation and report 85.0 % after 35 iterations with INLP . After a single MP projection , accuracy of the MLP drops to 81.6 % .

To obtain control codes from training data , we pre - define a set of topics to retrieve documents for and rely on an existing stance detection model to classify whether a sentence argues in favor ( pro ) or against ( con ) the given topic ( Stab et al . , 2018a ) . Regarding argument aspect detection , however , past work has two drawbacks : it either uses simple rule - based extraction of verb - and noun - phrases ( Fujii and Ishikawa , 2006 ) or the definition of aspects is based on target - concepts located within the same sentence ( Gemechu and Reed , 2019 ) . Aspects as we require and define them are not bound to any part - of - speech tag and ( 1 ) hold the core reason upon which the conclusion / evidence is built and ( 2 ) encode the stance towards a general but not necessarily explicitly mentioned topic the argument discusses . For instance :

Errors in the last category occur when the model predicts an entity whose name has no string overlap with that of the gold entity or the mention . This likely happens when the signals from the context override the signals from the mention itself .

Many educational activities involve sequential data , such as language translation , reading compre-

Q5 : Do supervised explanations help , even with GPT-4 ? Test : 5 - shot GPT-4 vs. Zero - shot GPT-4 . Answer : Yes . The zero - shot version of GPT-4 is missing access not only to the supervision of paired ( caption , explanation ) data , but also , explanations in the detailed style of our released corpus . Perhaps as a result , 5 - shot GPT-4 ( which also achieves significantly higher BLEU-4 / Rouge - L ) is preferred in 64 % of cases .

Now that we have a candidate reference responsẽ s t+1 , we can guide the language model towards generating a similar response . To do this , we modify the logits from the language model to encourage generation of the control words or similar words . We start with the first word w 0 ins t+1 and upweight logits in a way similar to DBS ( Pascual et al . , 2020 ) using similarity of GloVe vector embeddings :

The ICoref - inc model from Xia et al . ( 2020 ) is an important comparison point as the only baseline in the sentence - incremental setting . While ICorefinc does not rely on speaker embeddings , our own models ( both Part - Inc and Sent - Inc ) do . Given the important role of speaker identity in a dialogue setting , it is useful to know the effect of removing these embeddings in our models .

beddings using parallel data . They learn a linear transformation that maps a word embedding in a target language to the embedding of the aligned word in the source language . They show that their transformed embeddings are more effective on zero - shot cross - lingual dependency parsing .

Previous work on related tasks ( Taniguchi et al . , 2019 ; Kim and Choi , 2020 ) and on in - domain versions of our task ( Ishigaki et al . , 2021 ) have traditionally divided the efforts into simpler sub - tasks that can be combined into a pipeline approach . We follow a similar approach , also presenting two subtasks .

We train these three models on COCO-35L. In addition , we consider a fourth model based on mT5base + ViT - B / 16 and trained on CC3M-35L. The models are trained on a 4x4x4 TPU - v4 architecture using an Adafactor ( Shazeer and Stern , 2018 ) optimizer with a constant learning rate period between { 1k , 10k } steps , followed by a reversed square - root decay with the number of steps . The batch size is 2048 in all the experiments . The initial learning rate is between { 1e-4 , 3e-4 } . We use the same vocabulary ( size 250k ) as mT5 ( Xue et al . , 2021 ) . The model trained with CC3M-35L is subsequently finetuned on COCO-35L with constant learning rate 3e-5 for 1 epoch .

We use the pretrained mBERT model to initialize both the query and momentum encoders . mBERT is made of 12 Transformer blocks , 12 attention heads , and hidden size d h = 768 . For input , instead of feeding the query encoder with English examples and the momentum encoder with translation examples or vice versa , we propose a random input shuffling approach . Specifically , we randomly shuffle the order of S en i and S tr i when feeding the two encoders , so that the query encoder sees both English and translation examples . We observe that this is a crucial step towards learning good multilingual representations using our method . The final hidden state h ∈ R 1×d h of the [ CLS ] token , normalized with L 2 norm , is treated as the sentence representation 1 . Following Chen et al . ( 2020 ) , we add a non - linear projection layer on top of h :

Instruction - tuning enables stronger generalization to unseen tasks . Generally instruction - tuned models perform better compared to their untuned LM counterparts ( Tk - INSTRUCT vs. T5 - LM , In - structGPT vs. GPT-3 ) and heuristic baselines . This indicates models do learn to follow instructions by finetuning on instruction data , and this can generalize to new instructions for unseen tasks . T0 is an exception , which is only slightly better than T5 - LM . We suspect this is because the style of prompting in T0 's training data is very different from our style of instructions .

The first part of Table 2 presents the results for sentence - level multimodal QE with BiRNN . The best model is BiRNN+Vis - embed - mult2 , achieving a Pearson 's r of 0.535 , significantly outperforming the baseline ( p - value<0.01 ) . Visual features can , therefore , help to improve the performance of sentence - level neural - based QE systems significantly .

In Table 1 , the settings for Adapter-108 and Prefix - tuning-108 are clear , as the only arguments are the bottleneck size / prefix length ; for LoRA-54 , we apply rank-54 updates for both W q and W v , as suggested by Hu et al . ( 2021 ) ; for MAM adapter , we mimic the parameter assignment scheme ( bottleneck size 512 for FFN and prefix length 30 ) by He et al . ( 2021a ) , and use the ratio 102 : 6 to implement MAM adapters with 1.61 % tunable parameters .

We create a cross - topic split with the data of two topics as test set ( gun control , school uniforms ) , one topic as dev set ( death penalty ) , and the remaining topics as train set and evaluate two models with it . First , we use the ranking approach described in Step 2a-2b to fine - tune MT - DNN BASE on the newly generated data ( " Ranker " ) . At inference , we choose the top T aspects for each argument as candidates . We tune T on the dev set and find T = 2 to be the best choice . Second , we use BERT for sequence tagging ( Wolf et al . , 2020 ) and label all tokens of the samples with BIO tags . As previously done with the ranker , we experiment with BERT and MT - DNN weights and find BERT LARGE to be the best choice ( trained for 5 epochs , with a learning rate of 1 × 10 −5 and a batch size of 32 ) . We flatten the predictions for all test samples and calculate the F 1 , Precision , and Recall macro scores . All models are trained over five seeds and the averaged results are reported in Table 3 .

Treating entities as inputs of Transformer . Recent studies Yamada et al . , 2020;Sun et al . , 2020 ) have proposed Transformerbased models that treat entities as input tokens to enrich their expressiveness using additional information contained in the entity embeddings . However , these models were designed to solve general NLP tasks and not tested on ED . We treat entities as input tokens to capture the global context that is shown to be highly effective for ED .

-DOCSTART- Multimodal Quality Estimation for Machine Translation

It is made up of a set of selected cell values V i = { v 1 , . . . , v t } of H i and the rest of headers

We follow the experiment setup in ( Gururangan et al . , 2020 ) . RoBERTa ( Liu et al . , 2019 ) 7 is used as the LM . In each experiment , we first DA - train the LM and then fine - tune it on the end - task . The final evaluation is based on the end - task results .

Further , we point out some limitations of the Arg - CTRL that mitigate the risks discussed before . One of these limitations is that it can not be used to generate arguments for unseen topics , which makes a widespread application ( e.g. to produce fake news ) rather unlikely ( using an unseen topic as control code results in nonsensical repetitions of the input ) . The analysis in Section 6 of the paper shows that the model fails to produce aspectspecific sentences in 92 % of the cases if it was not explicitly conditioned on them at training time . Even in case of success , the aspect has to exist in the training data . Also , the model is trained with balanced classes , i.e. both supporting and opposing arguments for each topic are seen with equal frequency to prevent possible bias into one or the other direction .

Flat model . To decrease the models ' own expressive power ( Hewitt and Liang , 2019 ) , we keep the number of parameters in our probing model as low as possible . While Tenney et al . ( 2019b ) utilize pooled self - attentional span representations and a projection layer to enable cross - model comparison , we directly feed the wordpiece encoding into the classifier , using the first wordpiece of a word . To further increase the selectivity of the model , we directly project the source and target wordpiece representations into the label space , opposed to the two - layer MLP classifier used in the original setup .

one pass through the transformer for k noise samples and n − k data samples . However , this procedure only truly minimizes L ifp θ ( x t |x \t ) = p θ ( x t |x noised \t ) . To apply this efficiency trick we are making the assumption they are approximately equal , which we argue is reasonable because ( 1 ) we choose a small k of 0.15n and ( 2 ) q is trained to be close to the data distribution ( see below ) . This efficiency trick is analogous to BERT masking out multiple tokens per input sequence .

To exploit the value of TKGs , recent research effort has been devoted to process natural language questions over TKG , i.e. , question answering over TKG ( TKGQA in short ) ( Saxena et al . , 2021 ) . Given a question and a background TKG , it retrieves from the TKG an answer to the question . To foster research on TKGQA , several datasets have been introduced , among which CRONQUES - TIONS ( Saxena et al . , 2021 ) is by far the largest . We explain the task with a sample question in CRON - QUESTIONS .

To measure the prevalence of semantic memory in a story , we count the number of sentences that matched ATOMIC knowledge tuples in their surrounding context . We use a context window of size n c = n e = 2 to match inferences , and use the spaCy pipeline ( Honnibal and Montani , 2017 ) to extract noun and verb phrases .

• The effect of debiasing on internal representations is reflected in gender extractability , while not always in CEAT . Thus , gender extractability is a more reliable indicator of gender bias in NLP models .

Regression tasks . The original edge probing setup only considers classification tasks . Many language phenomena -including positional information and semantic proto - roles , are naturally modeled as regression . We extend the architecture by Tenney et al . ( 2019b ) and support both classification and regression : the former achieved via softmax , the latter via direct linear regression to the target value .

In this section , we start by presenting our proposed lightweight framework PTO ( Section 3.1 ) , then introducing two extensions of PTO to leverage optional training data ( Sections 3.2 to 3.4 ) . Finally , we make a summary in Section 3.5 .

Context Difference . Compared with plain text , which is a sequence of words , tables have welldefined structures , and understanding a table 's structure is crucial for schema translation . Specifically , a table consists of an ordered arrangement of rows and columns . Each column header describes the concept of that column . The intersection of a row and a column is called a cell . Each cell contains entities of the column header it belongs to . This structure plays an important role in schema translation , especially for polysemy words and abbreviation words . For example , in Figure 1 , the header " Match " could be translated to " kÙ ( Matchstick ) " , " 9 M ( Mapping ) " , and " ' [ ( Competition ) " , but its sibling column header " Hosted_by " provides important clues that the table might belong to the domain of sport . Thus , translating " Match " to " ' [ ( Competition ) " is more appropriate in the context . Moreover , a column header 's cell values could also provide hints to infer the meaning of the header . For example , successive numerical cell values indicate that " No . " might be an identity column in Figure 1 . NMT models trained with plain text have never seen the structure of tables , and consequently , they perform poorly in schema translation .

We proposed FPT , a prefix tuning - based method , to mitigate the effect of attribute transfer . FPT could encode implicit attributes in a dataset by a general prefix and use it to suppress the attribute transfer via inference - time logits manipulation . Results in the single - attribute control experiments showed that , with FPT , the generated texts can be more effectively controlled under the desired attribute with higher text fluency . Experimental results in the multi - attribute control suggested that FPT can achieve comparable performance to the state - ofthe - art approach while keeping the flexibility of adding new prefixes without retraining .

Word - level QE is generally framed as a supervised ML problem ( Kepler et al . , 2019;Lee , 2020 ) trained on data in which the correctness of translation is labelled at word - level ( i.e. good , bad , gap ) .

Although GPT3 can generalize to 80 digits on copying random numbers ( Figure 2 ) , it does not generalize well beyond 20 items on reversing , which suggests that reversing might require stronger locating capability than copying . This problem also occurs on DeBERTa and T5 . When tested on the OOD data , the models tends to generate only a sublist of the input . Using fine - grained steps ( Figure 9(b ) ) or positional markers , whether implicit or explicit ( Figure 9(c ) ) , does not significantly improve the generalization of the experimented models . The reason might be the increasing distance between the source item and the replicated item as stated above . Again , LMs + tutor maintains 100 % accuracy throughout the experiments . We put more discussion about the results in appendix A.5 due to the page limit .

Baseline . We use a method that relies on crosslingual language model pretraining , namely XLM ( Lample and Conneau , 2019 ) . This approach trains a bilingual MLM separately for En - Mk and En - Sq , which is used to initialize the encoder - decoder of the corresponding NMT system . Each system is then trained in an unsupervised way .

Redundancy is an essential problem in LDQA since in the answer generation phase , repetitious paragraphs could make the QA model confused ( Appendix D ) . Therefore , it 's crucial to select important and diverse evidence pieces for the QA model . To this end , we explore the effects of the evidence history module on redundancy reduction .

We begin by identifying the host speaker and focusing on their questions . Next , we predict which speaker ( s ) would answer the question by identifying speaker entities mentioned in utterances or from previous dialogue turns . Finally , we search utterances from the identified speakers until a stopping criterion is met and label it as the answer . Due to the assumptions made in the above process , models trained directly on this data could overfit on spurious correlations ( Jia and Liang , 2017 ; Wang and Bansal , 2018 ) . Thus , we apply various perturbations to the context such as separating the question and answer utterances , converting to unanswerable questions by removing relevant sentences , creating more speaker transitions , and masking speaker names . Refer to Appendix F for additional details .

1 . You can see the data in " sample_200.txt " , which contains results of 200 sentences .

SPR follows the work of Dowty ( 1991 ) and discards the notion of categorical semantic roles in favor of feature bundles .

To better understand the relationship between intrinsic and extrinsic fairness metrics , we conduct extensive experiments on 19 pre - trained language models ( BERT , etc . ) . We delve into three kinds of biases , toxicity , sentiment , and stereotype , with six fairness metrics across intrinsic and extrinsic metrics , in text classification and generation downstream settings . The protected group domains we focus on are gender , race , and religion .

-DOCSTART- We harness neural language and commonsense models to study how cognitive processes of recollection and imagination are engaged in storytelling . We rely on two key aspects of stories : narrative flow ( how the story reads ) and semantic vs. episodic knowledge ( the types of events in the story ) . We propose as a measure of narrative flow the likelihood of sentences under generative language models conditioned on varying amounts of history . Then , we quantify semantic knowledge by measuring the frequency of commonsense events ( from the ATOMIC knowledge graph ; Sap et al . , 2019 ) , and episodic knowledge by counting realis events ( Sims et al . , 2019 ) , both shown in Figure 1 .

To refine distractors , we use the " Large " version of RoBERTa and all models are trained for 4 epochs and a learning rate of 1 × 10 −5 . These hyperparameters are chosen based on previous experiments with RoBERTa on other multiple - choice datasets . The final UnifiedQA fine - tuning is done using the same multiple choices question answering setup as the one used in the original UnifiedQA paper ( Khashabi et al . , 2020 ) . We use the " Large " version of UnifiedQA and all the models are trained for 4 epochs using Adafactor and a learning rate of 1 × 10 −5 . The learning rate is loosely tuned to get the best performance on the validation set during the supervised training of UnifiedQA . We use the Hugging Face pytorch - transformers ( Wolf et al . , 2020 ) library for model implementation . Experiments presented in this paper were carried out using the Grid'5000 testbed ( Balouek et al . , 2013 ) , supported by a scientific interest group hosted by Inria and including CNRS , RENATER and several Universities as well as other organizations ( see https://www.grid5000.fr ) .

In order to create questions from a single constituency structure , jsRealB uses the classical grammar transformations : for a who question , it removes the subject ( i.e. the first noun phrase before the verb phrase ) , for a what question , it removes the subject or the direct object ( i.e. the first noun phrase within the verb phrase ) ; for other types of questions ( when , where ) it removes the first prepositional phrase within the verb phrase . Depending on the preposition , the question will be a when or a where . Note that the removed part becomes the answer to the question .

Task definitions . Our rich graph - based annotation scheme allows for a number of information extraction tasks . In the scope of this paper , we address the following steps of ( 1 ) identifying sentences that describe SOFC - related experiments , ( 2 ) recognizing and typing relevant named entities , and

n t=1 − log n•p θ ( xt|x \t ) n•p θ ( xt|x \t ) + k•q(xt|x \t ) . 2 . Sample k negative samples according to t ∼ unif{1 , n},x t ∼ q(x t |x \t ) . 3 . For each negative sample , add to the loss − log k•q(xt|x \t ) n•p θ ( xt|x \t ) + k•q(xt|x \t ) .

One limitation of our model is that , similar to existing ED models , our model can not handle entities that are not included in the vocabulary . In our future work , we will investigate the method to compute the embeddings of such entities using a post - hoc training with an extended vocabulary ( Tai et al . , 2020 ) .

In this paper , we build the optimal translation policy under all latency by simply setting the search interval , achieving high performance . However , we think that the performance of our method can be further improved by exploring more interval settings . Additionally , although we train the agent using a simple architecture and achieve good performance , there exists a performance gap between the learned policy and the searched optimal policy under low latency . Exploring more powerful models of the agent may help improve the performance and we leave it for future work .

Step 1 : Preliminary annotations To ensure the feasibility of creating a dataset for this task , two experts ( a post - doctoral researcher and an undergraduate student with NLP background ) independently annotate 800 random samples ( from four topics , 200 per topic ) taken from the UKP - Corpus . The annotations are binary and on token - level , where multiple spans of tokens could be selected as aspects . The resulting inter - annotator agreement of this study is Krippendorff 's α u = .38 . While this shows that the task is generally feasible , the agreement on exact token spans is rather low . Hence , in the following steps , we reduce the complexity of the annotation task .

where E = E L ∩ E C . As a result , the loss function L 2 enables PAR to ensure ideological stance consistency among political actors .

With respect to benefiting patients , the discussion of natural language artifacts we have presented is intended to encourage clinical researchers who rely on ( or construct ) expert - annotated clinical corpora to train domain - specific language models , or consume such models to perform downstream tasks , to be aware of the presence of annotation artifacts , and adjust their assessments of model performance accordingly . It is our hope that these findings can be used to inform error analysis and improve predictive models that inform patient care .

For any given example , the higher the cross - entropy is , the poorer the quality of the corresponding translation , which reflects the inability in this case .

Morphophonology refers to the bidirectional interaction between phonology and morphology and is crucial for understanding how morphologically related words may nevertheless surface with different forms . Arabic exhibits pervasive morphophonological processes governed by phonological constraints on syllable structure which interact both with concatenative and templatic morphology . 2 To make matters more complex , Arabic varieties exhibit distinct morphophonological processes , so words with identical morphological analyses may have different forms . Table 1 demonstrates dialectal variation in surface realizations for the same morphological analysis .

We conduct experiments on word order - diverse language pairs : WMT'14 English⇒German ( En - De ) , WAT'17 Japanese⇒English ( Ja - En ) , and WMT'17 Chinese⇔English ( Zh - En & En - Zh ) .

• 20Newsgroups : We downloaded the standard splits of the dataset using scikit - learn 11 . The header and the footer of each text were removed .

In this section , We evaluate our method on seven Semantic Textual Similarity ( STS ) tasks and seven transfer tasks . We use the SentEval ( Conneau and Kiela , 2018 ) toolkit for all of tasks .

Then we start investigating the impact of the additional bilingual dataset on the model performance .

Perplexity is one of the most common metrics for evaluating language models , and is defined as the exponential average negative log - likelihood of a sequence :

Contributions . This work studies the effect of the linguistic formalism on probing results . We conduct cross - formalism experiments on PropBank , VerbNet and FrameNet role prediction in English and German , and show that the formalism can affect probing results in a linguistically meaningful way ; in addition , we demonstrate that layer probing can detect subtle differences between implementations of the same formalism in different languages . On the technical side , we advance the recently introduced edge and layer probing framework ( Tenney et al . , 2019b ) ; in particular , we introduce anchor tasks -an analytical tool inspired by feature - based systems that allows deeper qualitative insights into the pre - trained models ' behaviour . Finally , advancing the current knowledge about the encoding of predicate semantics in BERT , we perform a fine - grained semantic proto - role probing study and demonstrate that semantic proto - role properties can be extracted from pre - trained BERT , contrary to the existing reports . Our results suggest that along with task and language , linguistic formalism is an important dimension to be accounted for in probing research .

Table 6 shows more cases when deleting all instances containing specific words , including " be - come " ( verb ) , " fresh " ( adjective ) , and " energy " ( noun ) . We can find that unlearned models ( i.e. , RE - TRAIN and KGA ) tend to generate alternatives with similar meanings regardless of the part of speech .

While ELECTRA learns whether a token is more likely to come from the data distribution p data or noise distribution q , Electric only learns p data because q is passed into the model directly . This difference is analogous to using negative sampling ( Mikolov et al . , 2013 ) vs. noise - contrastive estimation ( Mnih and Kavukcuoglu , 2013 ) for learning word embeddings . A disadvantage of Electric compared to ELEC - TRA is that it is less flexible in the choice of noise distribution . Since ELECTRA 's binary classifier does not need to access q , its q only needs to be defined for negative sample positions in the input sequence . Therefore ELECTRA can use a masked language model rather than a two - tower cloze model for q. An advantage of Electric is that it directly provides ( un - normalized ) probabilitieŝ p θ for tokens , making it useful for applications such as re - ranking the outputs of text generation systems . The differences between ELECTRA and Electric are summarized below :

We can improve on this by adding some structure to this representation by teaching our model that the v i belong to different segments . As in the baseline candidate re - ranking model , we do this by separating them with [ SEP ] tokens . We call this [ SEP]-separation . This approach is also used by Logeswaran et al . ( 2019 ) andMulang ' et al . ( 2020 ) " name " : " Douglas Adams " " place of birth " : " Cambridge " " occupation " : " novelist " " employer " : " BBC " to separate the entity attributes in their respective KBs .

1 . An attention check in the recruitment qualification task ( " How many letters are in the word ' banana ' ? " ) ; we only recruited participants who passed this initial attention check .

The last inequality is an empirical conclusion , since in our experiments l ≈ 10 while d = 32 in most situations .

We considered two tasks in this experiment . The first task aims to classify " Christianity " vs " Atheism " documents from the 20 Newsgroups dataset 5 . This dataset is special because it contains a lot of artifacts -tokens ( e.g. , person names , punctuation marks ) which are not relevant , but strongly co - occur with one of the classes . For evaluation , we used the Religion dataset by Ribeiro et al . ( 2016 ) , containing " Christianity " and " Atheism " web pages , as a target dataset . The second task is sentiment analysis . We used , as a training dataset , Amazon Clothes , with reviews of clothing , shoes , and jewelry products ( He and McAuley , 2016 ) , and as test sets three out - of - distribution datasets -Amazon Music ( He and McAuley , 2016 ) , Amazon Mixed , and the Yelp dataset ( which was used in Experiment 1 ) . Amazon Music contains only reviews from the " Digital Music " product category which was found to have an extreme distribution shift from the clothes category ( Hendrycks et al . , 2020 ) . Amazon Mixed compiles the reviews from various kinds of products , while Yelp focuses on restaurant reviews .

Most previous work seeking the numerical solution for low - rank approximation is designed for unweighted cases , with applications such as predicting the missing values recommendation system ( Yu et al . , 2014 ; Zhou et al . , 2008 ) . Also , a few attempts have been made to solve the weighted lowrank approximation problem through EM - based algorithm ( Srebro and Jaakkola , 2003 ) , or alternating least squares ( He et al . , 2016 ) .

Effect of τ in HeadXL SANs

We introduce an LM - based knowledge tracing model ( LM - KT ) to predict students ' difficulty on novel questions ( e.g. target phrases to translate ) . We show that LM - KT is well - calibrated , allowing us to pose the learning problem for the question generator : given a student state , generate a question that will achieve a target difficulty , according to LM - KT . We evaluate both LM - KT and question generation models on real users and responses from Duolingo 1 , a popular online second - language learning platform .

We also experimented with zero - shot QE with multilingual QE models . We trained the QE model in all the pairs except one and performed predic - tion on the test set of the language pair left out . In section II ( " All-1 " ) , we show its difference to the multilingual QE model . This also provides competitive results for the majority of the languages , proving it is possible to train a single multilingual QE model and extend it to a multitude of languages and domains . This approach provides better results than performing transfer learning from a bilingual model .

To enable applying BC for text - based reasoning , we introduce four LM - based modules : Fact Check , Rule Selection , Goal Decomposition , and Sign Agreement , each implemented by showing relevant in - context demonstrations to a pretrained LM ( see Appendix D.3 for details ) . We describe these modules and then proceed to the full algorithm .

Probing . We probe the representation of a profession word as extracted from Winobias sentences ,

The noise distribution q comes from a neural network trained to match p data . NCE commonly employs this idea to ensure the classification task is sufficiently challenging for the model ( Gutmann and Hyvärinen , 2010;Wang and Ou , 2018 ) . In particular , we use a two - tower cloze model as proposed by Baevski et al . ( 2019 ) , which is more accurate than a language model because it uses context to both sides of each token . The model runs two transformers T LTR and T RTL over the input sequence . These transformers apply causal masking so one processes the sequence left - to - right and the other operates right - to - left . The model 's predictions come from a softmax layer applied to the concatenated states of the two transformers :

D1 . Did you report the full text of instructions given to participants , including e.g. , screenshots , disclaimers of any risks to participants or annotators , etc . ? Appendix B D2 . Did you report information about how you recruited ( e.g. , crowdsourcing platform , students ) and paid participants , and discuss if such payment is adequate given the participants ' demographic ( e.g. , country of residence ) ? Appendix B D3 . Did you discuss whether and how consent was obtained from people whose data you 're using / curating ? For example , if you collected data via crowdsourcing , did your instructions to crowdworkers explain how the data would be used ? Not applicable . Left blank .

• We introduce a new task named Grounded Multimodal Named Entity Recognition ( GMNER ) , which aims to extract all the entity - type - region triples from a text - image pair . Moreover , we construct a Twitter dataset for the task based on two existing MNER datasets . • We extend four well - known MNER methods to benchmark the GMNER task and further propose a Hierarchical Index generation framework named H - Index , which generates the entity - typeregion triples in a hierarchical manner . • Experimental results on our annotated dataset show that the proposed H - Index framework performs significantly better than a number of unimodal and multimodal baseline systems on the GMNER task , and outperforms the second best system by 3.96 % absolute percentage points on F1 score .

Comparing VA - Rand to vip - AnT , we see accuracy increases in all classification and retrieval setups . For example , on AudioCaps , vip - AnT outperforms VA - Rand by 4.5 % R @ 1 and 13.6 % R @ 10 . This confirms that the findings of carry - over to unsupervised audio pre - training .

Table 18 shows some examples of variation in tokenization .

Objective AT Supervision VT Alignment Zero - shot AT Retrieval MMV ( Alayrac et al . , 2020 ) Random L bi - bi

Trainable VATT ( Akbari et al . , 2021 ) Random L bi - bi

Trainable AudioCLIP ( Guzhov et al . , 2021a ) ImageNet L tri 2 M Audio Tags Trainable Wav2CLIP ( Wu et al . , 2021 ) Random

We used this propagation rule , so called LRP- , in the experiments of this paper . For more details about LRP propagation rules , please see Montavon et al . ( 2019 ) .

